{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_02_WIDER_FACE.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarekGit/FACES_DNN/blob/master/notebooks/03_02_WIDER_FACE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNPMGlPA7tG7",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "[Spis treści](https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Praca_Dyplomowa.ipynb) | [1. Wstęp](01_00_Wstep.ipynb) | [2. Metryki oceny detekcji](02_00_Miary.ipynb) | [3. Bazy danych](03_00_Datasety.ipynb) | [4. Przegląd metod detekcji](04_00_Modele.ipynb) | [5. Detekcja twarzy z wykorzystaniem wybranych architektur GSN](05_00_Modyfikacje.ipynb) | [6. Porównanie modeli](06_00_Porownanie.ipynb) | [7. Eksport modelu](07_00_Eksport_modelu.ipynb) | [8. Podsumowanie i wnioski](08_00_Podsumowanie.ipynb) | [Bibliografia](Bibliografia.ipynb)\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p682jaHwBIzl",
        "colab_type": "text"
      },
      "source": [
        "## 3.2. WIDER FACE: A Face Detection Benchmark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1bFQ3OE8sAy",
        "colab_type": "text"
      },
      "source": [
        "WIDER FACE [[3]](Bibliografia.ipynb) jest wzorcowym zbiorem danych do detekcji twarzy. Zawiera 32 203 zdjęć z 393 703 anotacjami twarzy. Zdjęcia wykorzystane do stworzenia datasetu pochodzą ze zbioru danych [WIDER dataset](http://personal.ie.cuhk.edu.hk/~xy012/event_recog/WIDER). Obrazy wchodzące w skład datasetu zostały podzielone na 60 klas. Dla każdej z tych klas dokonano losowego wyboru danych o podziale 40% / 10% / 50% w celu wyodrębnienia podzbiorów: treningowego, walidacyjnego i testowego."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crvjV6Ew8sDu",
        "colab_type": "text"
      },
      "source": [
        "Twarze z datasetu WIDER FACE różnią się znacznie wyglądem, pozą i skalą. Podczas anotacji zdjęć przyjęto, że region reprezentujący twarz musi ściśle obejmować czoło, podbródek i policzek. Jeśli twarz jest przysłonięta nadal jest oznaczana ale dodatkowo posiada określony atrybut określający poziom zasłonięcia twarzy (częściowy, znaczny). Dodatkowy atrybut opisuje również pozę (typowa, nietypowa). Twarze o niskiej rozdzielczości i małej skali (10 pikseli lub mniej) nie zostały anotowane. Przykładowe zdjęcia z anotacjami przestawia rysunek 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR0Ox5dP8sG9",
        "colab_type": "text"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/groundai-web-prod/media%2Fusers%2Fuser_7525%2Fproject_12108%2Fimages%2Fx2.png\" alt=\"Przykłady adnotacji w zbiorze danych WIDER FACE.\" width=\"700\" ><br>\n",
        "\n",
        "\n",
        "Rys. 1. Przykłady adnotacji w zbiorze danych WIDER FACE <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/Bibliografia.ipynb\">[3]</a>\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDOmnJUK8sJ2",
        "colab_type": "text"
      },
      "source": [
        "Ze względu na zachowaną różnorodność w przygotowanym zbiorze danych dataset WIDER FACE jest uważany jako trudny w detekcji. Autorzy datasetu dokonali porównania WIDER FACE z innymi zbiorami danych. Na Rys. 2a <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/Bibliografia.ipynb\">[3]</a> zauważyć można, że WIDER FACE ma znacznie niższy wspołczynnik detekcji. Dokonano również analizy przy podziale datasetu na ’Easy’, ’Medium’ i ’Hard’ ze wzdlędu na wspołczynnik detekcji Edgebox <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/Bibliografia.ipynb\">[5]</a>.\n",
        "\n",
        "Dokonano również analizy detekcji w odniesieniu do wielkości (skali) twarzy. Twarze zostały pogrupowane według rozmiaru: małe (10-50 pikseli wysokości), średnie (50-300 pikseli) i duże (ponad 300 pikseli). Rys. 2b przedstawia histogram wskaźnika detekcji w odniesieniu do średniej liczby propozycji na obraz generowanych za pomocą metody Edgebox <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/Bibliografia.ipynb\">[5]</a> w odniesieniu do podziału datasetu ze względu na skalę. \n",
        "Duże i średnie skale osiągają wysoki wynik detekcji (ponad 90% już przy 8000 propozycji na obrazie). W przypadku twarzy małych wskaźniki wykrywalności stale utrzymuje się poniżej 30%.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Aj4HUjndYV4",
        "colab_type": "text"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/groundai-web-prod/media%2Fusers%2Fuser_7525%2Fproject_12108%2Fimages%2Fx3.png\" alt=\"Współczynnik detekcji przy różnej liczbie propozycji generowanych za pomocą Edgebox\" width=\"700\"  ><br>\n",
        "\n",
        "\n",
        "Rys. 2. Współczynnik detekcji przy różnej liczbie propozycji generowanych za pomocą Edgebox <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/Bibliografia.ipynb\">[5]</a>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uLJDtqrQk8D",
        "colab_type": "text"
      },
      "source": [
        "Rozpatrująć dataset pod względem poziomu przysłonięcia twarzy na analizowanym obrazie dokonano podziału na trzy grupy: twarze nie osłonięte, częściowo osłonięte (1%-30% całkowitej powierzni twarzy) i znacznie osłonięte (powyżej 30% całkowitej powierzchni twarzy). Bez względu na ilość przypadków wspołczynnik detekcji dla twarzy częściowo przysłoniętych osiąga maksymalnie 55% a twarzy znacznie osłoniętych utrzymuje się poniżej 40% (Rys 2c).\n",
        "\n",
        "Analizująć pozycję głowy zdefiniowano dwa poziomy pozycji: typowy i nietypowy.\n",
        "Twarz jest uważana jako nietypowa w przypadku gdy stopień przechylenia lub nachylenia jest większy niż 30 stopni lub gdy odchylenie głowy jest większe niż 90 stopni. Twarze o nietypowych pozach są znacznie trudniejsze w detekcji (Rys. 2d).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qh9BHc0kQd2v",
        "colab_type": "text"
      },
      "source": [
        "Dokonano również analizy detekcji twarzy ze względu na kategorię wydarzenia lub inaczej, sceny jaką zarejestrowano na zdjęciu. Wyodrębniono 60 klas reprezentujących różne zdarzenia. Dla każdej klasy analizowano współczynnik detekcji ze wzgędu na skalę twarzy, przysłonięcia twarzy i pozy. Na podstawie takiej oceny wyznaczono trzy grupy: łatwą (41-60 klas), średnią (21-40 klas) i trudną (1-20 klas) (Rys. 3)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq0oVUkAQd0W",
        "colab_type": "text"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/groundai-web-prod/media%2Fusers%2Fuser_7525%2Fproject_12108%2Fimages%2Fx4.png\" alt=\"Przykłady adnotacji w zbiorze danych WIDER FACE.\" ><br>\n",
        "\n",
        "\n",
        "Rys. 3. Wskaźnik detekcji dla różnych kategorii wydarzeń.  <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/Bibliografia.ipynb\">[3]</a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WuJfLeWiHSx",
        "colab_type": "text"
      },
      "source": [
        "### Pobranie datasetu "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyFCWFbplV7h",
        "colab_type": "text"
      },
      "source": [
        "Po pobraniu i rozpakowaniu datasetu do wskazanego podkatalogu otrzymujemy określoną strukturę katalogów zawierających zdjęcia:\n",
        "```\n",
        "WIDER/ WIDER_test/ images/ nr kategorii wydarzenia (scena) i nazwa / *.jpg\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyNRxpoklWEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "os.makedirs('WIDER/', exist_ok=True)\n",
        "\n",
        "### WIDER Face Training Images\n",
        "!gdown https://drive.google.com/uc?id=0B6eKvaijfFUDQUUwd21EckhUbWs -O WIDER/temptr.zip\n",
        "!unzip -q WIDER/temptr.zip -d WIDER\n",
        "!rm WIDER/temptr.zip\n",
        "\n",
        "### WIDER Face Validation Images\n",
        "!gdown https://drive.google.com/uc?id=0B6eKvaijfFUDd3dIRmpvSk8tLUk -O WIDER/tempv.zip\n",
        "!unzip -q WIDER/tempv.zip -d WIDER\n",
        "!rm WIDER/tempv.zip  \n",
        "\n",
        "### WIDER Face Testing Images\n",
        "!gdown https://drive.google.com/uc?id=0B6eKvaijfFUDbW4tdGpaYjgzZkU -O WIDER/tempt.zip\n",
        "!unzip -q WIDER/tempt.zip -d WIDER\n",
        "!rm WIDER/tempt.zip\n",
        "\n",
        "### Face annotations\n",
        "!wget mmlab.ie.cuhk.edu.hk/projects/WIDERFace/support/bbx_annotation/wider_face_split.zip -O WIDER/wider_face_split.zip\n",
        "!unzip -q WIDER/wider_face_split.zip -d WIDER\n",
        "!rm WIDER/wider_face_split.zip\n",
        "\n",
        "### Examples and formats of the submissions\n",
        "!wget mmlab.ie.cuhk.edu.hk/projects/WIDERFace/support/example/Submission_example.zip -O WIDER/Submission_example.zip\n",
        "!unzip -q WIDER/Submission_example.zip -d WIDER\n",
        "!rm WIDER/Submission_example.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQT8DTS-l3fO",
        "colab_type": "text"
      },
      "source": [
        "### Anotacje"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKm6ePDdjntx",
        "colab_type": "text"
      },
      "source": [
        "Po ściągnięciu i rozpakowaniu pliku **wider_face_split.zip** otrzymujemy **wider_face_test_filelist.txt** ze ścieżkami do wszystkich plików w datasecie. Anotacje dla podzbioru treningowego i walidacyjnego znajdują się w plikach **wider_face_train_bbx_gt.txt** i **wider_face_val_bbx_gt.txt**.\n",
        "\n",
        "Przykładowe anotacje z **wider_face_train_bbx_gt.txt**:\n",
        "\n",
        "```\n",
        "0--Parade/0_Parade_marchingband_1_849.jpg\n",
        "1\n",
        "449 330 122 149 0 0 0 0 0 0 \n",
        "0--Parade/0_Parade_Parade_0_904.jpg\n",
        "1\n",
        "361 98 263 339 0 0 0 0 0 0 \n",
        "```\n",
        "\n",
        "Format zastosowanych anotacji został określony zgodnie z ponizszym szablonem:\n",
        "\n",
        "```\n",
        "nazwa pliku\n",
        "liczba bounding boxów\n",
        "x1, y1, w, h, blur, expression, illumination, invalid, occlusion, pose\n",
        "```\n",
        "* x1, y1, w, h - służą do wyliczenia wspołrzędnych wierzchołkow anotacj prostokątnej,   \n",
        "\n",
        "* blur: - \n",
        "  clear->0, \n",
        "  normal blur->1, \n",
        "  heavy blur->2,\n",
        "\n",
        "* expression:\n",
        "  typical expression->0\n",
        "  exaggerate expression->1\n",
        "\n",
        "* illumination:\n",
        "  normal illumination->0\n",
        "  extreme illumination->1\n",
        "\n",
        "* occlusion:\n",
        "  no occlusion->0\n",
        "  partial occlusion->1\n",
        "  heavy occlusion->2\n",
        "\n",
        "* pose:\n",
        "  typical pose->0\n",
        "  atypical pose->1\n",
        "\n",
        "* invalid:\n",
        "  false->0(valid image)\n",
        "  true->1(invalid image)\n",
        "\n",
        "  <br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21cRWBO-Ld8n",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "\n",
        "\n",
        "[3.3 FACES_DD](03_03_FACES_DD.ipynb)<br> \n",
        "\n",
        "---\n",
        "\n",
        "[Spis treści](https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Praca_Dyplomowa.ipynb) | [1. Wstęp](01_00_Wstep.ipynb) | [2. Metryki oceny detekcji](02_00_Miary.ipynb) | [3 .Bazy danych](03_00_Datasety.ipynb) | [4. Przegląd metod detekcji](04_00_Modele.ipynb) | [5. Detekcja twarzy z wykorzystaniem wybranych architektur GSN](05_00_Modyfikacje.ipynb) | [6. Porównanie modeli](06_00_Porownanie.ipynb) | [7. Eksport modelu](07_00_Eksport_modelu.ipynb) | [8. Podsumowanie i wnioski](08_00_Podsumowanie.ipynb) | [Bibliografia](Bibliografia.ipynb)\n",
        "\n",
        "\n",
        "---"
      ]
    }
  ]
}