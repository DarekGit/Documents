{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_04_MOBILENETV2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarekGit/FACES_DNN/blob/master/notebooks/05_02_MOBILENETV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jctTuqA2dPF-",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "[Spis treści](https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Praca_Dyplomowa.ipynb) | [1. Wstęp](01_00_Wstep.ipynb) | [2. Metryki oceny detekcji](02_00_Miary.ipynb) | [3. Bazy danych](03_00_Datasety.ipynb) | [4. Przegląd metod detekcji](04_00_Modele.ipynb) | [5. Detekcja twarzy z wykorzystaniem wybranych architektur GSN](05_00_Modyfikacje.ipynb) | [6. Porównanie modeli](06_00_Porownanie.ipynb) | [7. Eksport modelu](07_00_Eksport_modelu.ipynb) | [8. Podsumowanie i wnioski](08_00_Podsumowanie.ipynb) | [Bibliografia](Bibliografia.ipynb)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1KfBE5jG-RD",
        "colab_type": "text"
      },
      "source": [
        "< [5.1. Detectron2 Faster R-CNN z FPN Resnet50](05_01_DETECTRON2.ipynb) | [5.2. MOBILENETV2](05_02_MOBILENETV2.ipynb) | [5.3. Techniki szybkiego i stabilnego uczenia GSN.](05_03_FrozenBN_Mish.ipynb) >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI1VfRvBSNfc",
        "colab_type": "text"
      },
      "source": [
        "## 5.2. MobileNet V2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGDENIZCovS4",
        "colab_type": "text"
      },
      "source": [
        "W 2017 roku Zespół Google z dużym sukcesem wprowadził MobileNet V1 do klasyfikacji obrazów, jak i ekstracji cech w większych sieciach neuronowych.\n",
        "Już rok później w 2018 zespół AI Google powiadomił o wprowadzeniu MobileNet version 2. <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[24]</a>\n",
        "\n",
        "Na bazie V1 udało się stworzyć jeszcze bardziej efektywną i wydajną wersję, **do zastosowania na urządzeniach mobilnych.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjdM9BZLqznx",
        "colab_type": "text"
      },
      "source": [
        "### Główne cechy MobileNet V1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIcjCRzxpabQ",
        "colab_type": "text"
      },
      "source": [
        "Podstawowym pomysłem w V1 było wykorzystanie sperowalnych sieci konwolucyjnych w celu obniżenia zapotrzebowania na moc obliczeniową. Standardowe sieci konwolucyjne wymagają wykonania dużej liczby obliczeń, których ilość jest proporcjonalna do iloczynu ilości kanałów wejściowych, ilości kanałów wyjściowych, wielkości danych wejściowych i wielkości kernela. Zastosowanie konwolucji separowalnych <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[25]</a> po kanałach, obniża zapotrzebowanie na moc obliczeniową kilkukrotnie.\n",
        "\n",
        "Podstawowa warstwa konwolucyjna została podzielona na dwa elementy: pierwszy to warstwa konwolucji separowanej po kanałach (depthwise convolution), po której mamy warstwę konwolucji 1×1 (pointwise) łączącej cechy z kanałów:<br><br>\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/MN2/MN2_SeparableConv.png?raw=1\" alt=\"Separable Convolution\" width=\"300\" >\n",
        "\n",
        "</div>\n",
        "\n",
        "<br>Razem obie warstwy tworzą blok konwolucyjny, który wykonuje podobne operacje do standardowej sieci konwolucyjnej, ale znacznie szybciej. (podobne rozwiązania stosujemy w sieciach typu Inception).\n",
        "<br>Architektura Mobilenet V1 składa się z standardowej warstwy konwolucyjnej 3x3 a następnie z trzynastu bloków opisanych powyżej.\n",
        "\n",
        "Pomiędzy warstwami konwolucji separowalnych nie stosujemy warstw Pool, zamiast tego wybrane warstwy konwolucji mają stride równe 2 w celu redukcji rozmiaru danych. Przy zastosowaniu redukcji rozmiaru danych, jednocześnie dublowana jest ilość kanałów wyjściowych w warstwie konwolucji 1x1 (pointwise). Przykładowo dla obrazu wejściowego o rozmiarze 224×224×3 na wyjściu sieci otrzymujemy mapę cech o rozmiarze 7×7×1024 na wyjściu.\n",
        "\n",
        "Tak jak w większości współczesnych architektur, po warstwach konwolucyjnych stosujemy batch normalization, a jako funkcję aktywacji **ReLU6**. ReLU6 ma dodatkowe ograniczenie aby uniknąć zbyt dużych wartości:\n",
        "\n",
        "<pre><code class=\"language-python\">y = min(max(0, x), 6)\n",
        "</code></pre>\n",
        "\n",
        "Autorzy MobileNet w dokumentacji wskazują, iż sieć z ReLU6 jest bardziej odporna niż ReLU dla obliczeń z małą precyzją (16-bit float np dla iOS). \n",
        "\n",
        "Kształt ReLU6 ma zbliżone cechy do funkcji sigmoid:\n",
        "\n",
        "<div align=\"center\"><br>\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/MN2/MN2_ReLU6.png?raw=1\" alt=\"ReLU6\" width=\"600\" ><br>\n",
        "</div>\n",
        "\n",
        "\n",
        "<br>Sieć MobileNet w zastosowaniach do klasyfikacji kończy się blokiem składającym się z warstwy Average Pooling, FC lub alternatywnie Convolution 1x1 oraz Softmax.\n",
        "\n",
        "<br>Sieć Mobilenet została przygotowana jako rodzina sieci. Mamy kilka hiperparametrów umożliwiających stosowanie różnych wersji sieci Mobilenet.\n",
        "<br>Najważniejszym hiperparametrem jest  <strong>depth multiplier</strong>, czasami nazywany \"width multiplier\". Zmienia on ilość kanałów w każdej warstwie. \n",
        "<br>Stosując depth multiplier równy 0.5 obniżamy ilość kanałów, a tym samym ilość koniecznych obliczeń czterokrotnie oraz ilość parametrów sieci trzykrotnie, dzięki czemu sieć jest szybsza, ale  mniej dokładna.\n",
        "\n",
        "<br>Dzięki zastosowaniu bloków z konwolucją separowalną MobileNet potrzebuje 9-krotnie mniej mocy obliczeniowej do porównywalnych sieci neuronowych z tą samą dokładnością klasyfikacji. Taki typ sieci może być stosowany z powodzeniem w czasie rzeczywistym do 200 warstw na iPhone 6s.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo_2ZJqXswtP",
        "colab_type": "text"
      },
      "source": [
        "### MobileNet V2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWidMl92tQiL",
        "colab_type": "text"
      },
      "source": [
        "MobileNet V2 <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[26]</a> bazuje na V1 i używa separowalnych warstw konwolucyjnych, a jego podstawowy blok wygląda następująco:\n",
        "\n",
        "<div align=\"center\"><br>\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/MN2/MN2_ResidualBlock.png?raw=1\" alt=\"bottleneck\" width=\"300\" ><br>\n",
        "</div>\n",
        "\n",
        "<br>W V2 mamy trzy warstwy konwolucyjne w bloku, dwie ostatnie są zgodne z wersją V1, ale konwolucja 1x1 ma inne zadanie. W wersji V1 zadaniem konwolucji 1x1 (pointwise) było utrzymanie lub zdublowanie ilości kanałów. W przypadku wersji V2 jej zadaniem jest zmniejszenie ilości kanałów. Dlatego nazywana jest **projection layer**, odwzorowuje większą ilość kanałów wejściowych na tensor z mniejszą ilością kanałów.\n",
        "<br>Przykładowo warstwa Depthwise może pracować ze 144 kanałami, które są ograniczane do tylko 24 w warstwie Pointwise. Czasami warstwę Pointwise nazywa się **bottlnect layer** poniewż zawęża ilość danych przepływających przez sieć.\n",
        "\n",
        "<br> Pierwsza warstwa w bloku spełnia nowe zadanie. Jest to również warstwa konwolucyjna 1x1, a jej zadaniem jest rozszerzenie ilości kanałów danych przed warstwą DepthWise. Nazywa się ją **expansion layer** i zawsze zwiększa ilość kanałów (odwrotnie niż **projection layer**).\n",
        "Współczynnik ekspansji podawany jest jako hiperparametr i można go zmieniać. Domyślnie przyjmowany jest jako 6 (**expansion factor**).\n",
        "<br>Przykładowo dla 24 kanałów wejściowych do bloku, Expansion Layer powiększa ilość kanałów do 144, następnie wykonywana jest konwolucja separowalna (Depthwise) na 144 kanałach i ostatecznie Projection Layer redukuje ilość kanałów do 24.\n",
        "\n",
        "<div align=\"center\"><br>\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/MN2/MN2_Expand.png?raw=1\" alt=\"Expansion and projection\" width=\"600\" ><br>\n",
        "</div>\n",
        "\n",
        "<br>Tak więc wejście i wyjście ma mniejszy rozmiar, podczas gdy rozmiar wewnątrz bloku jest większy.\n",
        "<br><br>Drugą nową cechą MobileNetV2 jest **residual connection**. Działa ono podobnie do ResNet i pomaga w utrzymaniu przepływu gradientu w sieci. Residual connection używane jest tylko i wyłącznie w przypadku kiedy ilość kanałów wejściowych jest równa ilości kanałów wyjściowych dla bloku.\n",
        "\n",
        "<br>Jak w innych sieciach konwolucyjnych każda warstwa ma batch normalization i funkcję aktywacji (ReLU6), oprócz warstwy wyjściowej. Autorzy MobileNet w dokumentacji wskazują, iż warstwa nieliniowa na wyjściu dla małych rozmiarów danych wyjściowych pogarsza wyniki pracy sieci.\n",
        "\n",
        "<br> Pełna architektura MobileNetV2 składa się ze standardowej warstwy konwolucyjnej 3x3 z 32 kanałami, następnie 17  bloków, zakończonych standardową warstwą konwolucyjną 1x1, Average Pool i warstwą klasyfikacyjną.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "**Cel zmian**\n",
        "\n",
        "<br>Pomysł dotyczący V1 opierał się na zmianie złożonej warstwy konwolucyjnej na mniej wymagająca warstwę separowalną **DeepWise**, co okazało sie dużym sukcesem. \n",
        "<br>Główną zmianą w wersji V2 jest zastosowanie **residual connection** oraz **expand/projection layers**, co umożliwia zachowanie małego rozmiaru na wejściu i wyjściu bloków:\n",
        "\n",
        "<div align=\"center\"><br>\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/MN2/MN2%20_LowDim.png?raw=1\" alt=\"dimensions between the blocks\" width=\"600\" ><br>\n",
        "</div>\n",
        "\n",
        "<br> Rozmiar tensora wyjściowego dla wersji V1 (7×7×1024) jest większy od V2. Dzięki zachowaniu małego rozmiaru tensora uzyskano redukcją koniecznej liczby obliczeń dla sieci przy zachowaniu zadowalającej jakości klasyfikacji sieci.\n",
        "<br>Podaje się porównanie, że mały rozmiar danych pomiędzy blokami odzwierciedla  skompresowane dane rzeczywiste, które są dekompresowane wewnątrz bloków:\n",
        "\n",
        "<div align=\"center\"><br>\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/MN2/MN2_Compression.png?raw=1\" alt=\"Decompression and compression \" width=\"600\" ><br>\n",
        "</div>\n",
        "\n",
        "<br> Expansion Layer działa jako dekompresor, który odtwarza dane rzeczywiste do przetwarzania wewnątrz bloku, a które są następnie kompresowane w Projection Layer w celu zmniejszenia rozmiaru danych na wyjściu bloku. Parametry całego procesu w tym kompresji i dekompresji podlegają optymalizacji w trakcie uczenia sieci.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i4tWD1GuIo_",
        "colab_type": "text"
      },
      "source": [
        "### Porównanie wersji MobileNet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzSO-fv9t2jp",
        "colab_type": "text"
      },
      "source": [
        "<br>Poniższa tabela zawiera porównanie MobileNet V1 i V2, wymaganą ilość obliczeń oraz rozmiar modelu - ilość parametrów:\n",
        "<br>\n",
        "\n",
        "<table>\n",
        "<thead>\n",
        "<tr>\n",
        "<th>Version</th>\n",
        "<th>MACs (millions)</th>\n",
        "<th>Parameters (millions)</th>\n",
        "</tr>\n",
        "</thead>\n",
        "\n",
        "<tbody>\n",
        "<tr>\n",
        "<td>MobileNet V1</td>\n",
        "<td>569</td>\n",
        "<td>4.24</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>MobileNet V2</td>\n",
        "<td>300</td>\n",
        "<td>3.47</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "\n",
        "<br>Do porównania wykorzystano dane z <a href=\"https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet_v1.md\">Github MNV1</a> oraz  <a href=\"https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet\">Github MNV2</a>. Modele w wersji z depth multiplier równe 1.0. Dla podanej tabeli, mniejsze liczby oznaczają lepszy wynik.\n",
        "\n",
        "\"MACs - multiply-accumulate operations\" określa ile obliczeń jest wymaganych dla wykonania operacji dla sieci na pojedyńczym obrazie RGB o wymiarach 224x224.\n",
        "\n",
        "\n",
        "Przewagi V2: \n",
        "1. Prawie dwa razy mniejszą ilość obliczeń,\n",
        "1. Mniej parametrów (80%) dzięki czemu szybciej ładuje je z pamięci, co jest istotne dla urządzeń mobilnych, gdzie dostęp do pamięci jest znacznie wolniejszy niż obliczenia.\n",
        "\n",
        "<br>Poniższa tabela pokazuje maksymalny FPS (frames-per-second) dla różnych urządzeń dla wersji modeli jak wyżej:</p>\n",
        "\n",
        "<table>\n",
        "<thead>\n",
        "<tr>\n",
        "<th>Version</th>\n",
        "<th>iPhone 7</th>\n",
        "<th>iPhone X</th>\n",
        "<th>iPad Pro 10.5</th>\n",
        "</tr>\n",
        "</thead>\n",
        "\n",
        "<tbody>\n",
        "<tr>\n",
        "<td>MobileNet V1</td>\n",
        "<td>118</td>\n",
        "<td>162</td>\n",
        "<td>204</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>MobileNet V2</td>\n",
        "<td>145</td>\n",
        "<td>233</td>\n",
        "<td>220</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "\n",
        "<br> Wyniki uzyskano przy zastosowaniu optymalizacji polegającej na równoczesnym przygotowaniu danych wejściowych na CPU w trakcie przetwarzania danych na GPU.\n",
        "\n",
        "<br>Porównanie jakości pracy modeli:\n",
        "\n",
        "<table>\n",
        "<thead>\n",
        "<tr>\n",
        "<th>Version</th>\n",
        "<th>Top-1 Accuracy</th>\n",
        "<th>Top-5 Accuracy</th>\n",
        "</tr>\n",
        "</thead>\n",
        "\n",
        "<tbody>\n",
        "<tr>\n",
        "<td>MobileNet V1</td>\n",
        "<td>70.9</td>\n",
        "<td>89.9</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>MobileNet V2</td>\n",
        "<td>71.8</td>\n",
        "<td>91.0</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1db0W86bvR3r",
        "colab_type": "text"
      },
      "source": [
        "## WNIOSKI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0SpMDIRkxqy",
        "colab_type": "text"
      },
      "source": [
        "**MobileNet V2 ma lepsze wynik od MobileNet V1 dla wszyskich wymienionych miar. Zarówno jest szybszy jak również ma lepszą dokładaność dla klasyfikacji.**\n",
        "Wykazano, że MobileNet może być z powodzeniem stosowany do detekcji obiektów i segmentacji obrazów.\n",
        "\n",
        "<br>Dla klasyfikacji ostatni blok wygląda jak na rysunku poniżej:\n",
        "\n",
        "<div align=\"center\"><br>\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/MN2/MN2_Classifier.png?raw=1\" alt=\"Classifier\" width=\"600\" >\n",
        "</div>\n",
        "\n",
        "\n",
        "<br>Użycie MobileNet w SSDlite do detekcji, może wyglądać następująco:\n",
        "\n",
        "\n",
        "<div align=\"center\"><br>\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/MN2/MN2_FeatureExtractor.png?raw=1\" alt=\"Feature Extractor\" width=\"600\" ><br>\n",
        "</div>\n",
        "\n",
        "<br>W powyższym rozwiązaniu wykorzystano nie tylko cechy z ostatniej warstwy MobileNet, ale również kilka wcześniejszych. Na pokazanym przykładzie MobileNet wykorzystywany jest jako ekstraktor cech obrazu.<br><br><br>\n",
        "\n",
        "\n",
        "**W dalszej części pracy zastosowano MobileNet V2 w detekcji twarzy w modelu Detectron2 z bardzo dobrym wynikiem.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJHnrRWCwCiI",
        "colab_type": "text"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLaa3YA6bz9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import BatchNorm2d\n",
        "from detectron2.layers import Conv2d, FrozenBatchNorm2d, ShapeSpec\n",
        "from detectron2.modeling.backbone.build import BACKBONE_REGISTRY\n",
        "from detectron2.modeling.backbone import Backbone\n",
        "from detectron2.modeling.backbone.fpn import FPN, LastLevelMaxPool\n",
        "\n",
        "from detectron2.layers import get_norm\n",
        "\n",
        "class Mish(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "  \n",
        "    def forward(self, x):\n",
        "        x = x * (torch.tanh(nn.functional.softplus(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "def conv_bn(inp, oup, stride,norm='BN',ReL=True):\n",
        "    return nn.Sequential(\n",
        "        Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
        "        get_norm(norm,oup), #zamiast FrozenBatchNorm2d(oup),\n",
        "        nn.ReLU6(inplace=True) if ReL else Mish()\n",
        "    )\n",
        "\n",
        "\n",
        "def conv_1x1_bn(inp, oup,norm='BN',ReL=True):\n",
        "    return nn.Sequential(\n",
        "        Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
        "        get_norm(norm,oup),\n",
        "        nn.ReLU6(inplace=True) if ReL else Mish()\n",
        "    )\n",
        "\n",
        "\n",
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(self, inp, oup, stride, expand_ratio,norm='BN',ReL=True):\n",
        "        super(InvertedResidual, self).__init__()\n",
        "        self.stride = stride\n",
        "        assert stride in [1, 2]\n",
        "\n",
        "        hidden_dim = int(round(inp * expand_ratio))\n",
        "        self.use_res_connect = self.stride == 1 and inp == oup\n",
        "\n",
        "        if expand_ratio == 1:\n",
        "            self.conv = nn.Sequential(\n",
        "                # dw\n",
        "                Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
        "                get_norm(norm,hidden_dim),\n",
        "                nn.ReLU6(inplace=True) if ReL else Mish(),\n",
        "                # pw-linear\n",
        "                Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                get_norm(norm,oup),\n",
        "            )\n",
        "        else:\n",
        "            self.conv = nn.Sequential(\n",
        "                # pw\n",
        "                Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
        "                get_norm(norm,hidden_dim),\n",
        "                nn.ReLU6(inplace=True) if ReL else Mish(),\n",
        "                # dw\n",
        "                Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
        "                get_norm(norm,hidden_dim),\n",
        "                nn.ReLU6(inplace=True) if ReL else Mish(),\n",
        "                # pw-linear\n",
        "                Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                get_norm(norm,oup),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_res_connect:\n",
        "            return x + self.conv(x)\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class MobileNetV2(Backbone):\n",
        "    \"\"\"\n",
        "    Should freeze bn\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg, n_class=1000, input_size=224, width_mult=1.):\n",
        "        super(MobileNetV2, self).__init__()\n",
        "        block = InvertedResidual\n",
        "        input_channel = 32\n",
        "        norm=cfg.MODEL.MOBILENET.NORM\n",
        "        ReL=True\n",
        "        if 'ACTIVATION' in cfg.MODEL.MOBILENET.keys():\n",
        "          if cfg.MODEL.MOBILENET.ACTIVATION == 'Mish': ReL = False\n",
        "          \n",
        "        interverted_residual_setting = [\n",
        "            # t, c, n, s\n",
        "            [1, 16, 1, 1],\n",
        "            [6, 24, 2, 2],\n",
        "            [6, 32, 3, 2],\n",
        "            [6, 64, 4, 2],\n",
        "            [6, 96, 3, 1],\n",
        "            [6, 160, 3, 2],\n",
        "            [6, 320, 1, 1],\n",
        "        ]\n",
        "\n",
        "        # building first layer\n",
        "        assert input_size % 32 == 0\n",
        "        input_channel = int(input_channel * width_mult)\n",
        "        self.return_features_indices = [3, 6, 13, 17]\n",
        "        self.return_features_num_channels = []\n",
        "        self.features = nn.ModuleList([conv_bn(3, input_channel, 2,norm,ReL=ReL)])\n",
        "        # building inverted residual blocks\n",
        "        for t, c, n, s in interverted_residual_setting:\n",
        "            output_channel = int(c * width_mult)\n",
        "            for i in range(n):\n",
        "                if i == 0:\n",
        "                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t,norm=norm,ReL=ReL))\n",
        "                else:\n",
        "                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t,norm=norm,ReL=ReL))\n",
        "                input_channel = output_channel\n",
        "                if len(self.features) - 1 in self.return_features_indices:\n",
        "                    self.return_features_num_channels.append(output_channel)\n",
        "\n",
        "        self._initialize_weights()\n",
        "        self._freeze_backbone(cfg.MODEL.BACKBONE.FREEZE_AT)\n",
        "\n",
        "    def _freeze_backbone(self, freeze_at):\n",
        "        for layer_index in range(freeze_at):\n",
        "            for p in self.features[layer_index].parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = []\n",
        "        for i, m in enumerate(self.features):\n",
        "            x = m(x)\n",
        "            if i in self.return_features_indices:\n",
        "                res.append(x)\n",
        "        return {'res{}'.format(i + 2): r for i, r in enumerate(res)}\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, (2. / n) ** 0.5)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                n = m.weight.size(1)\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "\n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3E5sMsivlqW",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "\n",
        "< [5.1. Detectron2 Faster R-CNN z FPN Resnet50](05_01_DETECTRON2.ipynb) | [5.2. MOBILENETV2](05_02_MOBILENETV2.ipynb) | [5.3. Techniki szybkiego i stabilnego uczenia GSN.](05_03_FrozenBN_Mish.ipynb) >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkcfdtBsPdO3",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "[Spis treści](https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Praca_Dyplomowa.ipynb) | [1. Wstęp](01_00_Wstep.ipynb) | [2. Metryki oceny detekcji](02_00_Miary.ipynb) | [3. Bazy danych](03_00_Datasety.ipynb) | [4. Przegląd metod detekcji](04_00_Modele.ipynb) | [5. Detekcja twarzy z wykorzystaniem wybranych architektur GSN](05_00_Modyfikacje.ipynb) | [6. Porównanie modeli](06_00_Porownanie.ipynb) | [7. Eksport modelu](07_00_Eksport_modelu.ipynb) | [8. Podsumowanie i wnioski](08_00_Podsumowanie.ipynb) | [Bibliografia](Bibliografia.ipynb)\n",
        "\n",
        "\n",
        "---\n"
      ]
    }
  ]
}