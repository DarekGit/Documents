{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.1"
    },
    "colab": {
      "name": "04_00_Modele.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarekGit/FACES_DNN/blob/master/notebooks/04_00_Modele.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en5qybg3KWoT",
        "colab_type": "text"
      },
      "source": [
        "### [Spis treści](https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Praca_Dyplomowa.ipynb)\n",
        "[3. Bazy danych](03_00_Datasety.ipynb) \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrhTv2Ym0Cb5",
        "colab_type": "text"
      },
      "source": [
        "## <b>4. Przegląd metod detekcji</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iUdxuWnWgDM",
        "colab_type": "text"
      },
      "source": [
        "W kolejnych podrozdziałach zostaną omowione metody detekcji z wykorzystaniem współczesnych architektur głębokich sieci neuronowych.\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw_4nFgPayVo",
        "colab_type": "text"
      },
      "source": [
        "### 4.1. YOLO\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ3J5W18a20x",
        "colab_type": "text"
      },
      "source": [
        "YOLO (You Only Look Once) to system detekcji obiektów stworzony do działania w czasie rzeczywistym. W wersji V1 obraz wejściowy dzielony jest na siatkę S x S. Dla pojedyńczej komórki siatki przewidywany jest tylko jeden obiekt. Dla każdej komórki tworzonych jest k masek (boundary boxes) zawierających współrzędne maski i box confidence score określającym prawdopodobieństwo, że maska zawiera obiekt oraz dokładność maski. Dla każdej komórki wyliczane są również prawdopobonieństwa przynależności do klasy.\n",
        "\n",
        "<br>\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/DarekGit/FACES_DNN/master/Figures/yolo.jpg\" alt=\"YOLO\" width=\"600\" >\n",
        "<br>\n",
        "Rys. 1. Schemat działania detektora YOLO. <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[17]</a>\n",
        "</div>\n",
        "<br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXLr1TN7ghJh",
        "colab_type": "text"
      },
      "source": [
        "### Historia YOLO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAZpOsomiv1_",
        "colab_type": "text"
      },
      "source": [
        "YOLO V1 jest prostą siecią konwolucyjną przedstawioną na rysunku poniżej:\n",
        "<div align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/DarekGit/FACES_DNN/master/Figures/YoloV1.jpg\" alt=\"YOLO V1\" width=\"800\" >\n",
        "</div>\n",
        "<br>\n",
        "\n",
        "Końcowym wyjściem naszej sieci jest tensor 7 × 7 × 30\n",
        "prognoz. Dzięki siatce 7x7 sieć przewiduje 98 obwiedni na obraz i prawdopodobieństwa klas dla każdej predykcji. Dla ImageNet 2012 uzyskano dokładność top-5 na poziomie 88%. Największą zaletą YOLO jest prędkość działania na poziomie 45 FPS, oraz 155 FPS dla Fast YOLO.\n",
        "\n",
        "<br>Więcej inormacji dostępnych jest na stronie: [arxiv.org](https://arxiv.org/pdf/1506.02640.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwOHQD36mbFa",
        "colab_type": "text"
      },
      "source": [
        "YOLO V2 wprowadzono modyfikacje umożliwiające detekcję 9000 obiektów oraz zestaw modyfikacji poprawiających jakość detekcji. Rysunek poniżej przedstawia wpływ poszczególnych modyfikacji na jakość detekcji:\n",
        "<div align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/DarekGit/FACES_DNN/master/Figures/YoloV2.png\" alt=\"YOLO V1\" width=\"700\" >\n",
        "</div>\n",
        "<br>\n",
        "\n",
        "Dla YOLOV2 przy 67 FPS, uzyskano mAP 76.8%  dla VOC 2007, oraz 78,6% przy 40 FPS.\n",
        "\n",
        "<br>Więcej inormacji dostępnych jest na stronie: [arxiv.org](https://arxiv.org/pdf/1612.08242.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8n--T3TqNh9",
        "colab_type": "text"
      },
      "source": [
        "YOLO V3 jest już bardziej złożoną siecią, zastosowano w niej rozwiązanie podobne do FPN oraz strukturę typy ResNet. Prędkość działania zależy od wielkości modelu zgodnie z rysunkiem poniżej:\n",
        "<div align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/DarekGit/FACES_DNN/master/Figures/YoloV3.png\" alt=\"YOLO V3\" width=\"600\" >\n",
        "</div>\n",
        "<br>\n",
        "\n",
        "YOLO V3 jest 1000 razy szybsze od R-CNN i 100 razy szybsze od  Fast R-CNN. Dla YOLO V3 uzyskano wynik mAP50 na poziomie 58% i był to najlepszy wynik dla dostępnych sieci przy zachowaniu jednocześnie najwyższej prędkości.\n",
        "\n",
        "<br>Więcej inormacji dostępnych jest na stronie: [arxiv.org](https://arxiv.org/pdf/1804.02767.pdf)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTP5htVeuxDT",
        "colab_type": "text"
      },
      "source": [
        "YOLO V4. \n",
        "<br>W tej wersji wprowadzono zestaw usprawnień, wśród których należy wymienić:\n",
        "- Activations: ReLU, leaky-ReLU, parametric-ReLU,\n",
        "ReLU6, SELU, Swish, or Mish\n",
        "- Bounding box regression loss: MSE, IoU, GIoU, CIoU, DIoU\n",
        "- Data augmentation: CutOut, MixUp, CutMix\n",
        "- Regularization method: DropOut, DropPath,Spatial DropOut, or DropBlock\n",
        "- Normalizacja aktywacji sieci: Batch Normalization (BN), Cross-GPU Batch Normalization (CGBN or SyncBN), Filter Response Normalization (FRN), Cross-Iteration Batch Normalization (CBN) [89]\n",
        "• Skip-connections: Residual connections, Weighted residual connections,Multi-input weighted residual connections, or Cross stage partial connections (CSP)\n",
        "\n",
        "<br>Wpływ wybranych usprawnień na wynik pokazuje rysunek poniżej: \n",
        "<div align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/DarekGit/FACES_DNN/master/Figures/YoloV4.png\" alt=\"YOLO V3\" width=\"600\" >\n",
        "</div>\n",
        "<br>\n",
        "\n",
        "\n",
        "<b>Dla YOLO V4 uzyskano mAP 43,5% na COCO przy prędkości 65 FPS!\n",
        "<br> Cześć usprawnień z powodzeniem wykorzystano w dalszej częsci pracy. </b>\n",
        "\n",
        "<br>Więcej inormacji dostępnych jest na stronie: [arxiv.org](https://arxiv.org/pdf/2004.10934.pdf)\n",
        "\n",
        "Obszerne podsumowanie zmian V1-V4 dostępne jest na sronie: [programmersought.com](https://programmersought.com/article/83764384231/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tcb0Ip6p3FeX",
        "colab_type": "text"
      },
      "source": [
        "YOLO V5\n",
        "<br>Pierwsza oficjalna wersja YOLO V5 została opublikowana przez Ultralytics 25 czerwca 2020 roku w Pytorchu.\n",
        "Dzięki procedusze trenowania Pytorch Yolo V5 zawdzięcza poprawę wydajności. Architektura sieci jest bardzo zbliżona do YOLO V4.\n",
        "<br><b>Główną zaletą YOLO V5 jest łatwość użycia.</b>\n",
        "<br>Porównanie dla YOLO V5:\n",
        "<div align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/DarekGit/FACES_DNN/master/Figures/YoloV5.jpg\" alt=\"YOLO V5\" width=\"700\" >\n",
        "</div>\n",
        "<br>\n",
        "\n",
        "Wykorzystano materiały ze strony: [YOLO V5 improvements](https://blog.roboflow.com/yolov5-improvements-and-evaluation/)\n",
        "<br>Oficjalna strona [YOLO V5 Github](https://github.com/ultralytics/yolov5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlmi5XEBD3ah",
        "colab_type": "text"
      },
      "source": [
        "YOLO PP\n",
        "<br>PP jest skrótem od PaddlePaddle (deep learning framework written by Baidu).\n",
        "<br>Zestawienie usprawnień dla YOLO PP znajduje się na rysunku poniżej:\n",
        "<div align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/DarekGit/FACES_DNN/master/Figures/YoloPP.jpg\" alt=\"YOLO PP\" width=\"600\" >\n",
        "</div>\n",
        "<br>\n",
        "\n",
        "\n",
        "<br>Ostatecznie model PP-YOLO poprawił mAP na COCO z 43.5% to 45.2% dla prędkości większej od YOLO V4.\n",
        "<br>Więcej informacji dostępnych jest na stronie: [arxiv.org](https://arxiv.org/pdf/2007.12099.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_YQo4ab6hIY",
        "colab_type": "text"
      },
      "source": [
        "### 4.2. SSD (Single Shot Multibox Detector) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7FDgrK0a2QO",
        "colab_type": "text"
      },
      "source": [
        "Single Shot Detector to metoda wykrywania obiektów na obrazach za pomocą jednej głębokiej sieci neuronowej. Sieć detektorów łączy prognozy z wielu map obiektów o różnych rozdzielczościach, aby w naturalny sposób obsługiwać obiekty o różnych rozmiarach.\n",
        "<br>\n",
        "<br>\n",
        "<div align=\"center\">\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/SSD.png?raw=1\" alt=\"SSD\" width=\"800\" >\n",
        "\n",
        "Rys. 2. Architektura Single Shot Multibox Detector <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[18]</a>\n",
        "</div>\n",
        "\n",
        "Zalety SSD:\n",
        "\n",
        "* Sieć SSD całkowicie eliminuje generowanie propozycji i kolejne etapy ponownego próbkowania pikseli lub funkcji i łączy wszystkie obliczenia w jednej sieci.\n",
        "* Łatwy do przeszkolenia i prosty do zintegrowania z innymi systemami.\n",
        "* SSD ma konkurencyjną dokładność w stosunku do metod, które wykorzystują dodatkowy krok propozycji obiektu, i jest znacznie szybszy, zapewniając jednolitą strukturę zarówno dla szkolenia, jak i detekcji.\n",
        "\n",
        "<b>Uwaga:</b><br>\n",
        "W związku z wykorzystaniem warstwy FC o stałym rozmiarze do klasyfikacji obiektów, rozmiar wejściowy obrazu musi być w określonym rozmiarze dla danej sieci.\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxEYKUQIBjhg",
        "colab_type": "text"
      },
      "source": [
        "### 4.3. R-CNN (Region-based Convolutional Neural Networks) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgmd6HB3UTrR",
        "colab_type": "text"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/DarekGit/FACES_DNN/master/Figures/RCNN.jpg\" alt=\"Architektura R-CNN\" width=800><br>\n",
        "\n",
        "\n",
        "Rys. 3. Architektura R-CNN. <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[8]</a>\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z046lEhNPlq",
        "colab_type": "text"
      },
      "source": [
        "W pierwszym kroku R-CNN wykorzystując algorytm selektywnego wyszukiwana w obrębie obrazu zostaje wyodrębnione 2000 regionów zainteresowania. Algorytm tworzy grupę dla pojedyńczych pikseli obrazu a następnie oblicza teksturę dla każdej grupy i łączymy dwie najbliższe. \n",
        "Podobne regiony są łączone, aby utworzyć większe regiony w oparciu o podobieństwo kolorów, tekstur, rozmiarów i zgodności kształtu. Proces jest kontunuowany aż do utworzenia regionów o ostatecznej lokalizacji obiektów \n",
        "Rys. Przedstawia powiększanie regionów a niebieskie prostokąty odpowiadające im RoI (region of interest)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMlDvefWMFz0",
        "colab_type": "text"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/DarekGit/FACES_DNN/master/Figures/Selective_Search.jpg\" alt=\"Selective Search\" width=500 ><br>\n",
        "\n",
        "\n",
        "Rys. 4. Wyszukiwanie selektywne i regiony zainteresowania. <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[9]</a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybOK2IcaMHDS",
        "colab_type": "text"
      },
      "source": [
        "Wyodrębnione regiony te mogą zawierać obiekty docelowe i mają różne rozmiary, dlatego są przekształcane na obrazy o ustalonym rozmiarze. Następnie są one przetwarszane przez klasyfikator CNN. W R-CNN do tego celu używany jest model (np. VGG, ResNet) wytrenowany na dużym zbiorze danych o dużej ilości klas (np. ImageNet dataset). Otrzymany wektor cech jest oceniany klasyfikatorem SVM, aby zidentyfikować przynależność do klasy a regresor liniowy przewiduje przesunięcia ramki oznaczenia obiektu względem pierwotnego wskaźnika obiektu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk9hxFVDT6eF",
        "colab_type": "text"
      },
      "source": [
        "Ograniczenia i wady R-CNN.\n",
        "\n",
        "Mimo zmniejszenia liczby regionów do 2000 na obraz, klasyfikacja tych regionów zajmuje ogromną ilość czasu. Zastosowanie algorytmu w aplikacjach czasu rzeczywistego nie było zatem możliwe. \n",
        "\n",
        "Algorytm wyszukiwania selektywnego jest algorytmem dla którego nie następuje uczenie. Skuteczność algorytmu nie poprawia się w trakcie trenowania. Algorytm nie jest dokładny i może generować złe propozycje regionów.\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc9Ega416dL9",
        "colab_type": "text"
      },
      "source": [
        "### 4.4. Fast R-CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81O83x0FW73i",
        "colab_type": "text"
      },
      "source": [
        "Fast R-CNN jest poprawionym algorytmem R-CNN działającym szybciej i zwracającym dokładniejszą detekcję obrazów. \n",
        "\n",
        "Na wejście CNN podawany jest cały obraz. Zestawiając otrzymaną mapę cech z propozycją regionów zewnetrznych realizowaną za pomocą wyszukiwania selektywnego otrzymywane są regiony zainteresowania. Następnie są one przekształcane do określonego rozmiaru przez RoI pooling layer. Warstwa ta generuje wektory cech o stałej długości propozycji regionów. Przesyłane są one do w pełni połączonych warstw (fully connected layers)w celu klasyfikacji z wykorzystaniem funkcji softmax i lokalizacji  regresorem liniowym, który zwraca ramki oznaczenia obiektu.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0rqfKBvA0iC",
        "colab_type": "text"
      },
      "source": [
        "<div align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/DarekGit/FACES_DNN/master/Figures/fast-RCNN.jpg\" alt=\"fast-RCNN\" width=500> \n",
        "\n",
        "Rys. 5. Architektura Fast R-CNN. <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[10]</a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mN7nEDZW780",
        "colab_type": "text"
      },
      "source": [
        "W porównaniu do R-CNN udoskonalony Fast R-CNN jest znacznie szybszy zarówno w trakcie treningu jak i testu. W czasie treningu z wykorzystaniem VGG16 był 9-krotnie szybszy. 213 razy szybszy w czasie testu. Osiągnął wyższe mAP na PASCAL VOC2012. <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[10]</a>  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJAOdgwcOxkq",
        "colab_type": "text"
      },
      "source": [
        "### W modelu zastosowano następującą funkcję straty \n",
        "\n",
        "<br>\n",
        "Funkcja stratu jest suma kosztu klasyfikacji i predykcji ramki oznaczenia obiektu $L=L_{cls}+L_{box}$\n",
        "\n",
        "$L(p,u,t^{u},v)=L_{cls}(p,u)+1[u>=1]L_{box}(t^{u},v)$\n",
        "\n",
        "$L_{box}$ tła jest pomijany: $1[u>=1]\\begin{cases}1 & if u \\geq 1\\\\0 & otherwise\\end{cases}$\n",
        "\n",
        "\n",
        "\n",
        "$L_{cls}(p,u)=-\\log p_{u}$\n",
        "\n",
        "$L_{box}=\\sum_{i\\in \\left\\{x,y,w,h\\right\\} }^n L_1^{smooth}(t_i^u-v_{i})$\n",
        "\n",
        "$L_1^{smooth}(x)=\\begin{cases}0.5x^2 & |x| < 1\\\\|x|-0.5 & otherwise\\end{cases}$\n",
        "\n",
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqeF1VJTW8CY",
        "colab_type": "text"
      },
      "source": [
        "### 4.5. Faster R-CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32KgPApOW8E-",
        "colab_type": "text"
      },
      "source": [
        "Podobnie jak w przypadku Fast-RCNN na wejście CNN podawany jest cały obraz i otrzymywana jest mapa cech. W przypadku Faster R-CNN za znajdowanie regionów odpowiedzialna jest sieć neuronowa Region Proposal Network. Propozycje regionów przekazywane są do RoI pooling a następnie na warstwy w pełni połączone. W celu przypisania do klasy wykorzystywana jest funkcji softmax natomiast regresor liniowy zwraca ramki oznaczenia obiektu.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6MAuOsUpQVt",
        "colab_type": "text"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/DarekGit/FACES_DNN/master/Figures/Faster-RCNN.jpg\" alt=\"Architektura Faster R-CNN\" width=400><br>\n",
        "\n",
        "\n",
        "Rys. 6. Architektura Faster R-CNN. <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[11]</a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_xBrbiApQYN",
        "colab_type": "text"
      },
      "source": [
        "RPN pobiera mapy cech wyjściowych z sieci konwolucyjnej jako dane wejściowe. \n",
        "W czasie przesuwania małego okna n x n przewidywanych jest k regionow o różnych skalach i proporcjach dzięki czemu na wyjściu otrzymywanych jest 4k wspołrzędnych i 2k odpowiadających im ocen, które szacują prawdopodobieństwo wystąpienia lub braku przedmiotu dla każdej propozycji. Autorzy wprowadzają tu pojęcie kotwicy (anchor) będącej centralnym punktem okna. Dla 3 różnych skali i 3 proporcji uzyskuje się 9 zakotwiczeń.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-ivE66EpQay",
        "colab_type": "text"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/DarekGit/FACES_DNN/master/Figures/RPN.png\" alt=\"Region Proposal Network\" width=500 ><br>\n",
        "\n",
        "\n",
        "Rys. 7. Region Proposal Network. <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[11]</a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISqC3jNYVXsR",
        "colab_type": "text"
      },
      "source": [
        "Szczegołowy opis Faster R-CNN w środowisku Detectron 2 został zamieszczony w pliku: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsdvhhQepQdc",
        "colab_type": "text"
      },
      "source": [
        "Funkcja straty.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "$L(\\left\\{p_{i}\\right\\},\\left\\{t_{i}\\right\\}) =\\frac{1}{N_{cls}}\\sum\\limits_i L_{cls}(p_{i}, p_i^*)+\\lambda \\frac{1}{N_{reg}}\\sum\\limits_i L_{reg}(t_{i}, t_i^*)$\n",
        "\n",
        "gdzie: \n",
        "i - anchor indeks,\n",
        "$p_{i}$ - prawdopodobieństwo, że anchor reprezentuje obiekt, \n",
        "$p_i^*$ - ground-truth label (przyjmuje wartość 1 lub 0),\n",
        "$t_{1}$ - reprezentuje wektor wspołrzędnych przewidywanego obiektu,\n",
        "$t_i^*$ - ground-truth box.\n",
        "\n",
        "\n",
        "$L_{reg}(t_{i}, t_i^*)=L_1^{smooth}(t_{i}, t_i^*)$\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJVSXNsCW44r",
        "colab_type": "text"
      },
      "source": [
        "### 4.6. RetinaNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXapB-LqYh5z",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "https://arxiv.org/pdf/1612.03144.pdf\n",
        "\n",
        "Lin, Tsung-Yi, et al. \"Feature pyramid networks for object detection.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.\n",
        "\n",
        "\n",
        "https://arxiv.org/pdf/1708.02002.pdf\n",
        "\n",
        "Lin, Tsung-Yi, et al. \"Focal loss for dense object detection.\" Proceedings of the IEEE international conference on computer vision. 2017.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-hqOxPCAoLM",
        "colab_type": "text"
      },
      "source": [
        "### 4.7. DE⫶TR (DEtection TRansformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdbFeZNXAuBo",
        "colab_type": "text"
      },
      "source": [
        "DETR <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[12]</a> jest nowym i bardzo ciekawym rozwiązaniem, ale  jeszcze na bardzo wstępnym etapie rozwoju. W modelu DETR, algorytmy przygotowane ręcznie (NMS, Anchor Boxes,..) zastąpione zostały modułem transformer. Dla DeTr z backbone  ResNet-50 z ekstrakcją cech uzyskano  <b>mAP  42% </b> na zbiorze COCO przy użyciu połowy mocy obliczeniowej (FLOP) i tej samej liczby parametrów co Faster RCNN.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/DETR.png?raw=1\" alt=\"DETR\" width=\"800\">\n",
        "<br>\n",
        "Rys. 8. Schemat działania DETR <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[12]</a>\n",
        "</div>\n",
        " \n",
        "\n",
        "\n",
        "<br>W przeciwieństwie do tradycyjnych technik detekcji obrazów, DETR traktuje wykrywanie obiektów jako problem bezpośredniego przewidywania zbioru. Model wykorzystuje do przewidywania dopasowanie dwudzielne (bipartite matching), oraz architekturę kodera-dekodera  transformer.\n",
        "Biorąc pod uwagę ustalony mały zestaw wyuczonych zapytań dotyczących obiektów, DETR uzasadnia relacje obiektów i globalny kontekst obrazu, aby bezpośrednio równolegle wyprowadzić końcowy zestaw prognoz. Ze względu na ten równoległy charakter DETR jest bardzo szybki i wydajny.\n",
        "\n",
        "\n",
        "**Problemy z treningiem.** \n",
        "<br>\n",
        "\n",
        "Dla 150 epok (3 dni treningu na pojedynczej karcie) uzyskano AP/AP50 39.5%/60.3% dla zbioru COCO 2017.\n",
        "\n",
        "<b>Niestety autorzy zgłaszają, iż sposób trenowania jest nadal nieprzewidywalny  i wymaga dalszych prac</b>.\n",
        "<br>W przypadku Wider Face, ilość klas ustawiono na 1, a ilość obiektów na 2000.<br>\n",
        "<b>Trenowanie ponad 200 epok nie przyniosło oczekiwanych resultatów.</b>\n",
        "<br>\n",
        "<br>Przykład trenowania DETR na bazie Detectron2 został zamieszczony w pliku [04_DeTr_R50.ipynb](https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/04_DeTr_R50.ipynb)\n",
        "\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czMEL3avAuHK",
        "colab_type": "text"
      },
      "source": [
        "### 4.8. MTCNN (Multi-task Cascaded Convolutional Networks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIUSJgA3AuKI",
        "colab_type": "text"
      },
      "source": [
        "Sieć MTCNN <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[13]</a> wykorzystywana jest do detekcji twarzy. Zbudowana jest jako trzystopniowe kaskada głębokich sieci konwolucyjnych (Rys. x )\n",
        "<br>\n",
        "<br> \n",
        "<div align=\"center\">\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/MTCNN.png?raw=1\" alt=\"MTCNN\" width=500>\n",
        "<br>\n",
        "Rys. 9. Schemat działania MTCNN <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[13]</a>\n",
        "</div>\n",
        "<br>\n",
        "\n",
        "W Proposal P-Net okna kandydatów są tworzone za pośrednictwem szybkiej sieci propozycji. W następnym etapie większość kandydatów jest odrzucana za pomocą sieci udoskonalania (Refinement R-Net). W trzecim etapie, sieć wyjściowa (Output O-Net) tworzy ostateczną ramkę ograniczającą i położenie punktów na twarzy Lendmarks. Wykrywanie i ustawianie twarzy w  środowisku jest trudne z powodu różnych pozycji, oświetlenia i okluzji twarzy. Autorzy MTCNN podają bardzo dobry wynik True Positive na poziomie 95% dla zbioru FDDB.\n",
        "<br><BR>\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/MTCNN_result.png?raw=1\" alt=\"MTCNN results\" width=400  height=350>\n",
        "<br>\n",
        "Rys. 10. Porównanie wyników MTCNN na FDDB z innymi modelami.<a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[13]</a>\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "</div>\n",
        "\n",
        "<br>W porównaniu do wcześniejszych sieci jest to bardzo dobry wynik. Równie ważne jest to, że model można użyć w trywialny sposób w dalszych badaniach. \n",
        "<br> <b>Model został wykorzystany jako punkt odniesienia w dalszej częsci pracy.\n",
        "</b>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVxmsXscxjj2",
        "colab_type": "text"
      },
      "source": [
        "### 4.9. Spatial Pyramid Pooling (SPP-net) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kN-7V5oxjsb",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Spatial Pyramid Pooling (SPP-net) <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[15]</a> to architektura, która może generować reprezentację o stałej długości niezależnie od rozmiaru / skali obrazu. SPP-net poprawia metody klasyfikacji obrazów oparte na CNN. Korzystając z sieci SPP można obliczyć mapy cech z całego obrazu tylko raz, a następnie połączyć cechy w podobrazach w celu wygenerowania reprezentacji o stałej długości. Ta metoda pozwala uniknąć wielokrotnego obliczania cech konwolucyjnych.\n",
        "<br><br>\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/SPP.png?raw=1\" alt=\"SPP\" width=\"500\" >\n",
        "<br>\n",
        "Rys. 11. Spatial pyramidpooling  layer. <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[15]</a>\n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aeJ1hhSxjyy",
        "colab_type": "text"
      },
      "source": [
        "### 4.10. Region-based Fully Convolutional Network (R-FCN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yNwTNs70MCM",
        "colab_type": "text"
      },
      "source": [
        "Region-based Fully Convolutional Networks (R-FCN) to detektor bazujący na regionach do wykrywania obiektów. W przeciwieństwie do innych detektorów opartych na regionach, które stosują kosztowną podsieć ROI, taką jak Fast R-CNN lub Faster R-CNN, ten detektor jest w pełni splotowy, a prawie wszystkie obliczenia są wspólne dla całego obrazu.\n",
        "<br>\n",
        "<br>\n",
        "<div align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/DarekGit/FACES_DNN/master/Figures/RFCN.png\" alt=\"Region-based Fully Convolutional Networks\" width=600 > \n",
        "<br>\n",
        "Rys. 12. Region-based Fully Convolutional Networks <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[16]</a>\n",
        "</div>\n",
        "\t\n",
        "\n",
        "\n",
        "<br> R-FCN składa się ze współdzielonych, w pełni konwolucyjnych warstw, i daje lepsze wyniki niż Faster R-CNN. \n",
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCxpD6hCUL-v",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QhxTW4uqQ2BL"
      },
      "source": [
        "[5. Detekcja twarzy z wykorzystaniem wybranych architektur GSN](05_00_Modyfikacje.ipynb)<br>\n",
        "[3. Bazy danych](03_00_Datasety.ipynb)<br> \n",
        "### [Spis treści](https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Praca_Dyplomowa.ipynb)\n"
      ]
    }
  ]
}