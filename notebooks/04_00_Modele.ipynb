{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.1"
    },
    "colab": {
      "name": "04_00_Modele.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarekGit/FACES_DNN/blob/master/notebooks/04_00_Modele.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en5qybg3KWoT",
        "colab_type": "text"
      },
      "source": [
        "# [Spis treści](https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Praca_Dyplomowa.ipynb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrhTv2Ym0Cb5",
        "colab_type": "text"
      },
      "source": [
        "### [4. Przegląd metod detekcji](04_00_Modele.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iUdxuWnWgDM",
        "colab_type": "text"
      },
      "source": [
        "W kolejnych podrozdziałach zostaną omowione metody detekcji z wykorzystaniem współczesnych architektur głębokich sieci neuronowych.\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw_4nFgPayVo",
        "colab_type": "text"
      },
      "source": [
        "### 4.1. YOLO\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ3J5W18a20x",
        "colab_type": "text"
      },
      "source": [
        "YOLO (You Only Look Once) to system detekcji obiektów stworzony do działania w czasie rzeczywistym. Obraz wejściowy dzielony jest na siatkę n x n. Dla pojedyńczej komórki siatki przewidywany jest tylko jeden obiekt. Dla każdej tworzonych jest k masek (boundary boxes) zawierających współrzędne maski i box confidence score okrelającym prawdopodobieństwo, że maska zawiera obiekt oraz dokładność maski. Dla każdej komórki wyliczane są również prawdopobonieństwa przynależności do klasy.\n",
        "\n",
        "<br>\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/Dets/YOLO.png?raw=1\" alt=\"YOLO\" width=\"800\" >\n",
        "<br>\n",
        "Rys. YOLO. <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[8]</a>\n",
        "</div>\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_YQo4ab6hIY",
        "colab_type": "text"
      },
      "source": [
        "### 4.2. SSD (Single Shot Multibox Detector) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7FDgrK0a2QO",
        "colab_type": "text"
      },
      "source": [
        "Single Shot Detector to metoda wykrywania obiektów na obrazach za pomocą jednej głębokiej sieci neuronowej. Sieć detektorów łączy prognozy z wielu map obiektów o różnych rozdzielczościach, aby w naturalny sposób obsługiwać obiekty o różnych rozmiarach.\n",
        "<br>\n",
        "<br>\n",
        "<div align=\"center\">\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/Dets/SSD.png?raw=1\" alt=\"SSD\" width=\"800\" >\n",
        "\n",
        "Rys. SSD <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[ ? ]</a>\n",
        "</div>\n",
        "\n",
        "\"https://arxiv.org/pdf/1512.02325.pdf%EF%BC%89\"\n",
        "\n",
        "\n",
        "Zalety SSD:\n",
        "\n",
        "* Sieć SSD całkowicie eliminuje generowanie propozycji i kolejne etapy ponownego próbkowania pikseli lub funkcji i łączy wszystkie obliczenia w jednej sieci.\n",
        "* Łatwy do przeszkolenia i prosty do zintegrowania z innymi systemami.\n",
        "* SSD ma konkurencyjną dokładność w stosunku do metod, które wykorzystują dodatkowy krok propozycji obiektu, i jest znacznie szybszy, zapewniając jednolitą strukturę zarówno dla szkolenia, jak i detekcji.\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxEYKUQIBjhg",
        "colab_type": "text"
      },
      "source": [
        "### 4.3. R-CNN (Region-based Convolutional Neural Networks) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgmd6HB3UTrR",
        "colab_type": "text"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/DarekGit/FACES_DNN/master/Figures/RCNN.jpg\" alt=\"Architektura R-CNN\" width=800><br>\n",
        "\n",
        "\n",
        "Rys. Architektura R-CNN. <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[8]</a>\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z046lEhNPlq",
        "colab_type": "text"
      },
      "source": [
        "W pierwszym kroku R-CNN wykorzystując algorytm selektywnego wyszukiwana w obrębie obrazu zostaje wyodrębnione 2000 regionów zainteresowania. Algorytm tworzy grupę dla pojedynczego piksela obrazu a następnie oblicza teksturę dla każdej grupy i łączymy dwie najbliższe. \n",
        "Podobne regiony są łączone, aby utworzyć większe regiony w oparciu o podobieństwo kolorów, tekstur, rozmiarów i zgodności kształtu. Proces jest kontunuowany aż do utworzenia regionów o ostatecznej lokalizacji obiektów \n",
        "Rys. Przedstawie powiekszanie regionów a niebieskie prostokąty odpowiadające im RoI (region of interest)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMlDvefWMFz0",
        "colab_type": "text"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/DarekGit/FACES_DNN/master/Figures/Selective_Search.jpg\" alt=\"Selective Search\" width=600 ><br>\n",
        "\n",
        "\n",
        "Rys. Wyszukiwanie selektywne i regiony zainteresowania. <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[9]</a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybOK2IcaMHDS",
        "colab_type": "text"
      },
      "source": [
        "Wyodrębnione regiony te mogą zawierać obiekty docelowe i mają różne rozmiary dlatego są przekształcane na obrazy o ustalonym rozmiarze. Następnie są one przetwarszane przez klasyfikator CNN. W R-CNN do tego celu używany jest model (np. VGG, ResNet) wytrenowany na dużym zbiorze danych o dużej ilości klas (ImageNet dataset). Otrzymany wektor cech jest oceniany klasyfikatorem SVM, aby zidentyfikować przynależność do klasę a regresor liniowy przewiduje przesunięcia ramki oznaczenia obiektu względem pierwotnego wskaźnika."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk9hxFVDT6eF",
        "colab_type": "text"
      },
      "source": [
        "Ograniczenia i wady R-CNN.\n",
        "\n",
        "Mimo zmniejszenia liczby regionów do 2000 na obraz, klasyfikacja tych regionów zajmuje ogromną ilość czasu. Zastosowanie algorytmu w aplikacjach czasu rzeczywistego nie było zatem możliwe. \n",
        "\n",
        "Algorytm wyszukiwania selektywnego jest algorytmem dla którego nie następuje uczenie. Skuteczność algorytmu nie poprawia się w trakcie trenowania. Algorytm nie jest dokładny i może generować złe propozycje regionów.\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc9Ega416dL9",
        "colab_type": "text"
      },
      "source": [
        "### 4.4. Fast R-CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81O83x0FW73i",
        "colab_type": "text"
      },
      "source": [
        "Fast R-CNN jest poprawionym algorytmem R-CNN działającym szybciej i zwracającym dokładniejszą detekcję obrazów. \n",
        "\n",
        "Na wejście CNN podawany jest cały obraz. Zestawiając otrzymaną mapę cech z propozycją regionów zewnetrznych realizowaną za pomocą wyszukiwania selektywnego otrzymywane są regiony zainteresowania. Następnie są one przekształcane do określonego rozmiaru przez RoI pooling layer. Warstwa ta generuje wektory cech o stałej długości propozycji regionów. Przesyłane są one do w pełni połączonych warstw (fully connected layers)w celu klasyfikacji z wykorzystaniem funkcji softmax i lokalizacji  regresorem liniowym, który zwraca ramki oznaczenia obiektu.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0rqfKBvA0iC",
        "colab_type": "text"
      },
      "source": [
        "<div align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/DarekGit/FACES_DNN/master/Figures/fast-RCNN.jpg\" alt=\"fast-RCNN\"> \n",
        "\n",
        "Rys. Architektura Fast R-CNN. <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[10]</a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mN7nEDZW780",
        "colab_type": "text"
      },
      "source": [
        "W porównaniu do R-CNN udoskonalony Fast R-CNN jest znacznie szybszy zarówno w krakcie treningu jak i testu. W czasie treningu z wykorzystaniem VGG16 był 9-krotnie szybszy. 213 razy szybszy w czasie testu. Osiągnął wyższe mAP na PASCAL VOC2012. <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[10]</a>  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJAOdgwcOxkq",
        "colab_type": "text"
      },
      "source": [
        "### Funkcja straty \n",
        "\n",
        "<br>\n",
        "Funkcja stratu jest suma kosztu klasyfikacji i predykcji ramki oznaczenia obiektu $L=L_{cls}+L_{box}$\n",
        "\n",
        "$L(p,u,t^{u},v)=L_{cls}(p,u)+1[u>=1]L_{box}(t^{u},v)$\n",
        "\n",
        "$L_{box}$ tła jest pomijany: $1[u>=1]\\begin{cases}1 & if u \\geq 1\\\\0 & otherwise\\end{cases}$\n",
        "\n",
        "\n",
        "\n",
        "$L_{cls}(p,u)=-\\log p_{u}$\n",
        "\n",
        "$L_{box}=\\sum_{i\\in \\left\\{x,y,w,h\\right\\} }^n L_1^{smooth}(t_i^u-v_{i})$\n",
        "\n",
        "$L_1^{smooth}(x)=\\begin{cases}0.5x^2 & |x| < 1\\\\|x|-0.5 & otherwise\\end{cases}$\n",
        "\n",
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqeF1VJTW8CY",
        "colab_type": "text"
      },
      "source": [
        "### 4.5. Faster R-CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32KgPApOW8E-",
        "colab_type": "text"
      },
      "source": [
        "Podobnie jak w przypadku Fast-RCNN na wejście CNN podawany jest cały obraz i otrzymywana jest mapa cech. W przypadku Faster R-CNN za znajdowanie regionów odpowiedzialna jest sieć neuronowa Region Proposal Network. Propozycje regionów przekazywane są do RoI pooling a następnie na warstwy w pełni połączone. W celu przypisania do klasy wykorzystywana jest funkcji softmax natomiast regresor liniowy zwraca ramki oznaczenia obiektu.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6MAuOsUpQVt",
        "colab_type": "text"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/DarekGit/FACES_DNN/master/Figures/Faster-RCNN.jpg\" alt=\"Architektura Faster R-CNN\" width=400><br>\n",
        "\n",
        "\n",
        "Rys. Architektura Faster R-CNN. <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[11]</a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_xBrbiApQYN",
        "colab_type": "text"
      },
      "source": [
        "RPN pobiera mapy cech wyjściowych z sieci konwolucyjnej jako dane wejściowe. \n",
        "W czasie przesuwania małego okna n x n przewidywanych jest k regionow o różnych skalach i proporcjach dzięki czemu na wyjściu otrzymywanych jest 4k wspołrzędnych i 2k odpowiadających im ocen, które szacują prawdopodobieństwo wystąpienia lub braku przedmiotu dla każdej propozycji. Autorzy wprowadzają tu pojęcie kotwicy (anchor) będącej centralnym punktem okna. Dla 3 różnych skali i 3 proporcji uzyskuje się 9 zakotwiczeń.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-ivE66EpQay",
        "colab_type": "text"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/DarekGit/FACES_DNN/master/Figures/RPN.png\" alt=\"Region Proposal Network\" width=500 ><br>\n",
        "\n",
        "\n",
        "Rys. Region Proposal Network. <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[11]</a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISqC3jNYVXsR",
        "colab_type": "text"
      },
      "source": [
        "Szczegołowy opis Faster R-CNN w środowisku Detectron 2 został zamieszczony w pliku: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsdvhhQepQdc",
        "colab_type": "text"
      },
      "source": [
        "Funkcja straty.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "$L(\\left\\{p_{i}\\right\\},\\left\\{t_{i}\\right\\}) =\\frac{1}{N_{cls}}\\sum\\limits_i L_{cls}(p_{i}, p_i^*)+\\lambda \\frac{1}{N_{reg}}\\sum\\limits_i L_{reg}(t_{i}, t_i^*)$\n",
        "\n",
        "gdzie: \n",
        "i - anchor indeks,\n",
        "$p_{i}$ - prawdopodobieństwo, że anchor reprezentuje obiekt, \n",
        "$p_i^*$ - ground-truth label (przyjmuje wartość 1 lub 0),\n",
        "$t_{1}$ - reprezentuje wektor wspołrzędnych przewidywanego obiektu,\n",
        "$t_i^*$ - ground-truth box.\n",
        "\n",
        "\n",
        "$L_{reg}(t_{i}, t_i^*)=L_1^{smooth}(t_{i}, t_i^*)$\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJVSXNsCW44r",
        "colab_type": "text"
      },
      "source": [
        "### 4.6. RetinaNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph9H4R12AneM",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXapB-LqYh5z",
        "colab_type": "text"
      },
      "source": [
        "<br><br><br>cdn..<br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-hqOxPCAoLM",
        "colab_type": "text"
      },
      "source": [
        "### 4.6. DETR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdbFeZNXAuBo",
        "colab_type": "text"
      },
      "source": [
        "<BR><BR> cdn.. <BR><BR>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czMEL3avAuHK",
        "colab_type": "text"
      },
      "source": [
        "### 4.6. MTCNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIUSJgA3AuKI",
        "colab_type": "text"
      },
      "source": [
        "<br><br>cdn..<br><BR>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCxpD6hCUL-v",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}