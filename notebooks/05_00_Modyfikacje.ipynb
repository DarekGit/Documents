{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_00_Modyfikacje.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarekGit/FACES_DNN/blob/master/notebooks/05_00_Modyfikacje.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7P4290g5hSCs",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "[Spis treści](https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Praca_Dyplomowa.ipynb) | [1. Wstęp](01_00_Wstep.ipynb) | [2. Metryki oceny detekcji](02_00_Miary.ipynb) | [3 .Bazy danych](03_00_Datasety.ipynb) | [4. Przegląd metod detekcji](04_00_Modele.ipynb) | [5. Detekcja twarzy z wykorzystaniem wybranych architektur GSN](05_00_Modyfikacje.ipynb) | [6. Porównanie modeli](06_00_Porownanie.ipynb) | [7. Eksport modelu](07_00_Eksport_modelu.ipynb) | [8. Podsumowanie i wnioski](08_00_Podsumowanie.ipynb) | [Bibliografia](Bibliografia.ipynb)\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM8CTuKDhSq9",
        "colab_type": "text"
      },
      "source": [
        "## 5. Detekcja twarzy z wykorzystaniem wybranych architektur GSN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQPGN3KghStr",
        "colab_type": "text"
      },
      "source": [
        "W pracy wykorzystano środowisko Detectron2 do wytrenowania modeli opartych na wybranych architekturach. Detectron2 został napisany w PyTorchu. Przez twórców został udostępniony jako biblioteka open-source, która może być wykorzystana do obsługi i rozwoju różnych projektów badawczych."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1HAmfSSVVIe",
        "colab_type": "text"
      },
      "source": [
        "Opis zastosowanych modeli i rozwiązań.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOgjA1M1ViwR",
        "colab_type": "text"
      },
      "source": [
        "### Model faster_rcnn_R_50_FPN_3x "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtuxxOkqVh0K",
        "colab_type": "text"
      },
      "source": [
        "Model pretenowany na zbiorze danych COCO train2017. Nastepnie trenowany na zbiorze treningowym WIDER FACE (270000 iteracji). W czasie treningu zastosowano:\n",
        "\n",
        "* Agumentację\n",
        "  * ResizeShortestEdge(short_edge_length=(640, 672, 704, 736), max_size=1333, sample_style='choice'),\n",
        "  * RandomFlip(),\n",
        "* FrozenBatchNorm2d, \n",
        "* Batch size: 8, \n",
        "* Scheduler: WarmupMultiStepLR\n",
        "* Bazowy LR: 0.02 z liniowym rozgrzewaniem (linear warm-up), zmniejszany dziesieciokrotnie przy 210000 i 250000 iteracji \n",
        "<br><br>\n",
        "\n",
        "Opis zastosowanego rozwiązania został zamieszczony w podrozdziale [5.1. Detectron2 Faster R-CNN z FPN Resnet50](05_01_DETECTRON2.ipynb).\n",
        "<br><br>\n",
        "\n",
        "Zapis trenowania modelu został dołączony do pracy jako załącznik w pliku: <br>\n",
        "[WIDERFACE_faster_rcnn_R_50_FPN_3x.ipynb](WIDERFACE_faster_rcnn_R_50_FPN_3x.ipynb)\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baVRjzQ-Vf7P",
        "colab_type": "text"
      },
      "source": [
        "### Model scratch_faster_rcnn_R_50_FPN_gn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4n1ZPzCWWJYG",
        "colab_type": "text"
      },
      "source": [
        "Model trenowany od zaera na na zbiorze treningowym WIDER FACE (540000 iteracji) na podstawie metod przedstawionych w artykule \"Rethinking ImageNet Pre-training\" <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[15]</a> \n",
        "\n",
        "* Zastosowano agumentację\n",
        "    * ResizeShortestEdge(short_edge_length=(640,), max_size=1333, sample_style='choice'), \n",
        "    * RandomFlip()\n",
        "* Group Normalization <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[16]</a>  - metoda normalizacji nie zależna od rozmiaru batcha \n",
        "* Batch size: 2,\n",
        "* Scheduler: WarmupMultiStepLR\n",
        "* Bazowy learning rate został ustawiony na 0.02 z liniowym rozgrzewaniem (linear warm-up), zmniejszany dzisieciokrotnie przy 480000 i 520000 iteracji.\n",
        "<br><br>\n",
        "\n",
        "Opis zastosowanego rozwiązania został zamieszczony w podrozdziale [5.1. Detectron2 Faster R-CNN z FPN Resnet50](05_01_DETECTRON2.ipynb).\n",
        "<br><br>\n",
        "\n",
        "Zapis trenowania modelu został dołączony do pracy jako załącznik w pliku:<br>[WIDERFACE_scratch_faster_rcnn_R_50_FPN_gn.ipynb](WIDERFACE_scratch_faster_rcnn_R_50_FPN_gn.ipynb)\n",
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20HgLG9ZX88h",
        "colab_type": "text"
      },
      "source": [
        "### Model '800k' "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyzpjCoFYgxJ",
        "colab_type": "text"
      },
      "source": [
        "Zastosowano parametry domyślne Detectron2 - jako punkt odniesienia do dalszych analiz dla backbone MobileNetV2.\n",
        "<br>\n",
        "Zastosowane rozwiązanie wykorzystuję architekturę MobileNet V2 w backbone opisaną w podrozdziale [5.2. MOBILENETV2](05_02_MOBILENETV2.ipynb).\n",
        "* Agumentację\n",
        "  * ResizeShortestEdge(short_edge_length=(640, 672, 704, 736), max_size=1333, sample_style='choice'),\n",
        "  * RandomFlip(),\n",
        "* FrozenBatchNorm2d, \n",
        "* Batch size: 2, \n",
        "* Scheduler: WarmupMultiStepLR\n",
        "* Bazowy LR: 0.001 z liniowym rozgrzewaniem (linear warm-up) do 1000 iteracji, zmniejszany dwukrotnie przy 40k,10k,200k,400k do 800k iteracji \n",
        "<br><br>\n",
        "\n",
        "Zapis trenowania modelu został dołączony do pracy jako załącznik w pliku: <br>\n",
        "[ mobilenet_v2_800k.ipynb]( https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/WIDERFACE_Detectron2_from_scratch_DD_mobilenet_v2_800k.ipynb)\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-xzr-lTYDzo",
        "colab_type": "text"
      },
      "source": [
        "### Model 'BN_800k' \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGVXJh3iYhtU",
        "colab_type": "text"
      },
      "source": [
        "Do modelu wprowadzono Batch Normalization - pokazanie wpływu BN przy małym batchu=2<br>\n",
        "\n",
        "Zastosowane rozwiązanie wykorzystuje architekturę MobileNet V2 w backbone opisaną w podrozdziale [5.2. MOBILENETV2](05_02_MOBILENETV2.ipynb).\n",
        "* Agumentację\n",
        "  * ResizeShortestEdge(short_edge_length=(640, 672, 704, 736), max_size=1333, sample_style='choice'),\n",
        "  * RandomFlip(),\n",
        "* <b>BatchNorm2d,</b> \n",
        "* Batch size: 2, \n",
        "* Scheduler: WarmupMultiStepLR\n",
        "* Bazowy LR: 0.001 z liniowym rozgrzewaniem (linear warm-up) do 1000 iteracji, zmniejszany dwukrotnie przy 40k,10k,200k,400k do 800k iteracji \n",
        "<br><br>\n",
        "\n",
        "Zapis trenowania modelu został dołączony do pracy jako załącznik w pliku: <br>\n",
        "[ mobilenet_v2_BN.ipynb](https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/WIDERFACE_Detectron2_from_scratch_DD_mobilenet_v2_BN.ipynb)\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz3euVMLYalt",
        "colab_type": "text"
      },
      "source": [
        "### 'BN_Mish_V2_250+F_2_50k' "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUyPFdXkYieu",
        "colab_type": "text"
      },
      "source": [
        "Uzyskano najlepszy wynik uczenia dla backbone z MobileNetV2 wykorzystano funkcję Mish, Batch Normalization dla 250k iteracji + 50k z FrozenBN.<br>\n",
        "\n",
        "Zastosowane rozwiązanie wykorzystuje architekturę MobileNet V2 w backbone opisaną w podrozdziale [5.2. MOBILENETV2](05_02_MOBILENETV2.ipynb).\n",
        "* Agumentację\n",
        "  * ResizeShortestEdge(short_edge_length=(640, 672, 704, 736), max_size=1333, sample_style='choice'),\n",
        "  * RandomFlip(),\n",
        "* <b>funkcję aktywacji Mish,</b> \n",
        "* BatchNorm2d,\n",
        "* Batch size: 2, \n",
        "* Scheduler: WarmupMultiStepLR\n",
        "* Bazowy LR: 0.002 z liniowym rozgrzewaniem (linear warm-up) do 1000 iteracji, zmniejszany 0.3 po 150k,210k  do 250k iteracji\n",
        "\n",
        "- <b>douczanie:</b>\n",
        "* <b>FrozenBatchNorm2d,</b>\n",
        "* LR: 4e-6, zmniejszony pięciokrotnie po 30k do 50k iteracji\n",
        "<br><br>\n",
        "\n",
        "Zapis trenowania modelu został dołączony do pracy jako załącznik w pliku: <br>\n",
        "[ mobilenet_v2_Mish_V22F2.ipynb](https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/WIDERFACE_Detectron2_from_scratch_DD_mobilenet_v2_Mish_V22F2.ipynb)\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aCB49OxX8-8",
        "colab_type": "text"
      },
      "source": [
        "### Model 'BN_Mish_V3_80+30k'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fguBB8-AYi84",
        "colab_type": "text"
      },
      "source": [
        "Uzyskano najszybsze uczenie dla backbone z MobileNetV1 - 110k iteracji.<br>\n",
        " \n",
        "Zastosowane rozwiązanie wykorzystuję architekturę MobileNet V2 w backbone opisaną w podrozdziale [5.2. MOBILENETV2](05_02_MOBILENETV2.ipynb).\n",
        "* Agumentację\n",
        "  * ResizeShortestEdge(short_edge_length=(640, 672, 704, 736), max_size=1333, sample_style='choice'),\n",
        "  * RandomFlip(),\n",
        "* <b>funkcję aktywacji Mish,</b> \n",
        "* BatchNorm2d,\n",
        "* Batch size: 2, \n",
        "* Scheduler: WarmupMultiStepLR\n",
        "* Bazowy LR: 0.005 z liniowym rozgrzewaniem (linear warm-up),  do 80k iteracji\n",
        "\n",
        "- <b>douczanie:</b>\n",
        "* LR: 5e-4, zmniejszony dziesięciokrotnie po 25k do 30k iteracji\n",
        "<br><br>\n",
        "\n",
        "Zapis trenowania modelu został dołączony do pracy jako załącznik w pliku: <br>\n",
        "[ mobilenet_v2_Mish_V32.ipynb](https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/WIDERFACE_Detectron2_from_scratch_DD_mobilenet_v2_Mish_V32.ipynb)\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w0IdIaWNTq8",
        "colab_type": "text"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nu9El0sySsWU",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<br>Ewaluacja modeli i porównanie otrzymanych wyników znajduje się w rodziale:<br>\n",
        "[6. Porównanie modeli](06_00_Porownanie.ipynb) \n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2p_2uO3bNUzG",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "[Spis treści](https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Praca_Dyplomowa.ipynb) | [1. Wstęp](01_00_Wstep.ipynb) | [2. Metryki oceny detekcji](02_00_Miary.ipynb) | [3 .Bazy danych](03_00_Datasety.ipynb) | [4. Przegląd metod detekcji](04_00_Modele.ipynb) | [5. Detekcja twarzy z wykorzystaniem wybranych architektur GSN](05_00_Modyfikacje.ipynb) | [6. Porównanie modeli](06_00_Porownanie.ipynb) | [7. Eksport modelu](07_00_Eksport_modelu.ipynb) | [8. Podsumowanie i wnioski](08_00_Podsumowanie.ipynb) | [Bibliografia](Bibliografia.ipynb)\n",
        "\n",
        "\n",
        "---"
      ]
    }
  ]
}