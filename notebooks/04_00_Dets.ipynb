{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_00_Dets.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOwMBvIAwBqfl9wU8lkapa/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarekGit/FACES_DNN/blob/master/notebooks/04_00_Dets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl1gpedgbEWv",
        "colab_type": "text"
      },
      "source": [
        "#<b>Podstawowe Metody Wykrywania Obiektów</b><br>\n",
        "<b>Wykrywanie obiektów </b> jest jedną z dziedzin, która przeżywa gwałtowny rozwój w obszarze sieci neuronowych. Konieczoność jednoczesnego określenia lokalizacji oraz klasy obietków sprawia, że jest to jedno z najtrudniejszych zadań w dziedzinie sieci neuronowych. Model w tym zastosowaniu musi  określić, gdzie znajdują się obiekty na danym obrazie, zwanym lokalizacją obiektu i do której kategorii należy każdy obiekt, czyli klasyfikacją obiektu.<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsDvky1dnK0w",
        "colab_type": "text"
      },
      "source": [
        "###<br><b>1. Histogram of Oriented Gradients (HOG)</b><br>\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/Dets/HOG.png?raw=1\" alt=\"R_CNN\" width=\"600\" >\n",
        "</div>\n",
        "\n",
        "<br>Histogram zorientowanych gradientów (HOG) zasadniczo jest deskryptorem cech, który jest używany do wykrywania obiektów w przetwarzaniu obrazów. Metoda histogramu zorientowanych gradientów obejmuje wykrywanie orientacji gradientu we wskazanych częściach obrazu, metodami typu przesuwane okno wykrywania lub obszar zainteresowania (ROI). Jedną z zalet funkcji podobnych do HOG jest ich prostota. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7yc1faYb_wE",
        "colab_type": "text"
      },
      "source": [
        "###<br><b>2. Region-based Convolutional Neural Networks (R-CNN)</b><br>\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/Dets/R_CNN.png?raw=1\" alt=\"R_CNN\" width=\"800\" >\n",
        "</div>\n",
        "\n",
        "<br>The Region-based Convolutional Network method (RCNN) to połączenie propozycji obszarów występowania obiektów (ROI) z sieciami konwolucyjnymi (CNN). R-CNN wykorzystuje głębokie sieci  neuronowe do lokalizowania obiektów i uczeniu modelu o dużej pojemności przy zastosowaniu  niewielkiej ilości danych wejściowych z adnotacjami. Osiąga wyższą dokładność wykrywania obiektów dzięki zastosowaniu głębokiej sieci ConvNet do klasyfikowania propozycji obiektów. R-CNN ma możliwość skalowania do tysięcy klas obiektów bez wykorzystania metod pomocniczych. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK10oZJmuRFQ",
        "colab_type": "text"
      },
      "source": [
        "###<br><b>3. Single Shot Detector (SSD)</b><br>\n",
        "\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/Dets/SSD.png?raw=1\" alt=\"SSD\" width=\"800\" >\n",
        "</div>\n",
        "\n",
        "<br> Single Shot Detector (SSD) to metoda wykrywania obiektów na obrazach za pomocą jednej głębokiej sieci neuronowej.  Sieć detektorów  łączy prognozy z wielu map obiektów o różnych rozdzielczościach, aby w naturalny sposób obsługiwać obiekty o różnych rozmiarach. <br>\n",
        "\n",
        "Zalety SSD:\n",
        "- Sieć SSD całkowicie eliminuje generowanie propozycji i kolejne etapy ponownego próbkowania pikseli lub funkcji i łączy wszystkie obliczenia w jednej sieci.\n",
        "- Łatwy do przeszkolenia i prosty do zintegrowania z innymi systemami.\n",
        "- SSD ma konkurencyjną dokładność w stosunku do metod, które wykorzystują dodatkowy krok propozycji obiektu, i jest znacznie szybszy, zapewniając jednolitą strukturę zarówno dla szkolenia, jak i detekcji.\n",
        "\n",
        "\n",
        "<br>Więcej informacji dostępnych jest na stronie:<br>\n",
        "<a href=\"https://arxiv.org/pdf/1512.02325.pdf%EF%BC%89\"\n",
        "rel=\"nofollow\">arxiv.org</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sxyz6eBnppE9",
        "colab_type": "text"
      },
      "source": [
        "###<br><b>4. Fast R-CNN</b><br>\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/Dets/Fast_R_CNN.png?raw=1\" alt=\"Fast R_CNN\" width=\"600\" >\n",
        "</div>\n",
        "\n",
        "<br> Napisany w Pythonie i C ++ (Caffe), Fast Region-Based Convolutional Network lub Fast R-CNN to algorytm uczący się do wykrywania obiektów. Algorytm ten eliminuje głównie wady R-CNN i SPPnet, jednocześnie poprawiając ich szybkość i dokładność. <br>\n",
        "\n",
        "<br>Zalety Fast R-CNN:\n",
        "\n",
        "- Wyższa jakość wykrywania (mAP) niż R-CNN, SPPnet\n",
        "- Trening jednoetapowe, z wykorzystaniem wieloelementowej  fuckji straty\n",
        "\n",
        "\n",
        "<br>Więcej informacji na stronie:<br> \n",
        "<a href=\"http://openaccess.thecvf.com/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf\"\n",
        "rel=\"nofollow\">openaccess.thecvf.com</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "324DCa7jsuS0",
        "colab_type": "text"
      },
      "source": [
        "###<br><b>5. Spatial Pyramid Pooling (SPP-net)</b><br>\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/Dets/SPP.png?raw=1\" alt=\"SPP\" width=\"600\" >\n",
        "</div>\n",
        "\n",
        "<br> Spatial Pyramid Pooling (SPP-net) to struktura sieciowa, która może generować reprezentację o stałej długości niezależnie od rozmiaru / skali obrazu.SPP-net poprawia  metody klasyfikacji obrazów oparte na CNN. Korzystając z sieci SPP można obliczyć mapy cech z całego obrazu tylko raz, a następnie połączyć cechy w podobrazach w celu wygenerowania reprezentacji o stałej długości. Ta metoda pozwala uniknąć wielokrotnego obliczania cech konwolucyjnych.\n",
        "\n",
        "<br> Więcej informacji dostępnych jest na stronie:<br>\n",
        "<a href=\"https://arxiv.org/pdf/1406.4729.pdf)%C3%AC%20%CB%9C\"\n",
        "rel=\"nofollow\">arxiv.org</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JhZcpqfwDSr",
        "colab_type": "text"
      },
      "source": [
        "###<br><b>6. Faster R-CNN</strong></b><br>\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/Dets/Faster_R_CNN.png?raw=1\" alt=\"Faster RCNN\" width=\"700\" >\n",
        "</div>\n",
        "\n",
        "<br> Faster R-CNN to algorytm wykrywania obiektów podobny do R-CNN. Algorytm ten wykorzystuje Region Proposal Network (RPN), który współdzieli funkcje splotu pełnego obrazu z siecią detekcyjną bardzije efektywnie niż R-CNN i Fast R-CNN. Sieć propozycji regionów jest siecią konwolucyjną, która jednocześnie przewiduje granice obiektu oraz ocenia klasę obiektu dla każdej propozycji.\n",
        "\n",
        "\n",
        "\n",
        "<br>Więcej informacji dostępnych jest na stronie:<br>\n",
        "<a href=\"http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf\"\n",
        "rel=\"nofollow\">papers.nips.cc</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1gADrvByVis",
        "colab_type": "text"
      },
      "source": [
        "###<br><b>7. Region-based Fully Convolutional Network (R-FCN)</b><br>\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/Dets/R_FCN.png?raw=1\" alt=\"R_FCN\" width=\"700\" >\n",
        "</div>\n",
        "\n",
        "<br> Region-based Fully Convolutional Networks lub R-FCN to  detektor bazujący na regionach do wykrywania obiektów. W przeciwieństwie do innych detektorów opartych na regionach, które stosują kosztowną podsieć ROI, taką jak Fast R-CNN lub Faster R-CNN, ten detektor jest w pełni splotowy, a prawie wszystkie obliczenia są wspólne dla całego obrazu.\n",
        "\n",
        "<br> R-FCN składa się ze współdzielonych, w pełni konwolucyjnych warstw, i daje lepsze wyniki niż Faster R-CNN. .\n",
        "\n",
        "<br> Więcej informacji dostępnych jest na stronie:<br>\n",
        "<a href=\"https://arxiv.org/pdf/1605.06409.pdf\" rel=\"nofollow\">arxiv.org</a>.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSZizC0SWZMS",
        "colab_type": "text"
      },
      "source": [
        "###<br><b>8. YOLO (You Only Look Once)</b><br>\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/Dets/YOLO.png?raw=1\" alt=\"YOLO\" width=\"800\" >\n",
        "</div>\n",
        "\n",
        "<br>You Only Look Once lub YOLO to jeden z popularnych algorytmów wykrywania obiektów używanych na całym świecie. Według naukowców z Facebook AI Research, ujednolicona architektura YOLO jest niezwykle szybka. Podstawowy model YOLO przetwarza obrazy w czasie rzeczywistym z szybkością 45 klatek na sekundę, a dla wersji Fast YOLO uzyskujemy aż 155 klatek na sekundę, jednocześnie osiągając dwukrotnie wyższą wartość MAP niż wcześniejszych detektorów działających w czasie rzeczywistym.\n",
        "\n",
        "\n",
        "<br>Więcej informacji dostępnych jest na stronie:<br>\n",
        "<a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf\" \n",
        "rel=\"nofollow\">cv-foundation.org</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3_FOCcSt4Gb",
        "colab_type": "text"
      },
      "source": [
        "###<br><b>9. Multi-task Cascaded Convolutional Networks MTCNN</b><br>\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/Dets/MTCNN.png?raw=1\" alt=\"MTCNN\" width=\"800\" >\n",
        "</div>\n",
        "\n",
        "<br>MTCNN wykorzystywana jest do detekcji twarzy. Sieć zbudowana jest jako trzystopniowe kaskada  głębokich sieci konwolucyjnych. Po pierwsze, okna kandydatów są tworzone za pośrednictwem szybkiej sieci propozycji (Proposal P-Net). W następnym etapie większość kandydatów jest odrzucana za pomocą sieci udoskonalania (Refinement R-Net). W trzecim etapie, Sieć wyjściowa (Output O-Net) tworzy ostateczną ramkę ograniczającą i położenie punktów na twarzy Lendmarks. Wykrywanie i ustawianie twarzy w  środowisku jest trudne z powodu różnych pozycji, oświetlenia i okluzji. Autorzy MTCNN podają bardzo dobry wynik True Positive na poziomie 95% dla zbioru FDDB.\n",
        " \n",
        " <div align=\"center\">\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/Dets/MTCNN_result.png?raw=1\" alt=\"MTCNN results\" width=\"400\" >\n",
        "</div>\n",
        "\n",
        "<br>W porównaniu do wcześniejszych sieci jest to bardzo dobry wynik. Równie istotne jest, że model można użyć w trywialny sposób w dalszych badaniach. <b>Model został wykorzystany jako punkt odniesienia w dalszej częsci pracy.</b><br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUQ4xuGrypky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install facenet-pytorch \n",
        "from facenet_pytorch import MTCNN\n",
        "#inicjalizacja sieci mtcnn, pelny opis dostepny: help(MTCNN)\n",
        "mtcnn = MTCNN(image_size=224, margin=0, keep_all=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jp62xnNRyup9",
        "colab_type": "text"
      },
      "source": [
        "Więcej informacji dostępnych jest na stronie:<br>\n",
        "<a href=\"https://arxiv.org/pdf/1604.02878.pdf\" \n",
        "rel=\"nofollow\">arxiv.org</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GUZUttf29_D",
        "colab_type": "text"
      },
      "source": [
        "##<br><b>DE⫶TR: End-to-End Object Detection with Transformers</b><br>\n",
        "\n",
        "PyTorch training code and pretrained models for **DETR** (**DE**tection **TR**ansformer).\n",
        "We replace the full complex hand-crafted object detection pipeline with a Transformer, and match Faster R-CNN with a ResNet-50, obtaining **42 AP** on COCO using half the computation power (FLOPs) and the same number of parameters. Inference in 50 lines of PyTorch.\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/Dets/DETR.png?raw=1\" alt=\"DETR\" width=\"800\" >\n",
        "</div>\n",
        "\n",
        "\n",
        "**What it is**. Unlike traditional computer vision techniques, DETR approaches object detection as a direct set prediction problem. It consists of a set-based global loss, which forces unique predictions via bipartite matching, and a Transformer encoder-decoder architecture. \n",
        "Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. Due to this parallel nature, DETR is very fast and efficient.\n",
        "\n",
        "For details see [End-to-End Object Detection with Transformers](https://ai.facebook.com/research/publications/end-to-end-object-detection-with-transformers) by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\n",
        "\n",
        "COCO val5k evaluation results can be found in this [gist](https://gist.github.com/szagoruyko/9c9ebb8455610958f7deaa27845d7918).\n",
        "\n",
        "\n",
        "\n",
        "# Notebooks\n",
        "\n",
        "* [Standalone Colab Notebook](https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb): In this notebook, we demonstrate how to implement a simplified version of DETR from the grounds up in 50 lines of Python, then visualize the predictions. It is a good starting point if you want to gain better understanding the architecture and poke around before diving in the codebase.\n",
        "\n",
        "\n",
        "## Training\n",
        "To ease reproduction of our results we provide\n",
        "[results and training logs](https://gist.github.com/szagoruyko/b4c3b2c3627294fc369b899987385a3f)\n",
        "for 150 epoch schedule (3 days on a single machine), achieving 39.5/60.3 AP/AP50.\n",
        "\n",
        "We train DETR with AdamW setting learning rate in the transformer to 1e-4 and 1e-5 in the backbone.\n",
        "Horizontal flips, scales an crops are used for augmentation.\n",
        "Images are rescaled to have min size 800 and max size 1333.\n",
        "The transformer is trained with dropout of 0.1, and the whole model is trained with grad clip of 0.1.\n",
        "\n",
        "Niestety autorzy raportują, iż sposób trenowania jest nadal niestabilny  i wymaga dalszych prac. \n",
        "W przypadku Wider Face, ilość klas należy ustawić na 1, a ilość obiektów do 2000. Trenowanie ponad 300 epok nie przyniosło oczekiwanych resultatów.\n",
        "<br>Przykład notbooka do trenowania DETR na bazie Detectron2:\n",
        "<br>[DETR notobook](https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/04_DeTr_R50.ipynb)\n",
        "\n",
        "<br>Więcej informacji dostępnych jest na stronie:<br>\n",
        "<a href=\"https://arxiv.org/pdf/2005.12872.pdf\" \n",
        "rel=\"nofollow\">arxiv.org</a>.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUOsSpcD1vQT",
        "colab_type": "text"
      },
      "source": [
        "###<br>Uwaga:<br>\n",
        "\n",
        "Dla entuzjastów, zagadnieniami komputerowego przetwarzania obrazów zajmuje się dedykowana konferencja <a href=\"https://cvdc.adasci.org/\" data-wpel-link=\"external\" target=\"_blank\" rel=\"nofollow\">Computer Vision DEVCON</a>. Jest to dwudniowa konferencja, która ma na celu zgromadzenie praktyków komputerowego przetwarzania obrazów i innowatorów na jednej platformie, aby dzielić się i omawiać najnowsze osiągnięcia w tej dziedzinie. Ostatnia odbyła sie w dniach 13-14 Sierpnia 2020 roku."
      ]
    }
  ]
}