{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_00_Dets.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMFbPeIjZly3NDyO7IDbrX7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarekGit/FACES_DNN/blob/master/notebooks/04_00_Dets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl1gpedgbEWv",
        "colab_type": "text"
      },
      "source": [
        "#<b>Podstawowe Metody Wykrywania Obiektów</b><br>\n",
        "<b>Wykrywanie obiektów </b> jest jedną z dziedzin, która przeżywa gwałtowny rozwój w obszarze sieci neuronowych. Konieczoność jednoczesnego określenia lokalizacji oraz klasy obietków sprawia, że jest to jedno z najtrudniejszych zadań w dziedzinie sieci neuronowych. Model w tym zastosowaniu musi  określić, gdzie znajdują się obiekty na danym obrazie, zwanym lokalizacją obiektu i do której kategorii należy każdy obiekt, czyli klasyfikacją obiektu.<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsDvky1dnK0w",
        "colab_type": "text"
      },
      "source": [
        "###<br><b>1. Histogram of Oriented Gradients (HOG)</b><br>\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/Dets/HOG.png?raw=1\" alt=\"R_CNN\" width=\"600\" >\n",
        "</div>\n",
        "\n",
        "<br>Histogram zorientowanych gradientów (HOG) zasadniczo jest deskryptorem cech, który jest używany do wykrywania obiektów w przetwarzaniu obrazów. Metoda histogramu zorientowanych gradientów obejmuje wykrywanie orientacji gradientu we wskazanych częściach obrazu, metodami typu przesuwane okno wykrywania lub obszar zainteresowania (ROI). Jedną z zalet funkcji podobnych do HOG jest ich prostota. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7yc1faYb_wE",
        "colab_type": "text"
      },
      "source": [
        "###<br><b>2. Region-based Convolutional Neural Networks (R-CNN)</b><br>\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/Dets/R_CNN.png?raw=1\" alt=\"R_CNN\" width=\"800\" >\n",
        "</div>\n",
        "\n",
        "<br>The Region-based Convolutional Network method (RCNN) to połączenie propozycji obszarów występowania obiektów (ROI) z sieciami konwolucyjnymi (CNN). R-CNN wykorzystuje głębokie sieci  neuronowe do lokalizowania obiektów i uczeniu modelu o dużej pojemności przy zastosowaniu  niewielkiej ilości danych wejściowych z adnotacjami. Osiąga wyższą dokładność wykrywania obiektów dzięki zastosowaniu głębokiej sieci ConvNet do klasyfikowania propozycji obiektów. R-CNN ma możliwość skalowania do tysięcy klas obiektów bez wykorzystania metod pomocniczych. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK10oZJmuRFQ",
        "colab_type": "text"
      },
      "source": [
        "###<br><b>3. Single Shot Detector (SSD)</b><br>\n",
        "\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/Dets/SSD.png?raw=1\" alt=\"SSD\" width=\"800\" >\n",
        "</div>\n",
        "\n",
        "<br> Single Shot Detector (SSD) to metoda wykrywania obiektów na obrazach za pomocą jednej głębokiej sieci neuronowej.  Sieć detektorów  łączy prognozy z wielu map obiektów o różnych rozdzielczościach, aby w naturalny sposób obsługiwać obiekty o różnych rozmiarach. <br>\n",
        "\n",
        "Zalety SSD:\n",
        "- Sieć SSD całkowicie eliminuje generowanie propozycji i kolejne etapy ponownego próbkowania pikseli lub funkcji i łączy wszystkie obliczenia w jednej sieci.\n",
        "- Łatwy do przeszkolenia i prosty do zintegrowania z innymi systemami.\n",
        "- SSD ma konkurencyjną dokładność w stosunku do metod, które wykorzystują dodatkowy krok propozycji obiektu, i jest znacznie szybszy, zapewniając jednolitą strukturę zarówno dla szkolenia, jak i detekcji.\n",
        "\n",
        "\n",
        "<br>Więcej informacji dostępnych jest na stronie:<br>\n",
        "<a href=\"https://arxiv.org/pdf/1512.02325.pdf%EF%BC%89\"\n",
        "rel=\"nofollow\">arxiv.org</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sxyz6eBnppE9",
        "colab_type": "text"
      },
      "source": [
        "###<br><b>4. Fast R-CNN</b><br>\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/Dets/Fast_R_CNN.png?raw=1\" alt=\"Fast R_CNN\" width=\"600\" >\n",
        "</div>\n",
        "\n",
        "<br> Napisany w Pythonie i C ++ (Caffe), Fast Region-Based Convolutional Network lub Fast R-CNN to algorytm uczący się do wykrywania obiektów. Algorytm ten eliminuje głównie wady R-CNN i SPPnet, jednocześnie poprawiając ich szybkość i dokładność. <br>\n",
        "\n",
        "<br>Zalety Fast R-CNN:\n",
        "\n",
        "- Wyższa jakość wykrywania (mAP) niż R-CNN, SPPnet\n",
        "- Trening jednoetapowe, z wykorzystaniem wieloelementowej  fuckji straty\n",
        "\n",
        "\n",
        "<br>Więcej informacji na stronie:<br> \n",
        "<a href=\"http://openaccess.thecvf.com/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf\"\n",
        "rel=\"nofollow\">openaccess.thecvf.com</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "324DCa7jsuS0",
        "colab_type": "text"
      },
      "source": [
        "###<br><b>5. Spatial Pyramid Pooling (SPP-net)</b><br>\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/Dets/SPP.png?raw=1\" alt=\"SPP\" width=\"600\" >\n",
        "</div>\n",
        "\n",
        "<br> Spatial Pyramid Pooling (SPP-net) to struktura sieciowa, która może generować reprezentację o stałej długości niezależnie od rozmiaru / skali obrazu.SPP-net poprawia  metody klasyfikacji obrazów oparte na CNN. Korzystając z sieci SPP można obliczyć mapy cech z całego obrazu tylko raz, a następnie połączyć cechy w podobrazach w celu wygenerowania reprezentacji o stałej długości. Ta metoda pozwala uniknąć wielokrotnego obliczania cech konwolucyjnych.\n",
        "\n",
        "<br> Więcej informacji dostępnych jest na stronie:<br>\n",
        "<a href=\"https://arxiv.org/pdf/1406.4729.pdf)%C3%AC%20%CB%9C\"\n",
        "rel=\"nofollow\">arxiv.org</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JhZcpqfwDSr",
        "colab_type": "text"
      },
      "source": [
        "###<br><b>6. Faster R-CNN</strong></b><br>\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/Dets/Faster_R_CNN.png?raw=1\" alt=\"Faster RCNN\" width=\"700\" >\n",
        "</div>\n",
        "\n",
        "<br> Faster R-CNN to algorytm wykrywania obiektów podobny do R-CNN. Algorytm ten wykorzystuje Region Proposal Network (RPN), który współdzieli funkcje splotu pełnego obrazu z siecią detekcyjną bardzije efektywnie niż R-CNN i Fast R-CNN. Sieć propozycji regionów jest siecią konwolucyjną, która jednocześnie przewiduje granice obiektu oraz ocenia klasę obiektu dla każdej propozycji.\n",
        "\n",
        "\n",
        "\n",
        "<br>Więcej informacji dostępnych jest na stronie:<br>\n",
        "<a href=\"http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf\"\n",
        "rel=\"nofollow\">papers.nips.cc</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1gADrvByVis",
        "colab_type": "text"
      },
      "source": [
        "###<br><b>7. Region-based Fully Convolutional Network (R-FCN)</b><br>\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/Dets/R_FCN.png?raw=1\" alt=\"R_FCN\" width=\"700\" >\n",
        "</div>\n",
        "\n",
        "<br> Region-based Fully Convolutional Networks lub R-FCN to  detektor bazujący na regionach do wykrywania obiektów. W przeciwieństwie do innych detektorów opartych na regionach, które stosują kosztowną podsieć ROI, taką jak Fast R-CNN lub Faster R-CNN, ten detektor jest w pełni splotowy, a prawie wszystkie obliczenia są wspólne dla całego obrazu.\n",
        "\n",
        "<br> R-FCN składa się ze współdzielonych, w pełni konwolucyjnych warstw, i daje lepsze wyniki niż Faster R-CNN. .\n",
        "\n",
        "<br> Więcej informacji dostępnych jest na stronie:<br>\n",
        "<a href=\"https://arxiv.org/pdf/1605.06409.pdf\" rel=\"nofollow\">arxiv.org</a>.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSZizC0SWZMS",
        "colab_type": "text"
      },
      "source": [
        "###<br><b>8. YOLO (You Only Look Once)</b><br>\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/Dets/YOLO.png?raw=1\" alt=\"YOLO\" width=\"800\" >\n",
        "</div>\n",
        "\n",
        "<br>You Only Look Once lub YOLO to jeden z popularnych algorytmów wykrywania obiektów używanych na całym świecie. Według naukowców z Facebook AI Research, ujednolicona architektura YOLO jest niezwykle szybka. Podstawowy model YOLO przetwarza obrazy w czasie rzeczywistym z szybkością 45 klatek na sekundę, a dla wersji Fast YOLO uzyskujemy aż 155 klatek na sekundę, jednocześnie osiągając dwukrotnie wyższą wartość MAP niż wcześniejszych detektorów działających w czasie rzeczywistym.\n",
        "\n",
        "\n",
        "<br>Więcej informacji dostępnych jest na stronie:<br>\n",
        "<a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf\" \n",
        "rel=\"nofollow\">cv-foundation.org</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3_FOCcSt4Gb",
        "colab_type": "text"
      },
      "source": [
        "###<br><b>9. Multi-task Cascaded Convolutional Networks MTCNN</b><br>\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/Dets/MTCNN.png?raw=1\" alt=\"MTCNN\" width=\"800\" >\n",
        "</div>\n",
        "\n",
        "<br>MTCNN wykorzystywana jest do detekcji twarzy. Sieć zbudowana jest jako trzystopniowe kaskada  głębokich sieci konwolucyjnych. Po pierwsze, okna kandydatów są tworzone za pośrednictwem szybkiej sieci propozycji (Proposal P-Net). W następnym etapie większość kandydatów jest odrzucana za pomocą sieci udoskonalania (Refinement R-Net). W trzecim etapie, Sieć wyjściowa (Output O-Net) tworzy ostateczną ramkę ograniczającą i położenie punktów na twarzy Lendmarks. Wykrywanie i ustawianie twarzy w  środowisku jest trudne z powodu różnych pozycji, oświetlenia i okluzji. Autorzy MTCNN podają bardzo dobry wynik True Positive na poziomie 95% dla zbioru FDDB.\n",
        " \n",
        " <div align=\"center\">\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/Dets/MTCNN_result.png?raw=1\" alt=\"MTCNN results\" width=\"600\" >\n",
        "</div>\n",
        "\n",
        "<br>W porównaniu do wcześniejszych sieci jest to bardzo dobry wynik. Równie istotne jest, że model można użyć w trywialny sposób w dalszych badaniach. <b>Model został wykorzystany jako punkt odniesienia w dalszej częsci pracy.</b><br>\n",
        "\n",
        "```\n",
        "!pip install facenet-pytorch \n",
        "from facenet_pytorch import MTCNN\n",
        "#inicjalizacja sieci mtcnn, pelny opis dostepny: help(MTCNN)\n",
        "mtcnn = MTCNN(image_size=224, margin=0, keep_all=True)\n",
        "```\n",
        "\n",
        "<br>Więcej informacji dostępnych jest na stronie:<br>\n",
        "<a href=\"https://arxiv.org/pdf/1604.02878.pdf\" \n",
        "rel=\"nofollow\">arxiv.org</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GUZUttf29_D",
        "colab_type": "text"
      },
      "source": [
        "##<br><b>DE⫶TR: DEtection TRansformer</b><br>\n",
        "\n",
        "<br>Jest to bardzo ciekawe rozwiązanie, ale na bardzo wstępnym etapie rozwoju. W modelu DETR, algorytmy przygotowane ręcznie (NMS, Anchor Boxes,..) zastąpione zostały modułem transformer. Dla backbone  ResNet-50 z ekstrakcją cech uzyskano  <b>mAP  42% </b> na zbiorze COCO przy użyciu połowy mocy obliczeniowej (FLOP) i tej samej liczby parametrów co Faster RCNN.<>br\n",
        "\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/Dets/DETR.png?raw=1\" alt=\"DETR\" width=\"800\" >\n",
        "</div>\n",
        "\n",
        "\n",
        "<br>W przeciwieństwie do tradycyjnych technik detekcji obrazów, DETR traktuje wykrywanie obiektów jako problem bezpośredniego przewidywania zbioru. Model wykorzystuje do przewidywania dopasowanie dwudzielne (bipartite matching), oraz architekturę kodera-dekodera  transformer.\n",
        "Biorąc pod uwagę ustalony mały zestaw wyuczonych zapytań dotyczących obiektów, DETR uzasadnia relacje obiektów i globalny kontekst obrazu, aby bezpośrednio równolegle wyprowadzić końcowy zestaw prognoz. Ze względu na ten równoległy charakter DETR jest bardzo szybki i wydajny.\n",
        "\n",
        "\n",
        "<br>Wyniki ewaluacji COCO val5k można sprawdzić na stronie:[gist](https://gist.github.com/szagoruyko/9c9ebb8455610958f7deaa27845d7918).\n",
        "\n",
        "\n",
        "\n",
        "Przykładowy notebook z demonstracją wykorzystania Detr:\n",
        "[Standalone Colab Notebook](https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb).\n",
        "\n",
        "\n",
        "###<b> Problemy z treningiem </b>\n",
        "Dla 150 epok (3 dni treningu na pojedynczej karcie) uzyskano 39.5/60.3 AP/AP50 dla zbioru COCO.\n",
        "\n",
        "<b>Niestety autorzy zgłaszają, iż sposób trenowania jest nadal nieprzewidywalny  i wymaga dalszych prac</b>.\n",
        "<br>W przypadku Wider Face, ilość klas ustawiono na 1, a ilość obiektów na 2000.<br>\n",
        "<b>Trenowanie ponad 200 epok nie przyniosło oczekiwanych resultatów.</b>\n",
        "<br>Przykład notbooka do trenowania DETR na bazie Detectron2: [WIDER DETR notobook](https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/04_DeTr_R50.ipynb)\n",
        "\n",
        "<br>Więcej informacji dostępnych jest na stronie:<br>\n",
        "<a href=\"https://arxiv.org/pdf/2005.12872.pdf\" \n",
        "rel=\"nofollow\">arxiv.org</a>.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUOsSpcD1vQT",
        "colab_type": "text"
      },
      "source": [
        "###<br>Uwaga:<br>\n",
        "\n",
        "Dla entuzjastów, zagadnieniami komputerowego przetwarzania obrazów zajmuje się dedykowana konferencja <a href=\"https://cvdc.adasci.org/\" data-wpel-link=\"external\" target=\"_blank\" rel=\"nofollow\">Computer Vision DEVCON</a>. Jest to dwudniowa konferencja, która ma na celu zgromadzenie praktyków komputerowego przetwarzania obrazów i innowatorów na jednej platformie, aby dzielić się i omawiać najnowsze osiągnięcia w tej dziedzinie. Ostatnia odbyła sie w dniach 13-14 Sierpnia 2020 roku."
      ]
    }
  ]
}