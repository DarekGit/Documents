{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_01_DETECTRON2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarekGit/FACES_DNN/blob/master/notebooks/05_01_DETECTRON2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clbDLrYzt4Il",
        "colab_type": "text"
      },
      "source": [
        "### [Spis Treści](https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Praca_Dyplomowa.ipynb)\n",
        "[5. Detekcja twarzy z wykorzystaniem wybranych architektur GNN](05_00_Modyfikacje.ipynb)<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZdGvPoauDBB",
        "colab_type": "text"
      },
      "source": [
        "## 5.1. Detectron2 Faster R-CNN z FPN Resnet50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9Gmzyert8UZ",
        "colab_type": "text"
      },
      "source": [
        "Detectron2 to system detekcji obiektów stworzony przez Facebook AI Research, posiadający implementacje najnowocześniejszych algorytmów wykrywania obiektów w tym Faster R-CNN, Mask R-CNN (state-of-the-art object detection algorithms). System został napisany w PyTorchu. Przez twórców został udostępniony jako biblioteka open-source, która może być wykorzystana do obsługi i rozwoju różnych projektów badawczych."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UnABHQe4L5r",
        "colab_type": "text"
      },
      "source": [
        "W pracy środowisko Detectron2 zostało wykorzystane do wytrenowania modeli opartych na Faster R-CNN z FPN Resnet50. \n",
        "\n",
        "\n",
        "### Model faster_rcnn_R_50_FPN_3x \n",
        "\n",
        "\n",
        "Model pretenowany na zbiorze danych COCO train2017. Nastepnie został dotrenowany na zbiorze treningowym WIDER FACE (270000 iteracji). W czasie treningu zastosowano \n",
        "* Agumentację\n",
        "  * ResizeShortestEdge(short_edge_length=(640, 672, 704, 736), max_size=1333, sample_style='choice'),\n",
        "  * RandomFlip(),\n",
        "* FrozenBatchNorm2d, \n",
        "* Batch size: 8, \n",
        "* Scheduler: WarmupMultiStepLR\n",
        "* Bazowy LR: 0.02 z liniowym rozgrzewaniem (linear warm-up), zmniejszany dziesieciokrotnie przy 210000 i 250000 iteracji \n",
        "\n",
        "Do pracy został dołączony przykładowy plik umożliwiający trenowanie modelu i zawierający narzędzia do ładowania datasetu <br>[WIDERFACE_faster_rcnn_R_50_FPN_3x.ipynb](WIDERFACE_faster_rcnn_R_50_FPN_3x.ipynb)\n",
        "\n",
        "<br>\n",
        "\n",
        "### Model scratch_faster_rcnn_R_50_FPN_gn\n",
        "\n",
        "\n",
        "W odniesieniu do metod przedstawionych w artykule \"Rethinking ImageNet Pre-training\" <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[15]</a> dokonano trenowania od zera na zbiorze treningowym WIDER FACE. Model trenowano przez 540000 iteracji. \n",
        "* Zastosowano agumentację\n",
        "    * ResizeShortestEdge(short_edge_length=(640,), max_size=1333, sample_style='choice'), \n",
        "    * RandomFlip()\n",
        "* Group Normalization <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[16]</a>  - metoda normalizacji nie zależna od rozmiaru batcha \n",
        "* Batch size: 2,\n",
        "* Scheduler: WarmupMultiStepLR\n",
        "* Bazowy learning rate został ustawiony na 0.02 z liniowym rozgrzewaniem (linear warm-up), zmniejszany dzisieciokrotnie przy 480000 i 520000 iteracji.\n",
        "\n",
        "Zapis trenowania modelu został dołączony do pracy jako załącznik [WIDERFACE_scratch_faster_rcnn_R_50_FPN_gn.ipynb](WIDERFACE_scratch_faster_rcnn_R_50_FPN_gn.ipynb)\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZQyjG0mRnbT",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "\n",
        "Porównanie wyników wytrenowanych modeli zostało zamieszczone w rozdziale [6. Porównanie modeli](06_00_Porownanie.ipynb).\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMG7gq6RG4jD",
        "colab_type": "text"
      },
      "source": [
        "W poniższych punktach znajduje się omówienie instalacj środowiska i jego krótki opis oraz informacje w jaki sposób ładowane są dane i jak przebiega proces treningu.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZdsI0ABRxba",
        "colab_type": "text"
      },
      "source": [
        "### Instalacja środowiska"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXCTbVLqR4L4",
        "colab_type": "text"
      },
      "source": [
        "Środowisko instalowane jest z repozytorium umieszczonym przez Facebook AI Research na [GitHub](https://github.com/facebookresearch/detectron2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBt48tE3R3dz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U torch==1.5 torchvision==0.6 -f https://download.pytorch.org/whl/cu101/torch_stable.html \n",
        "!pip install cython pyyaml==5.1\n",
        "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
        "\n",
        "!git clone https://github.com/facebookresearch/detectron2 detectron2_repo\n",
        "!pip install -q -e detectron2_repo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md1xunzZSf4D",
        "colab_type": "text"
      },
      "source": [
        "Po instalacji Detectron2 w środowisku Colab niezbędne jest ponowne uruchomienie środwiska wykonawczego (Ctrl+M.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCbH1tEsTFWG",
        "colab_type": "text"
      },
      "source": [
        "Import podstawowych modułów:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wE3hP1dTClC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from detectron2 import model_zoo\n",
        "import detectron2.utils.comm as comm\n",
        "from detectron2.engine import DefaultPredictor, DefaultTrainer, HookBase\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_train_loader\n",
        "from detectron2.structures import BoxMode\n",
        "from detectron2.data import build_detection_test_loader\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX9RUKJbRrZM",
        "colab_type": "text"
      },
      "source": [
        "### Struktura repozytorium\n",
        "Poniżej znajduje się drzewo katalogów Detectron2 i opis zawartości katalogów.\n",
        "\n",
        "```\n",
        "detectron2/\n",
        "\n",
        "├─checkpoint  <- checkpointer \n",
        "├─config      <- domyślne konfiguracje\n",
        "├─data        <- programy obsługi zbiorów danych i programy ładujące dane\n",
        "├─engine      <- moduły trenowania i predykcji\n",
        "├─evaluation  <- evaluatory dla podstawowych typów zbiorów danych\n",
        "├─export      <- konwerter modeli detectron2 do caffe2 (ONNX)\n",
        "├─layers      <- warstwy niestandardowe np.: Deformable convolution\n",
        "├─model_zoo   <- wytreniwane modele\n",
        "├─modeling   \n",
        "│  ├─meta_arch <- meta architektury np. R-CNN, RetinaNet\n",
        "│  ├─backbone  \n",
        "│  │    ├─backbone.py   <- zawiera abstrakcyjną klasę bazową Backbone\n",
        "│  │    ├─build.py      <- wywołanie funkcji konstruktora określonej w config\n",
        "│  │    ├─fpn.py        <- zawiera klasę FPN \n",
        "│  │    ├─resnet.py     <- zawiera klasę ResNet \n",
        "│  ├─proposal_generator <- RPN\n",
        "│  └─roi_heads <- ROI heads \n",
        "├─solver       <- moduły zawierające modyfikacje optimizera i schedulera\n",
        "├─structures   <- klasy struktur np.: Boxes, Instances\n",
        "└─utils        <- moduły użytkowe np.: visualizer, logger \t\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBnXKq9Wz_3Q",
        "colab_type": "text"
      },
      "source": [
        "### Omówienie środowiska\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8aSvL5O3XMR",
        "colab_type": "text"
      },
      "source": [
        "Podstawową konfiguracją Faster R-CNN z FPN Resnet50 jest [Base-RCNN-FPN](https://github.com/facebookresearch/detectron2/blob/master/configs/Base-RCNN-FPN.yaml).\n",
        "Base-RCNN-FPN określa przede wszystkim meta architekturę GeneralizedRCNN, Backbone Network, RPN i ROI Heads. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWPXiA-Yy-HF",
        "colab_type": "text"
      },
      "source": [
        "<div align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/DarekGit/FACES_DNN/master/Figures/DetectronFasterRCNN.png\" alt=\"detectron2\">\n",
        "<p>Rys. 1. Detectron2 Faster R-CNN z FPN Resnet50\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4_4oXK3pkk3",
        "colab_type": "text"
      },
      "source": [
        "Klasy realizujące architekturę przedstawioną na Rys. 1. są zapisane w poniższych lokalizacjach:\n",
        "\n",
        "GeneralizedRCNN [meta_arch/rcnn.py](https://github.com/facebookresearch/detectron2/blob/master/detectron2/modeling/meta_arch/rcnn.py)\n",
        " \n",
        "\n",
        "- Backbone Network: FPN (Feature Pyramid Network) [backbone/fpn.py](https://github.com/facebookresearch/detectron2/blob/master/detectron2/modeling/backbone/fpn.py)\n",
        "  - ResNet [backbone/resnet.py](https://github.com/facebookresearch/detectron2/blob/master/detectron2/modeling/backbone/resnet.py)\n",
        "\n",
        "- RPN (Region Proposal Network) [proposal_generator/rpn.py](https://github.com/facebookresearch/detectron2/blob/master/detectron2/modeling/proposal_generator/rpn.py)\n",
        "  - StandardRPNHead [proposal_generator/rpn.py](https://github.com/facebookresearch/detectron2/blob/master/detectron2/modeling/proposal_generator/rpn.py)\n",
        "  - RPNOutput [proposal_generator/rpn_outputs.py](https://github.com/facebookresearch/detectron2/blob/master/detectron2/modeling/proposal_generator/rpn_outputs.py)\n",
        " \n",
        "- ROI Heads: StandardROIHeads [roi_heads/roi_heads.py](https://github.com/facebookresearch/detectron2/blob/master/detectron2/modeling/roi_heads/roi_heads.py)\n",
        " - ROIPooler [poolers.py](https://github.com/facebookresearch/detectron2/blob/master/detectron2/modeling/poolers.py)\n",
        " - FastRCNNConvFCHead [roi_heads/box_heads.py](https://github.com/facebookresearch/detectron2/blob/master/detectron2/modeling/roi_heads/box_head.py)\n",
        " - FastRCNNOutputLayers [roi_heads/fast_rcnn.py](https://github.com/facebookresearch/detectron2/blob/master/detectron2/modeling/roi_heads/fast_rcnn.py)\n",
        " - FastRCNNOutputs [roi_heads/fast_rcnn.py](https://github.com/facebookresearch/detectron2/blob/master/detectron2/modeling/roi_heads/fast_rcnn.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "987fycBPNDLb",
        "colab_type": "text"
      },
      "source": [
        "## Backbone Network\n",
        "\n",
        "Implementację Feature Pyramid Network zawiera klasa FPN zapisana w pliku [backbone/fpn.py](https://github.com/facebookresearch/detectron2/blob/master/detectron2/modeling/backbone/fpn.py). Wybór i konfiguracja sieci Resnet50 realizowana jest poprzez wywołanie funkcji [build_resnet_fpn_backbone](https://github.com/facebookresearch/detectron2/blob/9fe2edaf6f22bb1c2069ec12b9089d19069e5286/detectron2/modeling/backbone/fpn.py#L202) przyjmującej jako jeden z argumentów słownik [cfg](https://github.com/facebookresearch/detectron2/blob/master/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml) zawierający konfigurację całej architektury w tym głębokość modelu. Wybierając model Resnet50 otrzymujemy poniższa strukturę wynikową:\n",
        "\n",
        "```\n",
        "ResNet(\n",
        "  (stem): BasicStem()              <- BasicStem blok \n",
        "  (res2):                          <- blok res2\n",
        "    (0): BottleneckBlock()         <- (shortcut), stride=(1, 1)\n",
        "    (1): BottleneckBlock()         <- stride=(1, 1)\n",
        "    (2): BottleneckBlock()         <- stride=(1, 1)\n",
        "  (res3):                          <- blok res3\n",
        "    (0): BottleneckBlock()         <- (shortcut), stride=(2, 2)\n",
        "    (1): BottleneckBlock()         <- stride=(1, 1) \n",
        "    (2): BottleneckBlock()         <- stride=(1, 1)\n",
        "    (3): BottleneckBlock()         <- stride=(1, 1)\n",
        "  (res4):                          <- blok res4\n",
        "    (0): BottleneckBlock()         <- (shortcut), stride=(2, 2)\n",
        "    (1): BottleneckBlock()         <- stride=(1, 1)\n",
        "    (2): BottleneckBlock()         <- stride=(1, 1)\n",
        "    (3): BottleneckBlock()         <- stride=(1, 1)\n",
        "    (4): BottleneckBlock()         <- stride=(1, 1)\n",
        "    (5): BottleneckBlock()         <- stride=(1, 1)\n",
        "  (res5):                          <- blok res5\n",
        "    (0): BottleneckBlock()         <- (shortcut), stride=(2, 2)\n",
        "    (1): BottleneckBlock()         <- stride=(1, 1)\n",
        "    (2): BottleneckBlock()         <- stride=(1, 1)\n",
        ")\n",
        "```\n",
        "Struktura wszystkich bloków została zapisana w pliku [faster_rcnn_R_50_FPN.txt](faster_rcnn_R_50_FPN.txt) <br>\n",
        "Reprezentuje ona model wygenerowany z konfiguracji [COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml](https://github.com/facebookresearch/detectron2/blob/master/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qPdtL0W6ydv",
        "colab_type": "text"
      },
      "source": [
        "- **BasicStem blok** jest realizowany przez klasę [BasicStem](https://github.com/facebookresearch/detectron2/blob/5b8f68436651c74608a3f4909cae570dbe23e51d/detectron2/modeling/backbone/resnet.py#L331). Składa się z konwolucji z kernel_size=(7, 7), stride=(2, 2), batch normalizacji. Po konwolucji wywołana jest funkcja aktywacji relu i max pooling kernel_size=3 i stride=2. Na wyjściu otrzymujemy tensor którego W, H są czterokrotnie mniejsze od rozmiaru obrazu na wejściu. \n",
        "\n",
        "- **BottleneckBlock** zastosowano aby zwiększyć głębokość sieci przy jednoczesnym zmniejszeniu parametrów. BottleneckBlock składa się z trzech konwolucji z kernel_size kolejno: (1, 1), (3, 3), (1, 1) ze stride=(1, 1). \n",
        "  \n",
        "  Występują tu trzy tyby bloków BottleneckBlock:\n",
        "\n",
        "  - BottleneckBlock z shortcut i stride=(1, 1)\n",
        "```\n",
        "      (res2): Sequential(\n",
        "        (0): BottleneckBlock(\n",
        "          (shortcut): Conv2d(\n",
        "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
        "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
        "          )\n",
        "          (conv1): Conv2d(\n",
        "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
        "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
        "          )\n",
        "          (conv2): Conv2d(\n",
        "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
        "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
        "          )\n",
        "          (conv3): Conv2d(\n",
        "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
        "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
        "          )\n",
        "        )\n",
        "```\n",
        "  - BottleneckBlock z shortcut i stride=(2, 2)\n",
        "```\n",
        "      (res4): Sequential(\n",
        "        (0): BottleneckBlock(\n",
        "          (shortcut): Conv2d(\n",
        "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
        "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
        "          )\n",
        "          (conv1): Conv2d(\n",
        "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
        "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
        "          )\n",
        "          (conv2): Conv2d(\n",
        "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
        "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
        "          )\n",
        "          (conv3): Conv2d(\n",
        "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
        "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
        "          )\n",
        "        )\n",
        "```\n",
        "  - BottleneckBlock z stride=(1, 1) bez połączenie skrótowego\n",
        "```\n",
        "      BottleneckBlock(\n",
        "          (conv1): Conv2d(\n",
        "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
        "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
        "          )\n",
        "          (conv2): Conv2d(\n",
        "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
        "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
        "          )\n",
        "          (conv3): Conv2d(\n",
        "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
        "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
        "          )\n",
        "        )\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QONYGHweZyT4",
        "colab_type": "text"
      },
      "source": [
        "Wyjścia bloków **res2**, **res3**, **res4**, **res5** używane jako wejścia połączeń bocznych FPN i przetwarzane są w ścieżce odgórnej. Pierwszym krokiem jest konwolucja Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1)) redukująca głębokość tensora wynikowego bloku res5 do 256. Tensor ten staje się pierwszą warstwą mapy obiektów używaną do dalszych transformacji. Na Rys. 1. opisany został jako feature map layer 5. \n",
        "\n",
        "\n",
        "Po zastosowaniu konwolucji wyjściowej Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1) jako **p5** jest pierwszym utworzonym argumentem wejściowym dla modułu FPN i ROI_HEADS. \n",
        "Następnie feature map layer 5 przetwarzany jest po raz kolejny. Ma miejsce upsampling poprzez wywołanie metody [torch.nn.functional.interpolate](https://github.com/facebookresearch/detectron2/blob/9fe2edaf6f22bb1c2069ec12b9089d19069e5286/detectron2/modeling/backbone/fpn.py#L131)\n",
        "\n",
        "```python\n",
        "top_down_features = F.interpolate(prev_features, scale_factor=2, mode=\"nearest\")\n",
        "lateral_features = lateral_conv(features)\n",
        "prev_features = lateral_features + top_down_features\n",
        "```\n",
        "\n",
        "Wynik dodawany jest do bocznego połączenia z res4. Po zastosowaniu konwolucji wyjściowej otrzymywany jest **p4**. \n",
        "Analogicznie uzyskiwane są wyniki **p3**, **p2**.\n",
        "\n",
        "Argument **p6** wykorzytywany jest jako wejściowy RPN generowany jest przez modół [LastLevelMaxPool](https://github.com/facebookresearch/detectron2/blob/9fe2edaf6f22bb1c2069ec12b9089d19069e5286/detectron2/modeling/backbone/fpn.py#L165), który realizuje max pooling z kernel_size=1, stride=2, padding=0 na wynikowym p5. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvPt_jcENDQi",
        "colab_type": "text"
      },
      "source": [
        "## Region Proposal Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKSQp8wO7CUt",
        "colab_type": "text"
      },
      "source": [
        "Moduł RPN składa się z dwóch części: [StandardRPNHead](https://github.com/facebookresearch/detectron2/blob/master/detectron2/modeling/proposal_generator/rpn.py) i [RPNOutput](https://github.com/facebookresearch/detectron2/blob/master/detectron2/modeling/proposal_generator/rpn_outputs.py). \n",
        "\n",
        "```\n",
        "RPN(\n",
        "  (rpn_head): StandardRPNHead(\n",
        "    (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
        "    (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
        "  )\n",
        "  (anchor_generator): DefaultAnchorGenerator(\n",
        "    (cell_anchors): BufferList()\n",
        "  )\n",
        ")\n",
        "```\n",
        "\n",
        "**StandardRPNHead** tworzą 3 konwolucje:\n",
        "```\n",
        "(conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "(objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
        "(anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
        "```\n",
        "Moduł przetwarza kolejno argumenty wejściowe p2, p3, p4, p5, p6 i zwraca listę prawdopodobieństw istnienia obiektu (pred_objectness_logits) i pred_anchor_deltas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNE-gyYyeprS",
        "colab_type": "text"
      },
      "source": [
        "**anchor_generator** \n",
        "\n",
        "[Base-RCNN-FPN](https://github.com/facebookresearch/detectron2/blob/master/configs/Base-RCNN-FPN.yaml) definiuje wielkość i proporcje. Kolejne elementy z ANCHOR_GENERATOR.SIZES odpowiadają kolejnym feature maps p2, p3, p4, p5, p6.\n",
        "\n",
        "Pięć elementów listy ANCHOR_GENERATOR.SIZES odpowiada pięciu poziomom map obiektów (od P2 do P6). <br> Na przykład P2 (stride = 4) ma jedną kotwicę o rozmiarze 32.\n",
        "\n",
        "```\n",
        "  ANCHOR_GENERATOR:\n",
        "    SIZES: [[32], [64], [128], [256], [512]]  # One size for each in feature map\n",
        "    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]  # Three aspect ratios (same for all in feature maps)\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKq3XZNTjeCJ",
        "colab_type": "text"
      },
      "source": [
        "## ROI Heads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz1M_4zPNDOr",
        "colab_type": "text"
      },
      "source": [
        "IN_FEATURES: [\"p2\", \"p3\", \"p4\", \"p5\"]\n",
        "\n",
        "\n",
        "```\n",
        "  (roi_heads): StandardROIHeads(\n",
        "    (box_pooler): ROIPooler(\n",
        "      (level_poolers): ModuleList(\n",
        "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
        "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
        "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
        "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
        "      )\n",
        "    )\n",
        "    (box_head): FastRCNNConvFCHead(\n",
        "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
        "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
        "    )\n",
        "    (box_predictor): FastRCNNOutputLayers(\n",
        "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
        "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
        "    )\n",
        "  )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCKeXoykHOvI",
        "colab_type": "text"
      },
      "source": [
        "## Ładowanie danych"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSyDrdB9HTnu",
        "colab_type": "text"
      },
      "source": [
        "Zestaw danych musi być dostosowany do oczekiwanego formatu danych. W pracy realizowane jest to za pomocą przygotowanych narzędzi. Wywołując funkcję annotations anotacje zdjęć z datasetu WIDER FACE konwertowane są do standardu Detectron2. Następnie aby załadować dane z zestawu danych, należy go zarejestrować w DatasetCatalog.\n",
        "\n",
        "```python\n",
        "train = annotations(\"train\")\n",
        "val = annotations('val')\n",
        "\n",
        "for d in [\"train\",\"val\"]:\n",
        "  DatasetCatalog.register(\"face_\" + d, lambda d=d: train if d == \"train\" else val)\n",
        "  MetadataCatalog.get(\"face_\" + d).set(thing_classes = ['face'])\n",
        "\n",
        "faces_metadata = MetadataCatalog.get(\"face_train\")\n",
        "```\n",
        "\n",
        "Jako wynik można uznać załadownie losowego zdjęcia z załadowanego datasetu.\n",
        "\n",
        "```python\n",
        "for item in random.sample(train, 1):\n",
        "    img = cv2.imread(item[\"file_name\"])\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=faces_metadata, scale=0.9)\n",
        "    vis = visualizer.draw_dataset_dict(item)\n",
        "    cv2_imshow(vis.get_image()[:, :, ::-1])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FZQJbBwLjbb",
        "colab_type": "text"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/DarekGit/FACES_DNN/master/Figures/sample_img.jpg\" alt=\"Przykładowe zdjęcie z WIDER FACE.\" width=\"600\" ><br>\n",
        "\n",
        "\n",
        "Rys. 2. Losowe zdjęcie z anotacjami z datasetu WIDER FACE. <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[2]</a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaUA2vWwSVCf",
        "colab_type": "text"
      },
      "source": [
        "## Konfiguracja modelu \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4jrKRMRM_c0",
        "colab_type": "text"
      },
      "source": [
        "Podstawowe konfiguracje modeli pretrenowanyc na Coco dataset zamieszczone są w plikach .yaml w katalogu configs/COCO-Detection.<br> W pracy jako wyjściowa została wykorzystana konfiguracja faster_rcnn_R_50_FPN_3x.yaml.\n",
        "Poniżej zamieszczone zostały przykładowe modyfikacje konfiguracji podstawowej.\n",
        "\n",
        "\n",
        "```python\n",
        "from detectron2.config import get_cfg\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "\n",
        "cfg.DATASETS.TRAIN = (\"face_train\",)\n",
        "cfg.DATASETS.TEST = (\"face_val\",)\n",
        "cfg.DATALOADER.NUM_WORKERS = cpu_count()\n",
        "cfg.TEST.EVAL_PERIOD=20000\n",
        "cfg.MODEL.PIXEL_MEAN = [104.14, 110.801, 119.85] # WIDER FACE PIXEL_MEAN\n",
        "cfg.MODEL.PIXEL_STD = [63.73, 56.45, 55.28] # WIDER FACE PIXEL_STD\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class \n",
        "cfg.SOLVER.IMS_PER_BATCH = 8 \n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256  \n",
        "cfg.INPUT.MIN_SIZE_TRAIN = (640, 672, 704, 736,)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByC0fRRUOqfw",
        "colab_type": "text"
      },
      "source": [
        "## Trenowanie modelu "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1oesuhpOqil",
        "colab_type": "text"
      },
      "source": [
        "W pętli trenowania modelu zastosowano hooka umożliwiającego podgląd wartości funkcji straty na zbiorze walidacyjnym."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPEjFkcXOqxI",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "class Validation_Loss(HookBase):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg.clone()\n",
        "        self.cfg.DATASETS.TRAIN = cfg.DATASETS.TEST\n",
        "        self._loader = iter(build_detection_train_loader(self.cfg))\n",
        "        \n",
        "    def after_step(self):\n",
        "        data = next(self._loader)\n",
        "        with torch.no_grad():\n",
        "            loss_dict = self.trainer.model(data)\n",
        "            \n",
        "            losses = sum(loss_dict.values())\n",
        "            assert torch.isfinite(losses).all(), loss_dict\n",
        "\n",
        "            loss_dict_reduced = { k+\"/val\": v.item() for k, v in comm.reduce_dict(loss_dict).items()}\n",
        "            losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
        "            loss_dict_reduced['total_loss/val']=losses_reduced\n",
        "            if comm.is_main_process():\n",
        "                self.trainer.storage.put_scalars(**loss_dict_reduced) #total_val_loss\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Gg3DNonOq0e",
        "colab_type": "text"
      },
      "source": [
        "Dodano również COCOEvaluator odpowiadający za ewaluzację na zbiorze walidacyjnym <br>i uruchamiany w czasie trenowania zgodnie z wartością cfg.TEST.EVAL_PERIOD=20000 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpEJQFC3Oq20",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "class Trainer(DefaultTrainer):\n",
        "  @classmethod\n",
        "  def build_evaluator(cls, cfg, DATASETS=cfg.DATASETS.TEST, out=cfg.OUTPUT_DIR):\n",
        "    return COCOEvaluator(dataset_name=DATASETS, \n",
        "                         cfg=cfg, \n",
        "                         distributed=False, \n",
        "                         output_dir=out)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuwqHjUxPW0T",
        "colab_type": "text"
      },
      "source": [
        "Uruchomienie treningu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3f9JPmfPW3D",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "trainer = Trainer(cfg) \n",
        "\n",
        "val_loss = Validation_Loss(cfg)  \n",
        "trainer.register_hooks([val_loss])\n",
        "trainer._hooks = trainer._hooks[:-2] + trainer._hooks[-2:][::-1]\n",
        "\n",
        "trainer.train()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ4Enc7XPW6y",
        "colab_type": "text"
      },
      "source": [
        "W czasie trenowania checkpoint domyśle zapisywany jest co 5000 iteracji (CHECKPOINT_PERIOD: 5000). \n",
        "<br>Dane byly zapisywane na Google drive (cfg.OUTPUT_DIR = ./drive/My Drive/scratch_faster_rcnn_R_50_FPN_gn)\n",
        "\n",
        "\n",
        "W przypadku przerwania trenowania możliwe jest załadowanie modelu i wznowienie trenowania."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo5IIYqBPW80",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "trainer = Trainer(cfg) \n",
        "\n",
        "val_loss = Validation_Loss(cfg)  \n",
        "trainer.register_hooks([val_loss])\n",
        "trainer._hooks = trainer._hooks[:-2] + trainer._hooks[-2:][::-1]\n",
        "\n",
        "cfg.MODEL.WEIGHTS = \"drive/My Drive/faster_rcnn_R_50_FPN_3x/model_0244999.pth\"  \n",
        "trainer.resume_or_load(resume=True)\n",
        "trainer.checkpointer.has_checkpoint()\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkcfdtBsPdO3",
        "colab_type": "text"
      },
      "source": [
        "<br><br>\n",
        "[5.2 MOBILENETV2](05_02_MOBILENETV2.ipynb)<br>\n",
        "[5. Detekcja twarzy z wykorzystaniem wybranych architektur GNN](05_00_Modyfikacje.ipynb)<br>\n",
        "### [Spis Treści](https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Praca_Dyplomowa.ipynb)\n"
      ]
    }
  ]
}