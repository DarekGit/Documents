{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "08_00_Podsumowanie.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarekGit/FACES_DNN/blob/master/notebooks/08_00_Podsumowanie.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlFNzbPN9W1R"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "[Spis treści](https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Praca_Dyplomowa.ipynb) | [1. Wstęp](01_00_Wstep.ipynb) | [2. Metryki oceny detekcji](02_00_Miary.ipynb) | [3. Bazy danych](03_00_Datasety.ipynb) | [4. Przegląd metod detekcji](04_00_Modele.ipynb) | [5. Detekcja twarzy z wykorzystaniem wybranych architektur GSN](05_00_Modyfikacje.ipynb) | [6. Porównanie modeli](06_00_Porownanie.ipynb) | [7. Eksport modelu](07_00_Eksport_modelu.ipynb) | [8. Podsumowanie i wnioski](08_00_Podsumowanie.ipynb) | [Bibliografia](Bibliografia.ipynb)\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4TgijzHQPws"
      },
      "source": [
        " <  [7.1. ONNX/jit/Caffe export](07_01_mobilenet_ONNX_export.ipynb) | [8. Podsumowanie i wnioski](08_00_Podsumowanie.ipynb)  |  [9. Bibliografia](Bibliografia.ipynb)  >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1QsrZV2S_oc"
      },
      "source": [
        "## 8. Podsumowanie i wnioski"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u3NM96_44-B"
      },
      "source": [
        "W niniejszej pracy opracowano i przeanalizowano modele umożliwiające detekcję twarzy w oparciu o architekturę Faster R-CNN. W trakcie pracy zastosowano dwa rozwiązania.<br>\n",
        "\n",
        "W pierwszym wykorzystano Faster R-CNN z FPN ResNet50. W oparciu o to rozwiązanie\n",
        "zostały wytrenowane 2 modele: faster_rcnn_R_50_FPN_3x pretrenowany na datasecie COCO train2017, a następnie dotrenowany na zbiorze treningowym WIDER FACE, oraz model scratch_faster_rcnn_R_50_FPN_gn trenowany od zera na zbiorze treningowym WIDER FACE. Kierując się opisem procedury trenowania zaproponowanej w artykule \"Rethinking ImageNet Pre-training\" [15] udało się uzyskać najlepszy wynik na zbiorze WIDER FACE i porównywalne z innymi modelami na zbiorze FACES_DD. Co najważniejsze model trenowany od zera uzyskał lepsze wyniki niż pretrenowany. Udało się zatem skutecznie zastosować zaproponowaną metodę trenowania\n",
        "uzyskując lepsze wyniki.<br>\n",
        "\n",
        "W drugim rozwiązaniu do detekcji twarzy wykorzystano Faster R-CNN MobileNet V2.\n",
        "W czasie treningu zastosowano nowoczesne rozwiązania pozwalające na szybsze i stabilniejsze trenowanie sieci takie jak: Frozen Batch Normalization, Batch Normalization, funkcję aktywacji Mish.\n",
        "Dzięki zastosowanej procedurze trenowania uzyskano poprawę wyników o 2pp. Wykazano, iż odchudzanie sieci poprzez separowanie warstw konwolucyjnych, nie wpływa znacząco na pogorszenie wyników. Wytrenowany model wykorzystujący MobileNet V2 lepiej poradził sobie z obrazami o dużej rozdzielczości. Na przygotowanej bazie zdjęć FACES_DD o dużej rozdzielczości\n",
        "(średnio 6Mpix) uzyskał najlepsze wyniki.<br><br>\n",
        "\n",
        "Wytrenowane sieci neuronowe zostały przebadane pod względem skuteczności i porównane.<br>\n",
        "Zasadniczo uzyskano dobre wyniki mAP, mAP50, time, które nie odbiegają od wyników osiąganych przez inne zespoły pracujące na datasecie WIDER FACE.<br>\n",
        "\n",
        "Potwierdzono również skuteczność przygotowanego modułu mAP. W ramach porównań\n",
        "zaobserwowano rozbieżności z pomiarem Detectron2 na poziomie poniżej 1pp, a jednocześnie moduł mAP umożliwiał w prosty sposób wykonanie pomiarów dla sieci zewnętrznych np. MTCNN.<br>\n",
        "\n",
        "Podsumowując, stosując procedury trenowania wskazane z artykule \"Rethinking ImageNet Pre-training\" oraz takie techniki jak Batch normalization, Frozen Batch Normalization, czy funkcja aktywacji typu Mish, ostatecznie uzyskano poprawę wyników mAP50 o 12pp dla WIDER FACE oraz 20pp dla FACES DD, przy 8 krotnym przyspieszeniu procesu trenowania stosunku do punktu wyjścia, jakim były modele pretrenowane.<br>\n",
        "\n",
        "Pracę kończy eksport stworzonych modeli do formatów\n",
        "ONNX i torch.jit, co w przyszłości pozwoli na ich produkcyjne zastosowanie np. w środowisku OpenVINO, Redis-AI, czy Triton Inference Server."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrZyY8TA9cbC"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}