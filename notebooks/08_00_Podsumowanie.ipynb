{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "08_00_Podsumowanie.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarekGit/FACES_DNN/blob/master/notebooks/08_00_Podsumowanie.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlFNzbPN9W1R",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "[Spis treści](https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Praca_Dyplomowa.ipynb) | [1. Wstęp](01_00_Wstep.ipynb) | [2. Metryki oceny detekcji](02_00_Miary.ipynb) | [3. Bazy danych](03_00_Datasety.ipynb) | [4. Przegląd metod detekcji](04_00_Modele.ipynb) | [5. Detekcja twarzy z wykorzystaniem wybranych architektur GSN](05_00_Modyfikacje.ipynb) | [6. Porównanie modeli](06_00_Porownanie.ipynb) | [7. Eksport modelu](07_00_Eksport_modelu.ipynb) | [8. Podsumowanie i wnioski](08_00_Podsumowanie.ipynb) | [Bibliografia](Bibliografia.ipynb)\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4TgijzHQPws",
        "colab_type": "text"
      },
      "source": [
        " <  [7.1. ONNX/jit/Caffe export](07_01_mobilenet_ONNX_export.ipynb) | [8. Podsumowanie i wnioski](08_00_Podsumowanie.ipynb)  |  [9. Bibliografia](Bibliografia.ipynb)  >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1QsrZV2S_oc",
        "colab_type": "text"
      },
      "source": [
        "## 8. Podsumowanie i wnioski"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdPF3m9BQc25",
        "colab_type": "text"
      },
      "source": [
        "W niniejszej pracy opracowano i przeanalizowano modele umożliwiające detekcję twarzy w oparciu o architekturę Faster R-CNN. W trakcie pracy zastosowano dwa rozwiązania.\n",
        "\n",
        "\n",
        "W pierwszym wykorzystano Faster R-CNN z FPN Resnet50. W oparciu o to rozwiązanie zostały wytrenowane 2 modele: faster_rcnn_R_50_FPN_3x pretrenowany na datasecie COCO train2017, a następnie dotrenowany na zbiorze treningowym WIDER FACE oraz scratch_faster_rcnn_R_50_FPN_gn trenowany od zera na zbiorze treningowym WIDER FACE. \n",
        "Kierując się opisem procedury trenowania zaproponowanej w artykule \"Rethinking ImageNet Pre-training\" <a href=\"https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Bibliografia.ipynb\">[15]</a> udało się uzyskać najlepszy wynik na zbiorze WIDER FACE i porównywalne z innymi modelami na zbiorze FACES_DD. Co najważniejsze model trenowany od zera uzyskał lepsze wyniki niż pretrenowany. Udało się zatem skutecznie zastosować zaproponowaną metodę trenowania uzyskując lepsze wyniki. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15YadEQoT5vY",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "W drugim rozwiązaniu do detekcji twarzy wykorzystano Faster R-CNN MobileNet \n",
        "V2.\n",
        "W czasie treningu zastosowano nowoczesne rozwiązania pozwalające na szybsze i stabilniejsze trenowanie sieci takie jak: Frozen Batch Normalization, Batch normalization, funkcję aktywacji Mish.\n",
        "Dzięki zastosowanej procedurze trenowania uzyskano poprawę wyników o 2pp. \n",
        "Wykazano, iż odchudzanie sieci poprzez separowanie warstw konwolucyjnych, nie wpływa znacząco na pogorszenie wyników. Wytrenowany model wykorzystujący MobileNet V2 lepiej poradził sobie z obrazami o dużej rozdzielczości.\n",
        "Na przygotowanej bazie zdjęć FACES_DD o dużej rozdzielczości (średnio 6Mpix) uzyskał najlepsze wyniki.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX-oBv6VQgwW",
        "colab_type": "text"
      },
      "source": [
        "Wytrenowane sieci neuronowe zostały przebadane pod względem skuteczności i porównane. Zasadniczo uzyskano dobre wyniki mAP, mAP50, time, które nie odbiegają od wyników osiąganych przez inne zespoły pracujące na datasecie WIDER FACE.\n",
        "\n",
        "\n",
        "\n",
        "Potwierdzono również skuteczność przygotowanego modułu mAP. W ramach porównań Zaobserwowano rozbieżności z pomiarem Detectron2 na poziomie poniżej 1pp, a jednocześnie moduł mAP umożliwiał w prosty sposób wykonanie pomiarów dla sieci zewnętrznych np. MTCNN.\n",
        "\n",
        "\n",
        "Udało się również przeprowadzić eksport modeli i wykazać skuteczność działania na przykładzie TorchSrcipt (jit).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SrZyY8TA9cbC"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}