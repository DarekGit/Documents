{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_04_MOBILENETV2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNufPDVyfwC6i4jW4QLhjks",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarekGit/FACES_DNN/blob/master/notebooks/04_04_MOBILENETV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jctTuqA2dPF-",
        "colab_type": "text"
      },
      "source": [
        "[Spis Treści](https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Praca_Dyplomowa.ipynb)<br>\n",
        "[4. Metody detekcji](04_00_Modele.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0SpMDIRkxqy",
        "colab_type": "text"
      },
      "source": [
        "##MobileNet version 2 \n",
        "W 2017 roku Zespół Google z dużym sukcesem wprowadził MobileNet V1 do klasyfikacji obrazów jak i ekstracji cech w większych sieciach neuronowych.\n",
        "\n",
        "<br> Już rok później w 2018 zespół AI Google <a href=\"https://research.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html\"> powiadomił o wprowadzeniu MobileNet version 2</a>. <br>Na bazie V1 udało sie stworzyć jeszcze bardziej efektywną i wydajną wersję, **do zostosowania na urządzeniach mobilnych.**\n",
        "\n",
        "\n",
        "###<br>Główne cechy V1\n",
        "Podstawowym pomysłem w V1 było wykorzystanie sperowalnych sieci konwolucyjnych w celu obniżenie zapotrzebowania na moc obliczniową. Standardowe sieci konwolucyjne wymagają wykonania dużej liczby obliczeń, których ilość jest proporcjonalny do iloczynu ilośc kanałów wejściowych, ilości kanałów wyjściowych, wielkości danych wejściowych i wielkości kernela. Zastosowanie konwolucji separowalnych po kanałach, obniża zapotrzebowanie na moc obliczeniową klikukrotnie.\n",
        "\n",
        "Podstawowa warstwa konwolucyjna została podzielona na dwa etapy: pierwszy to warstwa <a href=\"https://machinethink.net/blog/googles-mobile-net-architecture-on-iphone/\">konwolucji separowanej po kanałach (depthwise convolution)</a>, po której mamy warstwę konwolucji 1×1 (pointwise) łączącej cechy z kanałów:\n",
        "\n",
        "<div align=\"center\"><br>\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/MN2/MN2_SeparableConv.png\" alt=\"A depthwise separable convolution block\" width=\"600\" ><br>\n",
        "</div>\n",
        "\n",
        "<br>Razem obie warstwy tworzą blok konwolucyjny, który wykonuje podobne operacje do standardowej sieci konwolucyjnej, ale znacznie szybciej. (podobne rozwiązania stosujemy w sieciach typu Inception).\n",
        "<br>Architektura Mobilenet V1 składa się z standardowej warstwy konwolucyjnej 3x3 a następnie z trzynastu bloków opisanych powyżej.\n",
        "\n",
        "Pomiędzy warstawami konwolucji separowalnych nie stosujemy warstw Pool, zamist tego wybrane warsty konwolucji mają stride równe 2 w celu redukcji rozmiaru danych. Przy zastosowaniu redukcji rozmiaru danych, jednocześnie dublowana jest ilość kanałów wyjściowych w warstwie konwolucji 1x1 (pointwise). Przykładowo dla obrazu wejściowego o rozmiarze 224×224×3 na wyjściu sieci otrzymujemy mapę cech o rozmiarze 7×7×1024.\n",
        "\n",
        "Tak jak w większości współczesnych archiotektur, po warstwach konwolucyjnych stosujemy batch normalization, a jako funkcję aktywacji **ReLU6**. ReLU6 ma dodatkowe ograniczenie aby uniknąć zbyt dużych wartości:\n",
        "\n",
        "<pre><code class=\"language-python\">y = min(max(0, x), 6)\n",
        "</code></pre>\n",
        "\n",
        "Autorzy MobileNet w dokumentacji wskazują, iż sieć z ReLU6 jest bardziej odporna niż ReLU dla obliczeń z małą precyzją (16-bit float np iOS). \n",
        "\n",
        "Kształt ReLU6 ma zbiżone cechy do funckji sigmoid:\n",
        "\n",
        "<div align=\"center\"><br>\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/MN2/MN2_ReLU6.png\" alt=\"The ReLU6 activation function\" width=\"600\" ><br>\n",
        "</div>\n",
        "\n",
        "<br>Sieć MobileNet w zastosowaniach do klasyfikacji kończy się blokiem składającycm się z warstwy Average Pooling, FC lub alternatywnie Convolution 1x1 oraz Softmax.\n",
        "\n",
        "<br>Sieć Mobilenet została przygotowana jako rodziana sieci. Mamy kilka hiperparametrów umożliwiających stosowanie różnych wersji sieci Mobilenet.\n",
        "<br>Najważniejszym hiperparametrem jest  <strong>depth multiplier</strong>, czasami nazywany \"width multiplier\". Zmienia on ilość kanałów w każdej warstwie. \n",
        "<br>Stosując depth multiplier równy 0.5 obniżamy ilość kanałów, a tym samym ilość koniecznych obliczeń czterokrotnie oraz ilość parametrów sieci trzykrotnie, dzięki czemy sieć jest szybsza ale  mniej dokładna.\n",
        "\n",
        "<br>Dzięki zasosowaniu bloków z konwolucją separowalną MobileNet potrzebuje 9-krotnie mniej mocy obliczeniowej do porównywalnych sieci neuronowych z tą samą dokłądnością klasyfikacji. Taki typ sieci może być stosowany z powodzeniem w czasie rzeczywistym do 200 warstw na iPhone 6s.\n",
        "\n",
        "<br>Więcej szczegółów dostępne jest na stronie: https://arxiv.org/pdf/1704.04861.pdf\n",
        "\n",
        "###<br>Wersja 2 MobileNet\n",
        "\n",
        "<p><a href=\"https://arxiv.org/pdf/1801.04381.pdf\">MobileNet V2</a> bazuje na V1 i używa separowalnych warstw konwolucyjnych, a jego podstawowy blok wygląda następująco:\n",
        "\n",
        "<div align=\"center\"><br>\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/MN2/MN2_ResidualBlock.png\" alt=\"The bottleneck residual block\" width=\"600\" ><br>\n",
        "</div>\n",
        "\n",
        "<br>W V2 mamy trzy warstwy konwolucyjne w bloku, dwie ostatnie są zgodne z wersją V1, ale konwolucja 1x1 ma inne zadanie. W wersji V1 zadaniem konwolucji 1x1 (pointwise) było utrzymanie lub zdublowanie ilości kanałów. W przypadku wersji V2 jej zadaniem jest zmniejszenie ilości kanałów. Dlatego nazywana jest **projection layer**, odwzorowuje większą ilość kanałów wejsciowych na tensor z mniejszą ilością kanałów.\n",
        "<br>Przykładowo warstwa Depthwise może pracowanć ze 144 kanałami, które są ograniczane do tylko 24 w warstwie Pointwise. Czasami warstwę Pointwise nazywa się **bottlnect layer** poniewż zawęża ilośc danych przepływających przez sieć.\n",
        "\n",
        "<br> Pierwsza warstwa w bloku spełnia nowe zadanie. Jest to również warstwa konwolucyjna 1x1, a jej zadaniem jest rozszerzenie ilości kanałów danych przed warstwą DepthWise. Nazywa się ją **expansion layer** i zawsze zwiększa ilość kanałów (odwrotnie niż **projection layer**).\n",
        "Współczynnik ekspansji podawany jest jako hiperparametr i można go zmieniać. Domyślnie przyjmowany jest jako 6 (**expansion factor**).\n",
        "<br>Pryzkładowo dla 24 kanałów wejściowych do bloku, Expansion Layer powiększa ilość kanałów do 144, następnie wykonywana jest konwolucja separowalna (Depthwise) na 144 kanałach i ostatecznie Projection Layer redukuje ilość kanałów do 24.\n",
        "\n",
        "<div align=\"center\"><br>\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/MN2/MN2_Expand.png\" alt=\"Expansion and projection\" width=\"600\" ><br>\n",
        "</div>\n",
        "\n",
        "<br>Tak więc wejscie i wyjscie ma mniejszy rozmiar, podczas gdy rozmiar wewnątrz bloku jest większy.\n",
        "<br><br>Drugą nową cechą MobileNetV2 jest **residual connection**. Działa ono podobnie do ResNet i pomaga w utrzymaniu przepływu gradientu w sieci. Residual connection używane jest tylko i wyłądcznie w przypadku kiedy ilość kanałów wejściowych jest równa ilości kanałów wyjściowych dla bloku.\n",
        "\n",
        "<br>Jak w innych sieciach konwolucyjnych każda warstwa ma batch normalization i funkcję aktywacji (ReLU6), oprócz warstwy wyjściowej. Autorzy MobileNet w dokumentacji wskazują, iż warstwa nieliniowa na wyjściu dla małych rozmiarów danych wyjściowych pogarsza wyniki pracy sieci.\n",
        "\n",
        "<br> Pełna architektura MobileNetV2 składa się ze standardowej warstwy konwolucyjnej 3x3 z 32 kanałami, następnie 17  bloków, zakończonych standardową warstwą konwolucyjną 1x1, Average Pool i warstwą klasyfikacyjną.\n",
        "\n",
        "\n",
        "###Cel zmian\n",
        "\n",
        "<br>Pomysł dotyczący V1 opierał się na zmianie złożonej warstwy konwolucyjnej na mniej wymagająca warstwę separowalną **DeepWise**, co okazało sie dużym sukcesem. \n",
        "<br>Główną zmianą w wersji V2 jest zastosowanie **residual connection** oraz **expand/projection layers**, co umożliwia zachowanie małego rozmiaru na wejściu i wyjściu bloków:\n",
        "\n",
        "<div align=\"center\"><br>\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/MN2/MN2%20_LowDim.png\" alt=\"The tensor dimensions between the blocks\" width=\"600\" ><br>\n",
        "</div>\n",
        "\n",
        "<br> Rozmiar tensora wyściowego dla wersji V1 (7×7×1024) jest większy od V2. Dzięki zachowaniu małego rozmiaru tensora uzyskano redukcją koniecznej liczby obliczeń dla sieci przy zachowaniu zadawalającej jakości klasyfikacji sieci.\n",
        "<br>Podaje się porównanie, że mały rozmiar danych pomiędzy blokami odzwierciedla  skompresowane dane rzeczywiste, które są dekompresowane wewnątrz bloków:\n",
        "\n",
        "<div align=\"center\"><br>\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/MN2/MN2_Compression.png\" alt=\"Decompression and compression inside the bottleneck residual block\" width=\"600\" ><br>\n",
        "</div>\n",
        "\n",
        "<br> Expansion Layer działa jako dekompresor, który odtwarza dane rzeczywiste do przetwarzania wewnątrz bloku, a które są następnie kompresowane w Projection Layer w celu zmniejszenia rozmiaru danych na wyjściu bloku. Parametry całego procesu w tym kompresji i dekompresji podlegają optymalizacji w trakcie uczenia sieci.\n",
        "\n",
        "\n",
        "###<br>Porówanie wersji MobileNet.\n",
        "\n",
        "<br>Porównanie MobileNet V1 to V2, wymagana ilość obliczeń oraz rozmiar modelu - ilość parametrów:\n",
        "\n",
        "<table>\n",
        "<thead>\n",
        "<tr>\n",
        "<th>Version</th>\n",
        "<th>MACs (millions)</th>\n",
        "<th>Parameters (millions)</th>\n",
        "</tr>\n",
        "</thead>\n",
        "\n",
        "<tbody>\n",
        "<tr>\n",
        "<td>MobileNet V1</td>\n",
        "<td>569</td>\n",
        "<td>4.24</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>MobileNet V2</td>\n",
        "<td>300</td>\n",
        "<td>3.47</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "\n",
        "<br>Wykorzystano dane z <a href=\"https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet_v1.md\">Github MNV1</a> oraz  <a href=\"https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet\">Gothub MNV2</a>. Modele w wersji z depth multiplier równe 1.0. Dla podanej tabeli, mniejsze liczby oznaczają lepszy wynik.\n",
        "\n",
        "\"MACs - multiply-accumulate operations\". Pokazuje ile obliczeń jest wymaganych dla wykonania operacji dla sieci na pojedyńczym obrazie RGB o wymiarach 224x224.\n",
        "V2 ma dwie przewagi: \n",
        "<br>1. potrzebuje prawie dwa razy mniejszą ilość obliczeń.\n",
        "<br>2. ma mniej parametrów (80%) dzięki czemu szybciej ładuje je z pamięci, co jest istotne dla urządzeń mobilnych, gdzie dostęp do pamięci jest znacznie wolniejszy niż obliczenia.\n",
        "\n",
        "<br>Poniższa tabela pokazuje maksymalny FPS (frames-per-second) dla różnych urządzeń dla wersji modeli jak wyżej:</p>\n",
        "\n",
        "<table>\n",
        "<thead>\n",
        "<tr>\n",
        "<th>Version</th>\n",
        "<th>iPhone 7</th>\n",
        "<th>iPhone X</th>\n",
        "<th>iPad Pro 10.5</th>\n",
        "</tr>\n",
        "</thead>\n",
        "\n",
        "<tbody>\n",
        "<tr>\n",
        "<td>MobileNet V1</td>\n",
        "<td>118</td>\n",
        "<td>162</td>\n",
        "<td>204</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>MobileNet V2</td>\n",
        "<td>145</td>\n",
        "<td>233</td>\n",
        "<td>220</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "\n",
        "<br> Wyniki uzyskano przy zastosowaniu optymalizacji polegającej na równoczesnym przygtowaniu danych wejściowych na CPU w trakcie przetwarzania danych na GPU.\n",
        "\n",
        "<br>Porównanie jakości pracy modeli:\n",
        "\n",
        "<table>\n",
        "<thead>\n",
        "<tr>\n",
        "<th>Version</th>\n",
        "<th>Top-1 Accuracy</th>\n",
        "<th>Top-5 Accuracy</th>\n",
        "</tr>\n",
        "</thead>\n",
        "\n",
        "<tbody>\n",
        "<tr>\n",
        "<td>MobileNet V1</td>\n",
        "<td>70.9</td>\n",
        "<td>89.9</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>MobileNet V2</td>\n",
        "<td>71.8</td>\n",
        "<td>91.0</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "\n",
        "<br> Więcej wyników dostępne jest na stronie: http://www.image-net.org/challenges/LSVRC/2012/\n",
        "\n",
        "\n",
        "\n",
        "##<br>WNIOSKI:\n",
        "<br>**MobileNet V2 dla wszyskich miar mam lepsze wynik od MobileNet V1. Zarówno jest szybszy jak również ma lepszą dokładaność dla klasyfikacji.**\n",
        "<br> W innych pracach wskazano, że może być również stosowany do detekcji obiektów i segmentacji obrazów.\n",
        "\n",
        "<br>Dla klasyfikacji ostatni blok wygląda jak na rysunku poniżej:\n",
        "\n",
        "<div align=\"center\"><br>\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/MN2/MN2_Classifier.png\" alt=\"Classifier head on top of the base network\" width=\"600\" >\n",
        "</div>\n",
        "\n",
        "\n",
        "<br>Użycie MobileNet w SSDlite do detekcji, może wyglądać następująco:\n",
        "\n",
        "\n",
        "<div align=\"center\"><br>\n",
        "<img src=\"https://github.com/DarekGit/FACES_DNN/blob/master/Figures/MN2/MN2_FeatureExtractor.png\" alt=\"Using MobileNet as a feature extractor in a larger network\" width=\"600\" ><br>\n",
        "</div>\n",
        "\n",
        "<br>W powyższym rozwiązaniu wykorzystano nie tylko cechy z ostaniej warstwy MobileNet, ale również kilka wcześniejszych. Na pokazynym przykładzie MobileNet wykorzystwany jest jako ekstraktor cech obrazu.\n",
        "\n",
        "<br>**W dalszej części pracy zastosowano MobileNet V2 w detekcji twarzy w modelu Detectron2 z bardzo dobrym wynikiem.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVPlwJxIdZ8s",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgfIP_1pV4tx",
        "colab_type": "text"
      },
      "source": [
        "https://analyticsindiamag.com/why-googles-mobilenetv2-is-a-revolutionary-next-gen-on-device-computer-vision-network/\n",
        "\n",
        "https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofHitxEnV4kL",
        "colab_type": "text"
      },
      "source": [
        "Recently, a group of researchers from Google released a neural network architecture MobileNetV2, which is optimised for mobile devices. The architecture delivers high accuracy results while keeping the parameters and mathematical operations as low as possible to bring deep neural networks to mobile devices.\n",
        "\n",
        "Last year, the company introduced MobileNetV1 for Tensorflow, designed to support classification, detection, embedding and segmentation. “The ability to run deep networks on personal mobile devices improves user experience, offering anytime, anywhere access, with additional benefits for security, privacy, and energy consumption. As new applications emerge allowing users to interact with the real world in real time, so does the need for ever more efficient neural networks,” Google researchers Mark Sandler and Andrew Howard said in their research blog post.\n",
        "\n",
        "\n",
        "The new mobile architecture, MobileNetV2 is the improved version of MobileNetV1 and is released as a part of TensorFlow-Slim Image Classification Library. Developers can even access it in Colaboratory or can download the notebook and explore it using Jupyter. It is also available as modules on TensorFlow-Hub. The pretrained checkpoints can be found on the open source platform GitHub.\n",
        "\n",
        "What Is MobileNetV2?\n",
        "MobileNets are small, low-latency, low-power models parameterised to meet the resource constraints of a variety of use cases. According to the research paper, MobileNetV2 improves the state-of-the-art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. It is a very effective feature extractor for object detection and segmentation. For instance, for detection, when paired with Single Shot Detector Lite, MobileNetV2 is about 35 percent faster with the same accuracy than MobileNetV1.\n",
        "\n",
        "It builds upon the ideas from MobileNetV1, using depth-wise separable convolutions as efficient building blocks. However, Google says that the 2nd version of MobileNet has two new features:\n",
        "\n",
        "Linear bottlenecks between the layers: Experimental evidence suggests that using linear layers is crucial as it prevents nonlinearities from destroying too much information. Using non-linear layers in bottlenecks indeed hurts the performance by several percent, further validating our hypothesis\n",
        "Shortcut connections between the bottlenecks\n",
        "The Basic Structure of MobileNetV2\n",
        "The bottlenecks of the MobileNetV2 encode the intermediate inputs and outputs while the inner layer encapsulates the model’s ability to transform from lower-level concepts such as pixels to higher level descriptors such as image categories. With traditional residual connections, shortcuts enable faster training and better accuracy.\n",
        "\n",
        "\n",
        "\n",
        "Model Architecture\n",
        "The basic building block is a bottleneck depth-separable convolution with residuals. The architecture of MobileNetV2 contains the initial fully convolution layer with 32 filters, followed by 19 residual bottleneck layers. The researchers have tailored the architecture to different performance points, by using the input image resolution and width multiplier as tunable hyperparameters, that can be adjusted depending on desired accuracy or performance trade-offs. The primary network  (width multiplier 1, 224 × 224), has a computational cost of 300 million multiply-adds and uses 3.4 million parameters. The network computational cost ranges from 7 multiply-adds to 585M MAdds, while the model size varies between 1.7M and 6.9M parameters.\n",
        "\n",
        "How Is It Different From MobileNetV1?\n",
        "The MobileNetV2 models are much faster in comparison to MobileNetV1. It uses 2 times fewer operations, has higher accuracy, needs 30 percent fewer parameters and is about 30-40 percent faster on a Google pixel phone.\n",
        "\n",
        "SEE ALSO\n",
        "google\n",
        "Google Announces New AutoML Vision Edge & AutoML Video Upgrades\n",
        "\n",
        "\n",
        "\n",
        "To enable on-device semantic segmentation, the researcher used MobileNetV2 as a feature extractor in a reduced form of DeepLabv3 that controls the resolution of computed feature maps. On the semantic segmentation benchmark, PASCAL VOC 2012, MobileNetV2 performed similar to MobileNetV1 as feature extractor, but the V2 version requires 5.3 times fewer parameters and 5.2 times fewer operations in terms of multiply-adds.\n",
        "\n",
        "\n",
        "On A Concluding Note\n",
        "The new version of MobileNet has several properties that make it suitable for mobile applications and allows very memory-efficient inference and utilises standard operations present in all neural frameworks. For the ImageNet dataset, MobileNetV2 improves the state of the art for a wide range of performance points. For object detection task, it outperforms real-time detectors on COCO datasets. MobileNetV2 provides a very efficient mobile-oriented model that can be used as a base for many visual recognition tasks, claims Google."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geVts4o5V3WY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbCPJdaLXqfj",
        "colab_type": "text"
      },
      "source": [
        "https://towardsdatascience.com/review-mobilenetv2-light-weight-model-image-classification-8febb490e61c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRHMyf47XrTA",
        "colab_type": "text"
      },
      "source": [
        "Review: MobileNetV2 — Light Weight Model (Image Classification)\n",
        "Outperforms MobileNetV1, NASNet, and ShuffleNet V1\n",
        "Sik-Ho Tsang\n",
        "Sik-Ho Tsang\n",
        "Follow\n",
        "May 19, 2019 · 5 min read\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Image for post\n",
        "MobileNetV2 for Mobile Devices\n",
        "Inthis story, MobileNetV2, by Google, is briefly reviewed. In the previous version MobileNetV1, Depthwise Separable Convolution is introduced which dramatically reduce the complexity cost and model size of the network, which is suitable to Mobile devices, or any devices with low computational power. In MobileNetV2, a better module is introduced with inverted residual structure. Non-linearities in narrow layers are removed this time. With MobileNetV2 as backbone for feature extraction, state-of-the-art performances are also achieved for object detection and semantic segmentation. This is a paper in 2018 CVPR with more than 200 citations. (Sik-Ho Tsang @ Medium)\n",
        "Outline\n",
        "MobileNetV2 Convolutional Blocks\n",
        "Overall Architecture\n",
        "Ablation Study\n",
        "Experimental Results\n",
        "1. MobileNetV2 Convolutional Blocks\n",
        "Image for post\n",
        "1.1. MobileNetV1\n",
        "In MobileNetV1, there are 2 layers.\n",
        "The first layer is called a depthwise convolution, it performs lightweight filtering by applying a single convolutional filter per input channel.\n",
        "The second layer is a 1×1 convolution, called a pointwise convolution, which is responsible for building new features through computing linear combinations of the input channels.\n",
        "ReLU6 is used here for comparison. (Actually, in MobileNetV1 tech report, I cannot find any hints that they use ReLU6… Maybe we need to check the codes in Github…), i.e. min(max(x, 0), 6) as follows:\n",
        "Image for post\n",
        "ReLU6\n",
        "ReLU6 is used due to its robustness when used with low-precision computation, based on [27] MobileNetV1.\n",
        "1.2. MobileNetV2\n",
        "In MobileNetV2, there are two types of blocks. One is residual block with stride of 1. Another one is block with stride of 2 for downsizing.\n",
        "There are 3 layers for both types of blocks.\n",
        "This time, the first layer is 1×1 convolution with ReLU6.\n",
        "The second layer is the depthwise convolution.\n",
        "The third layer is another 1×1 convolution but without any non-linearity. It is claimed that if ReLU is used again, the deep networks only have the power of a linear classifier on the non-zero volume part of the output domain.\n",
        "Image for post\n",
        "And there is an expansion factor t. And t=6 for all main experiments.\n",
        "If the input got 64 channels, the internal output would get 64×t=64×6=384 channels.\n",
        "2. Overall Architecture\n",
        "Image for post\n",
        "MobileNetV2 Overall Architecture\n",
        "where t: expansion factor, c: number of output channels, n: repeating number, s: stride. 3×3 kernels are used for spatial convolution.\n",
        "In typical, the primary network (width multiplier 1, 224×224), has a computational cost of 300 million multiply-adds and uses 3.4 million parameters. (Width multiplier is introduced in MobileNetV1.)\n",
        "The performance trade offs are further explored, for input resolutions from 96 to 224, and width multipliers of 0.35 to 1.4.\n",
        "The network computational cost up to 585M MAdds, while the model size vary between 1.7M and 6.9M parameters.\n",
        "To train the network, 16 GPU is used with batch size of 96.\n",
        "Image for post\n",
        "Number of Maximum Channels/Memory in Kb) at Each Spatial Resolution for Different Architecture with 16-bit floats for activation\n",
        "3. Ablation Study\n",
        "3.1. Impact of Linear Bottleneck\n",
        "Image for post\n",
        "With the removal of ReLU6 at the output of each bottleneck module, accuracy is improved.\n",
        "3.2. Impact of Shortcut\n",
        "Image for post\n",
        "With shortcut between bottlenecks, it outperforms shortcut between expansions and the one without any residual connections.\n",
        "4. Experimental Results\n",
        "Image for post\n",
        "MobileNetV2 for Classification, Detection and Segmentation (From https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html)\n",
        "4.1. ImageNet Classification\n",
        "Image for post\n",
        "ImageNet Top-1 Accuracy\n",
        "MobileNetV2 outperforms MobileNetV1 and ShuffleNet (1.5) with comparable model size and computational cost.\n",
        "With width multiplier of 1.4, MobileNetV2 (1.4) outperforms ShuffleNet (×2), and NASNet with faster inference time.\n",
        "Image for post\n",
        "As shown above, different input resolutions and width multipliers are used. It consistently outperforms MobileNetV1.\n",
        "4.2. MS COCO Object Detection\n",
        "Image for post\n",
        "SSDLite\n",
        "First, SSDLite is introduced by modifying the regular convolutions in SSD with depthwise separable convolutions (MobileNetV1 one).\n",
        "SSDLite dramatically reduces both parameter count and computational cost.\n",
        "Image for post\n",
        "MS COCO Object Detection\n",
        "MobileNetV2 + SSDLite achieves competitive accuracy with significantly fewer parameters and smaller computational complexity.\n",
        "And the inference time is faster than MobileNetV1 one.\n",
        "Notably, MobileNetV2 + SSDLite is 20× more efficient and 10× smaller while still outperforms YOLOv2 on COCO dataset.\n",
        "4.3. PASCAL VOC 2012 Semantic Segmentation\n",
        "Image for post\n",
        "PASCAL VOC 2012 Validation Set\n",
        "Here, MobileNetV2 is used as feature extractor for DeepLabv3.\n",
        "With the disabling of Atrous Spatial Pyramid Pooling (ASPP) as well as Multi-Scale and Flipping (MP), also changing the output stride from 8 to 16, mIOU of 75.32% is obtained, with far low of model size and computational cost.\n",
        "Reference\n",
        "[2018 CVPR] [MobileNetV2]\n",
        "MobileNetV2: Inverted Residuals and Linear Bottlenecks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vLcPAnjdbKg",
        "colab_type": "text"
      },
      "source": [
        "[5. Modyfikacje](05_00_Modyfikacje.ipynb)<br>\n",
        "[Spis Treści](https://github.com/DarekGit/FACES_DNN/blob/master/notebooks/Praca_Dyplomowa.ipynb)"
      ]
    }
  ]
}