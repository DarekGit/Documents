{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"WIDERFACE_Detectron2_DD_DeTr_R50.ipynb","provenance":[{"file_id":"https://github.com/barad007/Face-Detection-and-Recognition/blob/master/WIDER_FACE/WIDERFACE_Detectron2_from_scratch.ipynb","timestamp":1589021771753}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"L8l7TjawuG71","colab_type":"text"},"source":["Detectron2 z DeTr z Resnet50\n","\n","LR=1-4 \n","\n","Gamma=0.1\n","\n","Scheduler 3/1200k \n","\n","Max 1500k"]},{"cell_type":"markdown","metadata":{"id":"yC-BR8L2E3X2","colab_type":"text"},"source":["https://arxiv.org/pdf/2005.12872.pdf"]},{"cell_type":"markdown","metadata":{"id":"7CHNa32TdpuN","colab_type":"text"},"source":["# Install detectron2\n","\n","Detectron2  https://github.com/facebookresearch/detectron2 <br>\n","Detectron2 Beginner's Tutorial https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5 <br>\n","Documentation https://detectron2.readthedocs.io <br>\n","Detectron2 Model Zoo and Baselines https://github.com/facebookresearch/detectron2/blob/master/MODEL_ZOO.md <br>\n","Rethinking ImageNet Pre-training https://arxiv.org/pdf/1811.08883.pdf <br>\n","\n","Wykorzystano kody z <br>\n","https://github.com/youngwanLEE/vovnet-detectron2"]},{"cell_type":"code","metadata":{"id":"EVOnv0H2xbTP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":118},"executionInfo":{"status":"ok","timestamp":1597779527744,"user_tz":-120,"elapsed":26344,"user":{"displayName":"Dariusz Działkowski","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjk19eQEsSx6603O8FXwgnrAlD6uVS5VP9UHjgGXw=s64","userId":"17377559919401364457"}},"outputId":"212217f9-c6bd-4005-d48f-5492aab064d7"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ms2PvHwVPYRS","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597779501887,"user_tz":-120,"elapsed":1703,"user":{"displayName":"Dariusz Działkowski","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjk19eQEsSx6603O8FXwgnrAlD6uVS5VP9UHjgGXw=s64","userId":"17377559919401364457"}}},"source":["import os\n","def Wider_load(val=True,train=True,test=False):\n","  os.makedirs('WIDER/', exist_ok=True)\n","\n","  if val:\n","    #!gdown https://drive.google.com/uc?id=0B6eKvaijfFUDd3dIRmpvSk8tLUk\n","    !gdown https://drive.google.com/uc?id=1-5A_pa_jDS7gk8mHVCBB7ApV5KN8jWDr -O WIDER/tempv.zip\n","    !unzip -q WIDER/tempv.zip -d WIDER\n","    !rm WIDER/tempv.zip  \n","\n","  if train:\n","    ### WIDER Face Training Images\n","    #!gdown https://drive.google.com/uc?id=0B6eKvaijfFUDQUUwd21EckhUbWs\n","    !gdown https://drive.google.com/uc?id=1-1iJfmXKYvAx9uLdRDX5W6HHG_KZv1jH -O WIDER/temptr.zip\n","    !unzip -q WIDER/temptr.zip -d WIDER\n","    !rm WIDER/temptr.zip\n","  \n","  if test:\n","    #!gdown https://drive.google.com/uc?id=0B6eKvaijfFUDbW4tdGpaYjgzZkU\n","    !gdown https://drive.google.com/uc?id=1tTpUJZEQMKDVxKT6100V5FwDuGX_8sDi -O WIDER/tempt.zip\n","    !unzip -q WIDER/tempt.zip -d WIDER\n","    !rm WIDER/tempt.zip\n","\n","\n","  ### Face annotations\n","  !wget mmlab.ie.cuhk.edu.hk/projects/WIDERFace/support/bbx_annotation/wider_face_split.zip -O WIDER/tempa.zip\n","  !unzip -q WIDER/tempa.zip -d WIDER\n","  !rm WIDER/tempa.zip\n","\n","  #annotations\n","  !gdown https://drive.google.com/uc?id=1_9ydMZlTNFXBOMl16xsU8FSBmK2PW4lN -O WIDER/tools.py\n","\n","\n","  ### Examples and formats of the submissions\n","  #!wget mmlab.ie.cuhk.edu.hk/projects/WIDERFace/support/example/Submission_example.zip\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZTdlDENQMRB4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":456},"executionInfo":{"status":"ok","timestamp":1597779749064,"user_tz":-120,"elapsed":28980,"user":{"displayName":"Dariusz Działkowski","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjk19eQEsSx6603O8FXwgnrAlD6uVS5VP9UHjgGXw=s64","userId":"17377559919401364457"}},"outputId":"35d16a07-57a2-4e82-a8e5-91163b140323"},"source":["def repo_load():\n","  !pip install cython pyyaml==5.1\n","  !pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n","\n","  # install detectron2:\n","  !git clone https://github.com/facebookresearch/detectron2 detectron2_repo\n","  !cd detectron2_repo && git reset a33fc53 --hard # v0.2\n","  !pip install -q -e detectron2_repo\n","\n","\n","repo_load()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (0.29.21)\n","Requirement already satisfied: pyyaml==5.1 in /usr/local/lib/python3.6/dist-packages (5.1)\n","Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n","  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-nsboob2w\n","  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-nsboob2w\n","Requirement already satisfied, skipping upgrade: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (49.2.0)\n","Requirement already satisfied, skipping upgrade: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (0.29.21)\n","Requirement already satisfied, skipping upgrade: matplotlib>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (3.2.2)\n","Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.2.0)\n","Requirement already satisfied, skipping upgrade: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.18.5)\n","Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.10.0)\n","Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.1)\n","Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.4.7)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools==2.0) (1.15.0)\n","Building wheels for collected packages: pycocotools\n","  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycocotools: filename=pycocotools-2.0-cp36-cp36m-linux_x86_64.whl size=266458 sha256=a7358102284f8d761adf50aaab40ed73f607d24c71cd6dc53f0c738eb51ab36c\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-8eo8rn6l/wheels/90/51/41/646daf401c3bc408ff10de34ec76587a9b3ebfac8d21ca5c3a\n","Successfully built pycocotools\n","\u001b[31mERROR: detectron2 0.2 has requirement pycocotools>=2.0.1, but you'll have pycocotools 2.0 which is incompatible.\u001b[0m\n","Installing collected packages: pycocotools\n","  Found existing installation: pycocotools 2.0.1\n","    Uninstalling pycocotools-2.0.1:\n","      Successfully uninstalled pycocotools-2.0.1\n","Successfully installed pycocotools-2.0\n","fatal: destination path 'detectron2_repo' already exists and is not an empty directory.\n","HEAD is now at a33fc53 release 0.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MpCvj21UPayq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"status":"ok","timestamp":1597779826281,"user_tz":-120,"elapsed":106179,"user":{"displayName":"Dariusz Działkowski","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjk19eQEsSx6603O8FXwgnrAlD6uVS5VP9UHjgGXw=s64","userId":"17377559919401364457"}},"outputId":"690e0854-a148-42fe-877a-f63062afe6ae"},"source":["Wider_load()"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1-5A_pa_jDS7gk8mHVCBB7ApV5KN8jWDr\n","To: /content/WIDER/tempv.zip\n","363MB [00:06, 56.4MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1-1iJfmXKYvAx9uLdRDX5W6HHG_KZv1jH\n","To: /content/WIDER/temptr.zip\n","1.47GB [00:23, 62.6MB/s]\n","--2020-08-18 19:43:36--  http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/support/bbx_annotation/wider_face_split.zip\n","Resolving mmlab.ie.cuhk.edu.hk (mmlab.ie.cuhk.edu.hk)... 137.189.99.12\n","Connecting to mmlab.ie.cuhk.edu.hk (mmlab.ie.cuhk.edu.hk)|137.189.99.12|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3591642 (3.4M) [application/zip]\n","Saving to: ‘WIDER/tempa.zip’\n","\n","WIDER/tempa.zip     100%[===================>]   3.42M  --.-KB/s    in 0.1s    \n","\n","2020-08-18 19:43:36 (24.0 MB/s) - ‘WIDER/tempa.zip’ saved [3591642/3591642]\n","\n","Downloading...\n","From: https://drive.google.com/uc?id=1_9ydMZlTNFXBOMl16xsU8FSBmK2PW4lN\n","To: /content/WIDER/tools.py\n","100% 4.47k/4.47k [00:00<00:00, 3.97MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TQHoKyzznizS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":98},"executionInfo":{"status":"ok","timestamp":1597779494733,"user_tz":-120,"elapsed":6098,"user":{"displayName":"Dariusz Działkowski","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjk19eQEsSx6603O8FXwgnrAlD6uVS5VP9UHjgGXw=s64","userId":"17377559919401364457"}},"outputId":"49776d5d-81ad-495b-fe37-b56984b96f34"},"source":["!git clone https://github.com/DarekGit/detr.git  \n","#!git clone https://github.com/facebookresearch/detr.git"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Cloning into 'detr'...\n","remote: Enumerating objects: 182, done.\u001b[K\n","remote: Total 182 (delta 0), reused 0 (delta 0), pack-reused 182\u001b[K\n","Receiving objects: 100% (182/182), 12.82 MiB | 6.61 MiB/s, done.\n","Resolving deltas: 100% (85/85), done.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qzx4JAMasOcs","colab_type":"text"},"source":["<font color=red> Restart runtime to continue... <b>Crtl+M.</b> </font>"]},{"cell_type":"code","metadata":{"id":"oGYW4qkoKnHt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":375},"executionInfo":{"status":"ok","timestamp":1597780170084,"user_tz":-120,"elapsed":4674,"user":{"displayName":"Dariusz Działkowski","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjk19eQEsSx6603O8FXwgnrAlD6uVS5VP9UHjgGXw=s64","userId":"17377559919401364457"}},"outputId":"4335f520-c0df-41b2-f763-942a39987a38"},"source":["!nvidia-smi\n","from psutil import virtual_memory\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(virtual_memory().total / 1e9))"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Tue Aug 18 19:49:27 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   32C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","Your runtime has 27.4 gigabytes of available RAM\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ucz8h0sBtzkJ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597780171423,"user_tz":-120,"elapsed":6009,"user":{"displayName":"Dariusz Działkowski","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjk19eQEsSx6603O8FXwgnrAlD6uVS5VP9UHjgGXw=s64","userId":"17377559919401364457"}}},"source":["import torch, torchvision\n","import detectron2\n","from detectron2.utils.logger import setup_logger\n","setup_logger()\n","\n","from google.colab import drive\n","import os\n","import cv2\n","import random\n","import itertools\n","import shutil\n","import glob\n","import json\n","import numpy as np\n","import pandas as pd\n","from PIL import ImageDraw, Image\n","from collections import defaultdict\n","import matplotlib.pyplot as plt\n","\n","from google.colab.patches import cv2_imshow\n","\n","from detectron2 import model_zoo\n","import detectron2.utils.comm as comm\n","from detectron2.engine import DefaultPredictor, DefaultTrainer, HookBase\n","from detectron2.config import get_cfg\n","from detectron2.utils.visualizer import Visualizer\n","from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_train_loader\n","from detectron2.structures import BoxMode\n","from detectron2.data import build_detection_test_loader\n","from detectron2.data.datasets import register_coco_instances\n","from detectron2.evaluation import COCOEvaluator, inference_on_dataset"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"aSh762BHzDlP","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597780171424,"user_tz":-120,"elapsed":6007,"user":{"displayName":"Dariusz Działkowski","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjk19eQEsSx6603O8FXwgnrAlD6uVS5VP9UHjgGXw=s64","userId":"17377559919401364457"}}},"source":["from WIDER.tools import annotations,output_Files\n","output_files=output_Files()"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"uUhwnqHSqJRz","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597780171424,"user_tz":-120,"elapsed":6005,"user":{"displayName":"Dariusz Działkowski","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjk19eQEsSx6603O8FXwgnrAlD6uVS5VP9UHjgGXw=s64","userId":"17377559919401364457"}}},"source":["import sys\n","sys.path.append('/content/detr')\n","from d2.train_net import Trainer, setup\n","from d2.detr.config import add_detr_config"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XXMv27Egzvf5","colab_type":"text"},"source":["# Prepare the dataset\n","\n","## WIDER FACE: A Face Detection Benchmark\n","http://shuoyang1213.me/WIDERFACE/ <br>\n","\n","https://arxiv.org/pdf/1511.06523.pdf <br>\n"]},{"cell_type":"code","metadata":{"id":"RMwVqEK1v9zo","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597780175725,"user_tz":-120,"elapsed":10304,"user":{"displayName":"Dariusz Działkowski","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjk19eQEsSx6603O8FXwgnrAlD6uVS5VP9UHjgGXw=s64","userId":"17377559919401364457"}}},"source":["train = annotations(\"train\")\n","val = annotations('val')"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"OofELeX8yX5q","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597780175727,"user_tz":-120,"elapsed":10304,"user":{"displayName":"Dariusz Działkowski","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjk19eQEsSx6603O8FXwgnrAlD6uVS5VP9UHjgGXw=s64","userId":"17377559919401364457"}}},"source":["for d in [\"train\", \"val\"]:\n","  DatasetCatalog.register(\"face_\" + d, lambda d=d: train if d == \"train\" else val)\n","  MetadataCatalog.get(\"face_\" + d).set(thing_classes = ['face'])\n","\n","faces_metadata = MetadataCatalog.get(\"face_train\")"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"ciNM6NTrfvNK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"status":"ok","timestamp":1597780176099,"user_tz":-120,"elapsed":10670,"user":{"displayName":"Dariusz Działkowski","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjk19eQEsSx6603O8FXwgnrAlD6uVS5VP9UHjgGXw=s64","userId":"17377559919401364457"}},"outputId":"bce7deff-7d49-43d3-98a8-3d3243cf4799"},"source":["\n","import pandas as pd\n","hist=dict(); hists={}\n","for i in (range(len(train))):\n","  f=len(train[i]['annotations'])\n","  hist[f]= 1 +hist.get(f,0)\n","  for k in sorted(hist):hists[k]=hist[k]\n","\n","pd.DataFrame(sorted([*zip(hist.keys(),hist.values())]), index=None, columns=['ilosc twarzy','ilosc obrazow'])"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ilosc twarzy</th>\n","      <th>ilosc obrazow</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>4635</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1793</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>827</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>680</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>504</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>251</th>\n","      <td>883</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>252</th>\n","      <td>1001</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>253</th>\n","      <td>1146</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>254</th>\n","      <td>1750</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>255</th>\n","      <td>1968</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>256 rows × 2 columns</p>\n","</div>"],"text/plain":["     ilosc twarzy  ilosc obrazow\n","0               1           4635\n","1               2           1793\n","2               3            827\n","3               4            680\n","4               5            504\n","..            ...            ...\n","251           883              1\n","252          1001              1\n","253          1146              2\n","254          1750              1\n","255          1968              1\n","\n","[256 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"MyQ2-ALw54Qt","colab_type":"text"},"source":["# \"detr_256_6_6_torchvision.yaml\"\n","detr/configs\n"]},{"cell_type":"code","metadata":{"id":"JmaIYgDpVDAN","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597780176100,"user_tz":-120,"elapsed":10669,"user":{"displayName":"Dariusz Działkowski","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjk19eQEsSx6603O8FXwgnrAlD6uVS5VP9UHjgGXw=s64","userId":"17377559919401364457"}}},"source":["cfg = get_cfg()\n","add_detr_config(cfg)\n","\n","cfg_file='detr_256_6_6_torchvision.yaml'\n","cfg.merge_from_file('detr/d2/configs/'+cfg_file)\n","\n","cfg.MODEL.PIXEL_MEAN =(119.857,110.808,104.148)\n","#cfg.MODEL.PIXEL_STD =(77.168,74.631,75.842)\n","cfg.MODEL.PIXEL_STD =(0.6076259372440945, 0.5876417505511812, 0.5971849743307086)\n","cfg.INPUT.FORMAT='RGB'\n","cfg.MODEL.FORMAT='RGB'\n","cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES=1\n","cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class \n","cfg.MODEL.DETR.NUM_CLASSES = 1\n","cfg.MODEL.DETR.NUM_OBJECT_QUERIES = 2000"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"veaaqv4g1cpZ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597780176101,"user_tz":-120,"elapsed":10667,"user":{"displayName":"Dariusz Działkowski","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjk19eQEsSx6603O8FXwgnrAlD6uVS5VP9UHjgGXw=s64","userId":"17377559919401364457"}}},"source":["#!python detr/d2/converter.py --source_model https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth --output_model converted_model.pth >_"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"ILZuPLmQvZKO","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597780176101,"user_tz":-120,"elapsed":10665,"user":{"displayName":"Dariusz Działkowski","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjk19eQEsSx6603O8FXwgnrAlD6uVS5VP9UHjgGXw=s64","userId":"17377559919401364457"}}},"source":["cfg.DATASETS.TRAIN = (\"face_train\",)\n","cfg.DATASETS.TEST = (\"face_val\",)\n","cfg.DATASETS.VAL = (\"face_val\",)\n","cfg.DATALOADER.NUM_WORKERS = 8\n","\n","cfg.MODEL.RESNETS.DEPTH = 50\n","cfg.MODEL.BACKBONE.FREEZE_AT = 0\n","cfg.MODEL.WEIGHTS = '' #\"converted_model.pth\" \n","cfg.MODEL.RESNETS.NORM = 'FrozenBN'\n","\n","cfg.SOLVER.IMS_PER_BATCH = 1\n","cfg.SOLVER.BASE_LR = 0.0001  #  LR\n","cfg.SOLVER.MAX_ITER = 3600000    \n","\n","cfg.SOLVER.LR_SCHEDULER_NAME = \"WarmupMultiStepLR\"\n","cfg.SOLVER.WARMUP_ITERS = 3000\n","cfg.SOLVER.WARMUP_FACTOR = .01\n","cfg.SOLVER.STEPS =[]\n","cfg.SOLVER.GAMMA = 0.1\n","cfg.SOLVER.CLIP_GRADIENTS.CLIP_VALUE = 0.1\n","cfg.SOLVER.CHECKPOINT_PERIOD = 10000\n","cfg.TEST.EVAL_PERIOD = 6440\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"nfKvgofD6dEi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"executionInfo":{"status":"ok","timestamp":1597780176102,"user_tz":-120,"elapsed":10661,"user":{"displayName":"Dariusz Działkowski","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjk19eQEsSx6603O8FXwgnrAlD6uVS5VP9UHjgGXw=s64","userId":"17377559919401364457"}},"outputId":"b3ebd15d-f73a-44d9-f34b-9bd0900782b4"},"source":["# OUTPUT_DIR on Google Drive\n","drive.mount('/content/drive')\n","\n","cfg.OUTPUT_DIR = os.path.join(\"./drive/My Drive/Face detection\", \"WIDER_DeTr\")\n","#cfg.OUTPUT_DIR = os.path.join(\"OUTPUT\", \"WIDER_DeTr\")\n","os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w_R_0kY1NXXF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597780176103,"user_tz":-120,"elapsed":10658,"user":{"displayName":"Dariusz Działkowski","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjk19eQEsSx6603O8FXwgnrAlD6uVS5VP9UHjgGXw=s64","userId":"17377559919401364457"}},"outputId":"81c2e308-7310-47b4-8286-1b1de32094fe"},"source":["print(cfg.dump())\n"],"execution_count":12,"outputs":[{"output_type":"stream","text":["CUDNN_BENCHMARK: false\n","DATALOADER:\n","  ASPECT_RATIO_GROUPING: true\n","  FILTER_EMPTY_ANNOTATIONS: false\n","  NUM_WORKERS: 8\n","  REPEAT_THRESHOLD: 0.0\n","  SAMPLER_TRAIN: TrainingSampler\n","DATASETS:\n","  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n","  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n","  PROPOSAL_FILES_TEST: []\n","  PROPOSAL_FILES_TRAIN: []\n","  TEST:\n","  - face_val\n","  TRAIN:\n","  - face_train\n","  VAL:\n","  - face_val\n","GLOBAL:\n","  HACK: 1.0\n","INPUT:\n","  CROP:\n","    ENABLED: true\n","    SIZE:\n","    - 384\n","    - 600\n","    TYPE: absolute_range\n","  FORMAT: RGB\n","  MASK_FORMAT: polygon\n","  MAX_SIZE_TEST: 1333\n","  MAX_SIZE_TRAIN: 1333\n","  MIN_SIZE_TEST: 800\n","  MIN_SIZE_TRAIN:\n","  - 480\n","  - 512\n","  - 544\n","  - 576\n","  - 608\n","  - 640\n","  - 672\n","  - 704\n","  - 736\n","  - 768\n","  - 800\n","  MIN_SIZE_TRAIN_SAMPLING: choice\n","MODEL:\n","  ANCHOR_GENERATOR:\n","    ANGLES:\n","    - - -90\n","      - 0\n","      - 90\n","    ASPECT_RATIOS:\n","    - - 0.5\n","      - 1.0\n","      - 2.0\n","    NAME: DefaultAnchorGenerator\n","    OFFSET: 0.0\n","    SIZES:\n","    - - 32\n","      - 64\n","      - 128\n","      - 256\n","      - 512\n","  BACKBONE:\n","    FREEZE_AT: 0\n","    NAME: build_resnet_backbone\n","  DETR:\n","    DEC_LAYERS: 6\n","    DEEP_SUPERVISION: true\n","    DIM_FEEDFORWARD: 2048\n","    DROPOUT: 0.1\n","    ENC_LAYERS: 6\n","    GIOU_WEIGHT: 2.0\n","    HIDDEN_DIM: 256\n","    L1_WEIGHT: 5.0\n","    NHEADS: 8\n","    NO_OBJECT_WEIGHT: 0.1\n","    NUM_CLASSES: 1\n","    NUM_OBJECT_QUERIES: 2000\n","    PRE_NORM: false\n","  DEVICE: cuda\n","  FORMAT: RGB\n","  FPN:\n","    FUSE_TYPE: sum\n","    IN_FEATURES: []\n","    NORM: ''\n","    OUT_CHANNELS: 256\n","  KEYPOINT_ON: false\n","  LOAD_PROPOSALS: false\n","  MASK_ON: false\n","  META_ARCHITECTURE: Detr\n","  PANOPTIC_FPN:\n","    COMBINE:\n","      ENABLED: true\n","      INSTANCES_CONFIDENCE_THRESH: 0.5\n","      OVERLAP_THRESH: 0.5\n","      STUFF_AREA_LIMIT: 4096\n","    INSTANCE_LOSS_WEIGHT: 1.0\n","  PIXEL_MEAN:\n","  - 119.857\n","  - 110.808\n","  - 104.148\n","  PIXEL_STD:\n","  - 0.6076259372440945\n","  - 0.5876417505511812\n","  - 0.5971849743307086\n","  PROPOSAL_GENERATOR:\n","    MIN_SIZE: 0\n","    NAME: RPN\n","  RESNETS:\n","    DEFORM_MODULATED: false\n","    DEFORM_NUM_GROUPS: 1\n","    DEFORM_ON_PER_STAGE:\n","    - false\n","    - false\n","    - false\n","    - false\n","    DEPTH: 50\n","    NORM: FrozenBN\n","    NUM_GROUPS: 1\n","    OUT_FEATURES:\n","    - res2\n","    - res3\n","    - res4\n","    - res5\n","    RES2_OUT_CHANNELS: 256\n","    RES5_DILATION: 1\n","    STEM_OUT_CHANNELS: 64\n","    STRIDE_IN_1X1: false\n","    WIDTH_PER_GROUP: 64\n","  RETINANET:\n","    BBOX_REG_WEIGHTS:\n","    - 1.0\n","    - 1.0\n","    - 1.0\n","    - 1.0\n","    FOCAL_LOSS_ALPHA: 0.25\n","    FOCAL_LOSS_GAMMA: 2.0\n","    IN_FEATURES:\n","    - p3\n","    - p4\n","    - p5\n","    - p6\n","    - p7\n","    IOU_LABELS:\n","    - 0\n","    - -1\n","    - 1\n","    IOU_THRESHOLDS:\n","    - 0.4\n","    - 0.5\n","    NMS_THRESH_TEST: 0.5\n","    NUM_CLASSES: 80\n","    NUM_CONVS: 4\n","    PRIOR_PROB: 0.01\n","    SCORE_THRESH_TEST: 0.05\n","    SMOOTH_L1_LOSS_BETA: 0.1\n","    TOPK_CANDIDATES_TEST: 1000\n","  ROI_BOX_CASCADE_HEAD:\n","    BBOX_REG_WEIGHTS:\n","    - - 10.0\n","      - 10.0\n","      - 5.0\n","      - 5.0\n","    - - 20.0\n","      - 20.0\n","      - 10.0\n","      - 10.0\n","    - - 30.0\n","      - 30.0\n","      - 15.0\n","      - 15.0\n","    IOUS:\n","    - 0.5\n","    - 0.6\n","    - 0.7\n","  ROI_BOX_HEAD:\n","    BBOX_REG_LOSS_TYPE: smooth_l1\n","    BBOX_REG_LOSS_WEIGHT: 1.0\n","    BBOX_REG_WEIGHTS:\n","    - 10.0\n","    - 10.0\n","    - 5.0\n","    - 5.0\n","    CLS_AGNOSTIC_BBOX_REG: false\n","    CONV_DIM: 256\n","    FC_DIM: 1024\n","    NAME: ''\n","    NORM: ''\n","    NUM_CONV: 0\n","    NUM_FC: 0\n","    POOLER_RESOLUTION: 14\n","    POOLER_SAMPLING_RATIO: 0\n","    POOLER_TYPE: ROIAlignV2\n","    SMOOTH_L1_BETA: 0.0\n","    TRAIN_ON_PRED_BOXES: false\n","  ROI_HEADS:\n","    BATCH_SIZE_PER_IMAGE: 512\n","    IN_FEATURES:\n","    - res4\n","    IOU_LABELS:\n","    - 0\n","    - 1\n","    IOU_THRESHOLDS:\n","    - 0.5\n","    NAME: Res5ROIHeads\n","    NMS_THRESH_TEST: 0.5\n","    NUM_CLASSES: 1\n","    POSITIVE_FRACTION: 0.25\n","    PROPOSAL_APPEND_GT: true\n","    SCORE_THRESH_TEST: 0.05\n","  ROI_KEYPOINT_HEAD:\n","    CONV_DIMS:\n","    - 512\n","    - 512\n","    - 512\n","    - 512\n","    - 512\n","    - 512\n","    - 512\n","    - 512\n","    LOSS_WEIGHT: 1.0\n","    MIN_KEYPOINTS_PER_IMAGE: 1\n","    NAME: KRCNNConvDeconvUpsampleHead\n","    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true\n","    NUM_KEYPOINTS: 17\n","    POOLER_RESOLUTION: 14\n","    POOLER_SAMPLING_RATIO: 0\n","    POOLER_TYPE: ROIAlignV2\n","  ROI_MASK_HEAD:\n","    CLS_AGNOSTIC_MASK: false\n","    CONV_DIM: 256\n","    NAME: MaskRCNNConvUpsampleHead\n","    NORM: ''\n","    NUM_CONV: 0\n","    POOLER_RESOLUTION: 14\n","    POOLER_SAMPLING_RATIO: 0\n","    POOLER_TYPE: ROIAlignV2\n","  RPN:\n","    BATCH_SIZE_PER_IMAGE: 256\n","    BBOX_REG_LOSS_TYPE: smooth_l1\n","    BBOX_REG_LOSS_WEIGHT: 1.0\n","    BBOX_REG_WEIGHTS:\n","    - 1.0\n","    - 1.0\n","    - 1.0\n","    - 1.0\n","    BOUNDARY_THRESH: -1\n","    HEAD_NAME: StandardRPNHead\n","    IN_FEATURES:\n","    - res4\n","    IOU_LABELS:\n","    - 0\n","    - -1\n","    - 1\n","    IOU_THRESHOLDS:\n","    - 0.3\n","    - 0.7\n","    LOSS_WEIGHT: 1.0\n","    NMS_THRESH: 0.7\n","    POSITIVE_FRACTION: 0.5\n","    POST_NMS_TOPK_TEST: 1000\n","    POST_NMS_TOPK_TRAIN: 2000\n","    PRE_NMS_TOPK_TEST: 6000\n","    PRE_NMS_TOPK_TRAIN: 12000\n","    SMOOTH_L1_BETA: 0.0\n","  SEM_SEG_HEAD:\n","    COMMON_STRIDE: 4\n","    CONVS_DIM: 128\n","    IGNORE_VALUE: 255\n","    IN_FEATURES:\n","    - p2\n","    - p3\n","    - p4\n","    - p5\n","    LOSS_WEIGHT: 1.0\n","    NAME: SemSegFPNHead\n","    NORM: GN\n","    NUM_CLASSES: 1\n","  WEIGHTS: ''\n","OUTPUT_DIR: ./drive/My Drive/Face detection/WIDER_DeTr\n","SEED: -1\n","SOLVER:\n","  BACKBONE_MULTIPLIER: 0.1\n","  BASE_LR: 0.0001\n","  BIAS_LR_FACTOR: 1.0\n","  CHECKPOINT_PERIOD: 10000\n","  CLIP_GRADIENTS:\n","    CLIP_TYPE: full_model\n","    CLIP_VALUE: 0.1\n","    ENABLED: true\n","    NORM_TYPE: 2.0\n","  GAMMA: 0.1\n","  IMS_PER_BATCH: 1\n","  LR_SCHEDULER_NAME: WarmupMultiStepLR\n","  MAX_ITER: 3600000\n","  MOMENTUM: 0.9\n","  NESTEROV: false\n","  OPTIMIZER: ADAMW\n","  REFERENCE_WORLD_SIZE: 0\n","  STEPS: []\n","  WARMUP_FACTOR: 0.01\n","  WARMUP_ITERS: 3000\n","  WARMUP_METHOD: linear\n","  WEIGHT_DECAY: 0.0001\n","  WEIGHT_DECAY_BIAS: 0.0001\n","  WEIGHT_DECAY_NORM: 0.0\n","TEST:\n","  AUG:\n","    ENABLED: false\n","    FLIP: true\n","    MAX_SIZE: 4000\n","    MIN_SIZES:\n","    - 400\n","    - 500\n","    - 600\n","    - 700\n","    - 800\n","    - 900\n","    - 1000\n","    - 1100\n","    - 1200\n","  DETECTIONS_PER_IMAGE: 100\n","  EVAL_PERIOD: 6440\n","  EXPECTED_RESULTS: []\n","  KEYPOINT_OKS_SIGMAS: []\n","  PRECISE_BN:\n","    ENABLED: false\n","    NUM_ITER: 200\n","VERSION: 2\n","VIS_PERIOD: 0\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VPylJbuk9c30","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597780178194,"user_tz":-120,"elapsed":12747,"user":{"displayName":"Dariusz Działkowski","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjk19eQEsSx6603O8FXwgnrAlD6uVS5VP9UHjgGXw=s64","userId":"17377559919401364457"}}},"source":["import json\n","cfg_all=cfg_file.split('.')[-2]+'_ALL.json'\n","cfg_all = os.path.join(cfg.OUTPUT_DIR, cfg_all)\n","with open(cfg_all,'w') as f:\n","  json.dump(cfg,f)\n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BS-GIO0t5v-C","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"4ed27d26-9e62-45c8-c36e-c80a73eb3591"},"source":["os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n","trainer = Trainer(cfg) \n","\n","'''\n","val_loss = Validation_Loss(cfg)  \n","trainer.register_hooks([val_loss])\n","trainer._hooks = trainer._hooks[:-2] + trainer._hooks[-2:][::-1]\n","'''\n","\n","trainer.resume_or_load(resume=True)\n","trainer.train()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[32m[08/18 19:49:47 d2.engine.defaults]: \u001b[0mModel:\n","Detr(\n","  (detr): DETR(\n","    (transformer): Transformer(\n","      (encoder): TransformerEncoder(\n","        (layers): ModuleList(\n","          (0): TransformerEncoderLayer(\n","            (self_attn): MultiheadAttention(\n","              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n","            )\n","            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n","            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (dropout1): Dropout(p=0.1, inplace=False)\n","            (dropout2): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): TransformerEncoderLayer(\n","            (self_attn): MultiheadAttention(\n","              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n","            )\n","            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n","            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (dropout1): Dropout(p=0.1, inplace=False)\n","            (dropout2): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): TransformerEncoderLayer(\n","            (self_attn): MultiheadAttention(\n","              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n","            )\n","            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n","            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (dropout1): Dropout(p=0.1, inplace=False)\n","            (dropout2): Dropout(p=0.1, inplace=False)\n","          )\n","          (3): TransformerEncoderLayer(\n","            (self_attn): MultiheadAttention(\n","              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n","            )\n","            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n","            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (dropout1): Dropout(p=0.1, inplace=False)\n","            (dropout2): Dropout(p=0.1, inplace=False)\n","          )\n","          (4): TransformerEncoderLayer(\n","            (self_attn): MultiheadAttention(\n","              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n","            )\n","            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n","            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (dropout1): Dropout(p=0.1, inplace=False)\n","            (dropout2): Dropout(p=0.1, inplace=False)\n","          )\n","          (5): TransformerEncoderLayer(\n","            (self_attn): MultiheadAttention(\n","              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n","            )\n","            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n","            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (dropout1): Dropout(p=0.1, inplace=False)\n","            (dropout2): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (decoder): TransformerDecoder(\n","        (layers): ModuleList(\n","          (0): TransformerDecoderLayer(\n","            (self_attn): MultiheadAttention(\n","              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n","            )\n","            (multihead_attn): MultiheadAttention(\n","              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n","            )\n","            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n","            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (dropout1): Dropout(p=0.1, inplace=False)\n","            (dropout2): Dropout(p=0.1, inplace=False)\n","            (dropout3): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): TransformerDecoderLayer(\n","            (self_attn): MultiheadAttention(\n","              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n","            )\n","            (multihead_attn): MultiheadAttention(\n","              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n","            )\n","            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n","            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (dropout1): Dropout(p=0.1, inplace=False)\n","            (dropout2): Dropout(p=0.1, inplace=False)\n","            (dropout3): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): TransformerDecoderLayer(\n","            (self_attn): MultiheadAttention(\n","              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n","            )\n","            (multihead_attn): MultiheadAttention(\n","              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n","            )\n","            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n","            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (dropout1): Dropout(p=0.1, inplace=False)\n","            (dropout2): Dropout(p=0.1, inplace=False)\n","            (dropout3): Dropout(p=0.1, inplace=False)\n","          )\n","          (3): TransformerDecoderLayer(\n","            (self_attn): MultiheadAttention(\n","              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n","            )\n","            (multihead_attn): MultiheadAttention(\n","              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n","            )\n","            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n","            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (dropout1): Dropout(p=0.1, inplace=False)\n","            (dropout2): Dropout(p=0.1, inplace=False)\n","            (dropout3): Dropout(p=0.1, inplace=False)\n","          )\n","          (4): TransformerDecoderLayer(\n","            (self_attn): MultiheadAttention(\n","              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n","            )\n","            (multihead_attn): MultiheadAttention(\n","              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n","            )\n","            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n","            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (dropout1): Dropout(p=0.1, inplace=False)\n","            (dropout2): Dropout(p=0.1, inplace=False)\n","            (dropout3): Dropout(p=0.1, inplace=False)\n","          )\n","          (5): TransformerDecoderLayer(\n","            (self_attn): MultiheadAttention(\n","              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n","            )\n","            (multihead_attn): MultiheadAttention(\n","              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n","            )\n","            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n","            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (dropout1): Dropout(p=0.1, inplace=False)\n","            (dropout2): Dropout(p=0.1, inplace=False)\n","            (dropout3): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","    (class_embed): Linear(in_features=256, out_features=2, bias=True)\n","    (bbox_embed): MLP(\n","      (layers): ModuleList(\n","        (0): Linear(in_features=256, out_features=256, bias=True)\n","        (1): Linear(in_features=256, out_features=256, bias=True)\n","        (2): Linear(in_features=256, out_features=4, bias=True)\n","      )\n","    )\n","    (query_embed): Embedding(2000, 256)\n","    (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (backbone): Joiner(\n","      (0): MaskedBackbone(\n","        (backbone): ResNet(\n","          (stem): BasicStem(\n","            (conv1): Conv2d(\n","              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n","              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","            )\n","          )\n","          (res2): Sequential(\n","            (0): BottleneckBlock(\n","              (shortcut): Conv2d(\n","                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","              )\n","              (conv1): Conv2d(\n","                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","              )\n","              (conv2): Conv2d(\n","                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","              )\n","              (conv3): Conv2d(\n","                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","              )\n","            )\n","            (1): BottleneckBlock(\n","              (conv1): Conv2d(\n","                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","              )\n","              (conv2): Conv2d(\n","                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","              )\n","              (conv3): Conv2d(\n","                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","              )\n","            )\n","            (2): BottleneckBlock(\n","              (conv1): Conv2d(\n","                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","              )\n","              (conv2): Conv2d(\n","                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","              )\n","              (conv3): Conv2d(\n","                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","              )\n","            )\n","          )\n","          (res3): Sequential(\n","            (0): BottleneckBlock(\n","              (shortcut): Conv2d(\n","                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","              )\n","              (conv1): Conv2d(\n","                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","              )\n","              (conv2): Conv2d(\n","                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","              )\n","              (conv3): Conv2d(\n","                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","              )\n","            )\n","            (1): BottleneckBlock(\n","              (conv1): Conv2d(\n","                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","              )\n","              (conv2): Conv2d(\n","                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","              )\n","              (conv3): Conv2d(\n","                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","              )\n","            )\n","            (2): BottleneckBlock(\n","              (conv1): Conv2d(\n","                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","              )\n","              (conv2): Conv2d(\n","                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","              )\n","              (conv3): Conv2d(\n","                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","              )\n","            )\n","            (3): BottleneckBlock(\n","              (conv1): Conv2d(\n","                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","              )\n","              (conv2): Conv2d(\n","                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","              )\n","              (conv3): Conv2d(\n","                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","              )\n","            )\n","          )\n","          (res4): Sequential(\n","            (0): BottleneckBlock(\n","              (shortcut): Conv2d(\n","                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","              )\n","              (conv1): Conv2d(\n","                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","              )\n","              (conv2): Conv2d(\n","                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","              )\n","              (conv3): Conv2d(\n","                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","              )\n","            )\n","            (1): BottleneckBlock(\n","              (conv1): Conv2d(\n","                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","              )\n","              (conv2): Conv2d(\n","                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","              )\n","              (conv3): Conv2d(\n","                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","              )\n","            )\n","            (2): BottleneckBlock(\n","              (conv1): Conv2d(\n","                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","              )\n","              (conv2): Conv2d(\n","                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","              )\n","              (conv3): Conv2d(\n","                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","              )\n","            )\n","            (3): BottleneckBlock(\n","              (conv1): Conv2d(\n","                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","              )\n","              (conv2): Conv2d(\n","                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","              )\n","              (conv3): Conv2d(\n","                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","              )\n","            )\n","            (4): BottleneckBlock(\n","              (conv1): Conv2d(\n","                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","              )\n","              (conv2): Conv2d(\n","                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","              )\n","              (conv3): Conv2d(\n","                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","              )\n","            )\n","            (5): BottleneckBlock(\n","              (conv1): Conv2d(\n","                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","              )\n","              (conv2): Conv2d(\n","                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","              )\n","              (conv3): Conv2d(\n","                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","              )\n","            )\n","          )\n","          (res5): Sequential(\n","            (0): BottleneckBlock(\n","              (shortcut): Conv2d(\n","                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","              )\n","              (conv1): Conv2d(\n","                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","              )\n","              (conv2): Conv2d(\n","                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","              )\n","              (conv3): Conv2d(\n","                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","              )\n","            )\n","            (1): BottleneckBlock(\n","              (conv1): Conv2d(\n","                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","              )\n","              (conv2): Conv2d(\n","                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","              )\n","              (conv3): Conv2d(\n","                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","              )\n","            )\n","            (2): BottleneckBlock(\n","              (conv1): Conv2d(\n","                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","              )\n","              (conv2): Conv2d(\n","                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","              )\n","              (conv3): Conv2d(\n","                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","              )\n","            )\n","          )\n","        )\n","      )\n","      (1): PositionEmbeddingSine()\n","    )\n","  )\n","  (criterion): SetCriterion(\n","    (matcher): HungarianMatcher()\n","  )\n",")\n","\u001b[32m[08/18 19:49:47 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n","\u001b[36m|  category  | #instances   |\n","|:----------:|:-------------|\n","|    face    | 159424       |\n","|            |              |\u001b[0m\n","\u001b[32m[08/18 19:49:47 d2.data.common]: \u001b[0mSerializing 12880 elements to byte tensors and concatenating them all ...\n","\u001b[32m[08/18 19:49:48 d2.data.common]: \u001b[0mSerialized dataset takes 12.74 MiB\n","\u001b[32m[08/18 19:49:48 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oehRmfi00F_t","colab_type":"text"},"source":["Użycie modelu"]},{"cell_type":"code","metadata":{"id":"PLLN3H_E0GR8","colab_type":"code","colab":{}},"source":["cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set the testing threshold for this model\n","cfg.DATASETS.TEST = (\"face_val\", )\n","predictor = DefaultPredictor(cfg)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZkWwYijgQ61c","colab_type":"code","colab":{}},"source":["trainer = Trainer(cfg) \n","trainer.resume_or_load(resume=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"02GIDL6O0GYP","colab_type":"code","colab":{}},"source":["dataset_dicts = val\n","for d in random.sample(dataset_dicts, 1):    \n","    im = cv2.imread(d[\"file_name\"])\n","    outputs = predictor(im)\n","    v = Visualizer(im,metadata=faces_metadata, scale=0.5)\n","    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n","    cv2_imshow(v.get_image())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G60JHKE01Kdy","colab_type":"text"},"source":["AP metric on val"]},{"cell_type":"code","metadata":{"id":"Sn2C4HA31Kkn","colab_type":"code","colab":{}},"source":["from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n","from detectron2.data import build_detection_test_loader\n","\n","evaluator = COCOEvaluator(\"face_val\", cfg, False, output_dir=\"./Output/\")\n","val_loader = build_detection_test_loader(cfg, \"face_val\")\n","inference_on_dataset(trainer.model, val_loader, evaluator)"],"execution_count":null,"outputs":[]}]}